{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSE6240_Project_Discourse.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASSyV-BGIzCs",
        "colab_type": "text"
      },
      "source": [
        "# **1. Clean Dataset**\n",
        "\n",
        "From Google coarse discourse dataset and Reddit API, find comments and its corresponding discourse type. \n",
        "\n",
        "There are two files saved. One is with preprocess and without preprocess."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yx-ImzjtTByR",
        "colab_type": "code",
        "outputId": "b0db2d7a-665d-4eb0-86c4-e6bd994bfd0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "# Reddit API\n",
        "!pip install praw"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting praw\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/c0/b9714b4fb164368843b41482a3cac11938021871adf99bf5aaa3980b0182/praw-6.5.1-py3-none-any.whl (134kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 2.7MB/s \n",
            "\u001b[?25hCollecting prawcore<2.0,>=1.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/76/b5/ce6282dea45cba6f08a30e25d18e0f3d33277e2c9fcbda75644b8dc0089b/prawcore-1.0.1-py2.py3-none-any.whl\n",
            "Collecting update-checker>=0.16\n",
            "  Downloading https://files.pythonhosted.org/packages/17/c9/ab11855af164d03be0ff4fddd4c46a5bd44799a9ecc1770e01a669c21168/update_checker-0.16-py2.py3-none-any.whl\n",
            "Collecting websocket-client>=0.54.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from prawcore<2.0,>=1.0.1->praw) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client>=0.54.0->praw) (1.12.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.0.1->praw) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.0.1->praw) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.0.1->praw) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0,>=2.6.0->prawcore<2.0,>=1.0.1->praw) (2.8)\n",
            "Installing collected packages: prawcore, update-checker, websocket-client, praw\n",
            "Successfully installed praw-6.5.1 prawcore-1.0.1 update-checker-0.16 websocket-client-0.57.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZrvjYRoKZyx",
        "colab_type": "text"
      },
      "source": [
        "**1.1 function to clean posts**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b1OUFzJT6EI",
        "colab_type": "code",
        "outputId": "925dd2ae-2690-437e-8bef-f652e62d9717",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Preprocess the dataset\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')  # Download text data sets, including stop words\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def clean_posts(post):\n",
        "    # remove urls\n",
        "    # print(type(post))\n",
        "    text = re.sub(r'(https|http)?:\\/\\/.*[\\r\\n]*', '', post)\n",
        "    # lower cases\n",
        "    text = text.lower() # lowercase text\n",
        "    # remove some symbols that is useless for indentify text\n",
        "    # text = re.sub(r\"[/(){}\\[\\]\\|@,;''\\*\\-\\#&$%^_]\", ' ', text)\n",
        "    # some symbols may be helpful for identify discourse, like question mark, Exclamation mark\n",
        "    text = re.sub( r'([^a-zA-Z])', r' ', text)\n",
        "    # remove digits\n",
        "    text = re.sub(r'[0-9]+', '', text)\n",
        "    # split text into a words vector\n",
        "    words = text.split()\n",
        "    # # remove stop words\n",
        "    # stops = set(stopwords.words(\"english\"))\n",
        "    # meaningful_words = [w for w in words if w not in stops]\n",
        "    # tokenize the word and some punctuation\n",
        "    # words = nltk.word_tokenize(' '.join(meaningful_words))\n",
        "    \n",
        "    return words\n",
        "\n",
        "# print(posts_to_words(\"4/7/13  \\n\\n7/27/12  \\n\\nhttp://www.imdb.com/title/tt0073440/reference\\n\\nIt was only a few minutes into Robert Altman's homespun epic *Nashville* that I got the feeling I was watching a great movie. By the end it could not be denied. Now I'm sure it helps that I'm a musician, since this created an immediate connection to the subject matter. I spent a portion of the movie with my Telecaster in my lap trying to play along with the characters who all seem to be really playing and singing these songs. However I also am *not* a fan of country western, so that could have easily been a turn off.  \\n\\nTo begin describing the action in the film is daunting. I can't even process a lot of what I saw. This movie is extremely dense, and the first 30 minutes or so are spent just trying to figure out who people are. Their relationships to one another - some of which are purely incidental - slowly become clear as things progress. It's an ensemble cast with no clear lead and lots of overlapping conversations. Some developments all tie together in the end. Some seem to be kind of loose ends. There is throughout, however, a very keen sense about people. These naturalistic performances are believable and so is the world they inhabit. It feels like a film that doesn't exaggerate - rather, it downplays some of the outrageousness which just makes it seem all the more outrageous. Consider for example Shelley Duvall's wig or Jeff Goldblum's entire character.  \\n\\nI feel I should mention this. Jeff Goldblum is in this film. He never speaks, and he is awesome.  \\n\\n*Nashville* is a musical. There is reportedly around an hour of music in its 150+ minute run time. I believe it. Sometimes these songs, most if not all of which are original, seem to speak pointedly to or about a character. Sometimes it's all about the shifts and glances in the audience. There are many memorable scenes, such as when a fragile country starlet spaces out on stage and begins rambling between numbers, or a disenchanted housewife waits at the back of a bar for a womanizing folk singer, or a poor deceived waitress is goaded into a striptease by a raucous crowd and still refuses to believe that she can't actually sing.   \\n\\nThis is rich film, a complex tapestry perfectly suited to a moment in time (the US Bicentennial) that seems as though it will reward additional viewings. I sometimes become fidgety during long movies but I was never bored with *Nashville*. It swept me along with its quirky and interesting characters to a climax that maybe I should have seen coming.  \\n\\n8/10\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9Pr1mBXKj_o",
        "colab_type": "text"
      },
      "source": [
        "**1.2 load reddit api and coarse dataset to find comment and its corresponding discourse type**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HwQqokdYYXv",
        "colab_type": "code",
        "outputId": "e4da7c6d-30e2-438d-e6c3-d4a7a88b2f9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Process the dataset and save in csv files\n",
        "import json\n",
        "import praw\n",
        "import csv\n",
        "import time\n",
        "\"\"\"\n",
        "    Dataset used: https://github.com/google-research-datasets/coarse-discourse/blob/master/coarse_discourse_dataset.json\n",
        "    Reference: https://github.com/google-research-datasets/coarse-discourse\n",
        "               Amy X. Zhang, Bryan Culbertson, Praveen Paritosh. Characterizing Online Discussion Using Coarse Discourse Sequences. In Proceedings of the International AAAI Conference on Weblogs and Social Media (ICWSM '17). Montreal, Canada. 2017.\n",
        "    API used: praw Reddit\n",
        "\n",
        "    Clean up the the dataset, which contains the annotations of discourse, and use the reddit API to find the corresponding text.\n",
        "\"\"\"\n",
        "\n",
        "reddit = praw.Reddit(user_agent='jerry',\n",
        "                     client_id='ek57mJrjcLRibw', client_secret=\"dsqR0Hqow1lh1Z_k8LAcB98ZPwM\",\n",
        "                     username='jerry5631876', password='yjn19961022')\n",
        "discourse_data_path = \"/content/drive/My Drive/GT_classes/CSE_6240 - Text Mining/Reddit-Roles-Identification/discourse/coarse_discourse_dataset.json\"\n",
        "# read the entire file into a python array\n",
        "discourses = []\n",
        "discourses_type = []\n",
        "urls = set()\n",
        "prev_time = time.time()\n",
        "with open(discourse_data_path) as f:\n",
        "    lines = f.readlines()\n",
        "    \n",
        "    posts_annotations = open('/content/drive/My Drive/GT_classes/CSE_6240 - Text Mining/Reddit-Roles-Identification/discourse/post_annotations_without_word_preprocess.csv', 'w')\n",
        "    fieldnames = ['post', 'post_type']\n",
        "    writer = csv.DictWriter(posts_annotations, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writeheader()\n",
        "\n",
        "    print(\"Total length of lines: \", len(lines))\n",
        "    for i in range(len(lines)):\n",
        "        line = lines[i]\n",
        "        reader = json.loads(line)\n",
        "    ########### Test if there is a duplicate url ##########\n",
        "    #     if reader['url'] in urls:\n",
        "    #         print(\"It's duplicate url: \", reader['url'])\n",
        "    #     else:\n",
        "    #         urls.add(reader['url'])\n",
        "    ########### No duplicate url ##########################\n",
        "        if i % 10 == 0:\n",
        "            print(\"Time usage for {} to {} lines: {}\".format(i-10, i, time.time() - prev_time))\n",
        "            prev_time = time.time()\n",
        "        submission = reddit.submission(url = reader['url'])\n",
        "        post_id_dict = {}\n",
        "        \n",
        "        for post in reader['posts']:\n",
        "            # There are multiple annotations about same post\n",
        "            # choose the majority annotations type of the post\n",
        "            try:\n",
        "                post_type = post['majority_type']\n",
        "                post_id = post['id']\n",
        "                post_id_dict[post_id] = {\"majority_type\": post_type}\n",
        "            except Exception as e:\n",
        "                # Discard the text without the 'majority_type'\n",
        "                continue\n",
        "        \n",
        "        try:\n",
        "            full_submission_id = 't3_' + submission.id\n",
        "            if full_submission_id in post_id_dict:\n",
        "                post_id_dict[full_submission_id]['body'] = submission.selftext\n",
        "\n",
        "            for comment in submission.comments.list():\n",
        "                full_comment_id = 't1_' + comment.id\n",
        "                if full_comment_id in post_id_dict:\n",
        "                    post_id_dict[full_comment_id]['body'] = comment.body\n",
        "\n",
        "        except Exception as e:\n",
        "            print('Error %s' % (e))\n",
        "        \n",
        "        # print(\"--------------------start printing dict-----------------------\")\n",
        "        # print(\"Length of posts (including post and comments)\", len(post_id_dict))\n",
        "        # print(post_id_dict)\n",
        "        for discourse_id, content in post_id_dict.items():\n",
        "            if content.get('body'):\n",
        "                # discard the text that is not in the reddit api comment body\n",
        "                # discard the text that is [deleted] or [removed] or ''\n",
        "                text = content['body']\n",
        "                if text and text != '[deleted]' and text != '[removed]':\n",
        "                    # discourses.append(text)\n",
        "                    # discourses_type.append(content['majority_type'])\n",
        "                    dict_to_store = {\"post_type\":content['majority_type'], \"post\":text}\n",
        "                    # print(dict_to_store)\n",
        "                    writer.writerow(dict_to_store)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total length of lines:  9483\n",
            "Time usage for -10 to 0 lines: 0.09100580215454102\n",
            "4/7/13  \n",
            "\n",
            "7/27/12  \n",
            "\n",
            "http://www.imdb.com/title/tt0073440/reference\n",
            "\n",
            "It was only a few minutes into Robert Altman's homespun epic *Nashville* that I got the feeling I was watching a great movie. By the end it could not be denied. Now I'm sure it helps that I'm a musician, since this created an immediate connection to the subject matter. I spent a portion of the movie with my Telecaster in my lap trying to play along with the characters who all seem to be really playing and singing these songs. However I also am *not* a fan of country western, so that could have easily been a turn off.  \n",
            "\n",
            "To begin describing the action in the film is daunting. I can't even process a lot of what I saw. This movie is extremely dense, and the first 30 minutes or so are spent just trying to figure out who people are. Their relationships to one another - some of which are purely incidental - slowly become clear as things progress. It's an ensemble cast with no clear lead and lots of overlapping conversations. Some developments all tie together in the end. Some seem to be kind of loose ends. There is throughout, however, a very keen sense about people. These naturalistic performances are believable and so is the world they inhabit. It feels like a film that doesn't exaggerate - rather, it downplays some of the outrageousness which just makes it seem all the more outrageous. Consider for example Shelley Duvall's wig or Jeff Goldblum's entire character.  \n",
            "\n",
            "I feel I should mention this. Jeff Goldblum is in this film. He never speaks, and he is awesome.  \n",
            "\n",
            "*Nashville* is a musical. There is reportedly around an hour of music in its 150+ minute run time. I believe it. Sometimes these songs, most if not all of which are original, seem to speak pointedly to or about a character. Sometimes it's all about the shifts and glances in the audience. There are many memorable scenes, such as when a fragile country starlet spaces out on stage and begins rambling between numbers, or a disenchanted housewife waits at the back of a bar for a womanizing folk singer, or a poor deceived waitress is goaded into a striptease by a raucous crowd and still refuses to believe that she can't actually sing.   \n",
            "\n",
            "This is rich film, a complex tapestry perfectly suited to a moment in time (the US Bicentennial) that seems as though it will reward additional viewings. I sometimes become fidgety during long movies but I was never bored with *Nashville*. It swept me along with its quirky and interesting characters to a climax that maybe I should have seen coming.  \n",
            "\n",
            "8/10\n",
            "I've wanted to watch this for a long time. I was also turned off by the country western aspect.\n",
            "You strike me as the type who would appreciate it. I would give it a go. This is also my first Altman film so I didn't really know what to expect, except that people always compare PTA's Boogie Nights and Magnolia as being influenced by Altman. Magnolia is probably the best analog in terms of structure (having no lead character) but it is stylistically very different, much more melodramatic and transparently earnest. \n",
            "Yeah, I've always heard that Altman was famous for his ensemble casts. But I, too, have never seen an Altman.\n",
            "Alright guys, little background about myself. I'm a good looking, 23 year old male, have had modest success in the past with women, but have decided that modest just isn't good enough anymore.\n",
            "\n",
            "The problem has never been a lack of attention, or opportunity. It's just been not having the killer instinct and extreme AA, to the point where women will basically EYE FUCK me, and I still don't have the testicular fortitude to say anything unless they say something first.\n",
            "\n",
            "My goal is to approach 100 sets by the end of April, and hopefully break my AA.\n",
            "**January 16th 3 Sets:** \n",
            "Went out shopping with my grandma as I visited her. We go to a Factory store. Opened 3 sets.\n",
            "\n",
            "* First Set: was the changing room gal, HB 6 cute asian gal. Asked her a lot of questions about what i was wearing. Made her laugh several times with corny jokes, introduced her to my grandma, and we all talked for a minute after I was done picking out the clothes I wanted and left. \n",
            "\n",
            "* Second set: was this older gal, mid 40's. Wouldn't give her an HB because it was more of just a casual chat about the outfit I put on. She said she didn't like it, I responded with \"aw, I'm not attractive?!\" and she said \"I never said that haha\" and grinned. I gave her a wink as she continued laughing and went into her stall.\n",
            "\n",
            "* Third set:  **NUMBER CLOSE** As me and my Grandma are waiting in line to be helped checking out, and HB7 walks to the register and helps us. We IMMEDIATELY small talk. Exchanging intellectual talking points such as martyrs and assassinations, making her laugh with jokes, poking fun at her, transitioned to talking about movies, and she suggested I watch one. The Darjeeling Limited, I gave her a queer look, she then rolled out some receipt to write it down. I asked her to put her name down there too. She did, and then said \"Want my number so you can tell me when you watch it?\" I obliged her saying \"Yeah, that could work\". It was only after I got her number she even ACKNOWLEDGED my grandmother being ONE FOOT next to me the whole time! HAHA. She asked who she was, I said my Grandmother. Mind you I look completely white, and my grandmother is full Japanese. We talked for a bit about my grandma and the war, and how my grandma was bombed etc. Then I take my leave and text her about an hour later. We have been talking since.\n",
            "\n",
            "TL;DR: Got a number with my Grandma right next to me!\n",
            "dude, these sets are awesome. You're doing great. Sounds like you're a natural at meeting people once you get past the AA.\n",
            "Thanks man! Yeah I'm trying to just keep the \"who cares have fun\" attitude on. Because normally I freak out about what to say. But I find if I just bring a subject up and get some momentum going, I'm actually a pretty good conversationalist.\n",
            "Ok. Update! Sorry I haven't been doing this day to day like I should be. Let me see what I can remember\n",
            "\n",
            "**January 17-23 Sets:** Various sets I remember opening, unfortunately no number closes. Just good old fashioned chit chatting.\n",
            "\n",
            "* 4th set: During the 49ers and Giants game I went to grab a bite to eat at the local burger joint. Before ordering since no line was behind me I decided to small talk with the Female at the register HB5. Mostly talked about sports, a little about myself, and her coworker HB6 joined in the conversation. This lasted about 5 minutes, learned both had boyfriends through small talk and proceeded to order.\n",
            "\n",
            "* 5th set: Also small talked with the female who works at my local grocery chain. She was new there, we talked about the rain of all things haha. Male coworker semi interrupted us, still small talked with both. Then ordered my drink.\n",
            "\n",
            "Blahhh, I know there's more than this...Hence why I should try to update every day. Be back soon guys.\n",
            "More updates!\n",
            "I love cheese cake! I love both making and eating it, so I'm sad to see that they usually have the most crazy calorie counts, and I don't see many low cal recipes for them especially not ones that can work for many different people (for instance, I see a lot that require sugar free Jell-O, cool whip, Neufchatel cream cheese, very specific items that aren't available everywhere).\n",
            "\n",
            "[So here is a reciped I made!](http://imgur.com/a/z6VbS) it's very delicious, and I hope others will enjoy it too :)\n",
            ">very specific items that aren't available everywhere\n",
            "\n",
            "I find this funny, because I'm in the US and find it practically impossible to find gelatin in sheet form, digestive cookies, and Ribena light.\n",
            "\n",
            "I was, however, inspired by your recipe. to just wing it with what I *could* find.  I used sugar-free raspberry Jell-O and pre-made mini graham cracker crusts.  They came out entirely too Jell-O-like in texture, but still tasty, and I'll be tweaking them and trying again.  Thanks for the idea!\n",
            "I know,  but I figured it would be easier to find equivilants to those items (like you did :D ) than it is for Europeans to find jell-o, cool whip and zero cal cheese. I see posts about gelatine from r/keto on the front page, so I was under the impression that was easily available too? And digestive cookies, I know y'all have them! :b but that's the easiest thing to replace in this recipe,  didn't even use them myself\n",
            "/r/keto \n",
            "\n",
            "*****\n",
            "[^report ^a ^**problem**](http://reddit.com/r/LinkFixerBotSnr) ^| [^delete ^comment](http://www.reddit.com/message/compose?to=LinkFixerBotSnr&subject=Comment%20Deletion%20%28Parent%20Commenter%20Only%29&message=%2Bdelete+chs0fj3) ^| [^source ^code](http://github.com/WinneonSword/LFB) ^| [^contact ^developer](http://reddit.com/user/WinneonSword)\n",
            "Oh, we have gelatin, but I've never seen it in sheet form.  From what I've read, conversion between the two forms is [no simple matter](http://www.nigella.com/kitchen-queries/view/Gelatine-Leaves-to-Powder-Conversion/2376). \n",
            "\n",
            "As for digestive biscuits, I haven't seen those either, though I admittedly didn't look too hard.  Typically here we use graham crackers in their place, but it's not exactly the same.  According to [Wikipedia](http://en.wikipedia.org/wiki/Digestive_\\biscuit), \"graham crackers are typically a little dryer and more brittle.\"\n",
            "#####&#009;\n",
            "\n",
            "######&#009;\n",
            "\n",
            "####&#009;\n",
            " [**Digestive biscuit**](https://en.wikipedia.org/wiki/Digestive%20biscuit): [](#sfw) \n",
            "\n",
            "---\n",
            "\n",
            ">A __digestive biscuit__, sometimes described as a __sweet-meal biscuit__, is a semi-[sweet](https://en.wikipedia.org/wiki/Sweetness) [biscuit](https://en.wikipedia.org/wiki/Biscuit) (usually known in [American English](https://en.wikipedia.org/wiki/American_English) as a \"cookie\" ) that originated in the [United Kingdom](https://en.wikipedia.org/wiki/United_Kingdom) and is popular worldwide. The term \"digestive\" is derived from the belief that they had [antacid](https://en.wikipedia.org/wiki/Antacid) [properties](https://en.wikipedia.org/wiki/Chemical_property) due to the use of [sodium bicarbonate](https://en.wikipedia.org/wiki/Sodium_bicarbonate) when they were first developed.  Historically, some producers used [diastatic](https://en.wikipedia.org/wiki/%C2%B0Lintner) [malt extract](https://en.wikipedia.org/wiki/Malt) to \"digest\" some of the [starch](https://en.wikipedia.org/wiki/Starch) that existed in [flour](https://en.wikipedia.org/wiki/Flour) prior to [baking](https://en.wikipedia.org/wiki/Baking).  \n",
            "\n",
            "\n",
            ">====\n",
            "\n",
            ">[**Image**](https://i.imgur.com/FVw4oA3.jpg) [^(i)](https://commons.wikimedia.org/wiki/File:Digestive_biscuits.jpg)\n",
            "\n",
            "---\n",
            "\n",
            "^Interesting: [^Biscuit](https://en.wikipedia.org/wiki/Biscuit) ^| [^McVitie's](https://en.wikipedia.org/wiki/McVitie%27s) ^| [^Cracker ^\\(food)](https://en.wikipedia.org/wiki/Cracker_\\(food\\)) \n",
            "\n",
            "^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&subject=AutoWikibot NSFW toggle&message=%2Btoggle-nsfw+chs51gj) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&subject=AutoWikibot Deletion&message=%2Bdelete+chs51gj)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)\n",
            "Looks delicious, thanks for sharing!\n",
            "Looks tasty!\n",
            "I can't find them anywhere! I just want the most basic model they have such as the GI or Mil-spec. I would like to get a new one but they are sold out everywhere. [like this listing](http://www.firearmsforyou.com/guns/hand-guns/springfield-armory-pb9113l-1911-mil-spec-38-super-5-9-1-cocobolo-wood-grip-pk-1911-mil-spec-series-pb9113l/)\n",
            "You want your 1911 to be in .38?\n",
            "Where do you live? Eagle Guns and Range in Concord, NC has one in stock. Was looking at it yesterday as a matter of fact :) \n",
            "\n",
            "And it is the .45acp model\n",
            "\n",
            "3789 Roberta Church Road SW\n",
            "Concord, North Carolina 28027-9011\n",
            "\n",
            "PHONE: (704) 788-9013\n",
            "I saw several new ones yesterday, including a Range Officer at Piazza Jewelery and Pawn.  I don't know if they ship, though, but they do transfers.  Good prices, too\n",
            "\n",
            "10201 Page Ave.\n",
            "St. Louis, MO 63132\n",
            "314-427-2277\n",
            "lots of pawn shops around me, Augusta GA, have 1911 .45 ACP on the shelves.\n",
            "I want this one for myself but here you go. \n",
            "\n",
            "http://www.impactguns.com/product.aspx?zpid=15785\n",
            "\n",
            "Paying off some bills first though. \n",
            "Oh hell no. Sorry, I linked to the wrong pistol. I want that but in .45 ACP.\n",
            "Hahaha. Ok. I was wondering. \n",
            "Thank you! I'll give them a call tomorrow\n",
            "\n",
            "That's a good idea. I'll have to check around some pawn shops. \n",
            "Nothing to fancy though. Rock Island, Remington R1, ect, ect.\n",
            "Yea I really want something with a forged frame to build on. I'm pretty set on a Springfield.\n",
            "then I believe the R1 model is what you are looking for.\n",
            "Fixed Sights, Full frame, the same 3 safeties, lower ejection port, but again, nothing to fancy. The grips are a nice wood, but not crazy. No extended beaver tail.\n",
            "They are, or were last time I saw, relatively cheap. $520 I seen a few at. And that is brand new. As opposed to the 1911 I got at $780. Also brand new.\n",
            "So you're saying the R1 is better than a Springfield?\n",
            "R1 isnt better. But it is Remington. So its still good.\n",
            "Its just a step down from the Range Officer model.\n",
            "\n",
            "Yea I just wish I could find [this](http://www.impactguns.com/product.aspx?zpid=15780). Sold out everywhere.\n",
            "Whew! Can't afford that one. I'm lookin for the base model that sells on impactguns.com for like $540.\n",
            "Hey there, I just discovered this sub so this is my first post. I recently purchased my first 1911 from an active police officer. Its a Colt Defender. It is in pretty good shape with a few blemishes here and there but nothing major. I assume the gun as seen some range time since the previous owner used it as his side arm. I have a guy who wants to trade me his Kimber Pro Carry II for my Defender. His seems to be pretty much brand new. I'd like to hear some opinions as to whether or not I should make the trade. Neither gun will be an everyday carry so I'm not concerned with the difference in barrel length. I'm more curious if going from a Colt to a Kimber would be a bad move? Thanks for any advice.\n",
            "\n",
            "Edit: Here is my Colt: http://imgur.com/6KQIvzY  Here is his Kimber: http://imgur.com/L0PdiD7\n",
            "\"Going from a Colt to a Kimber is a bad move.\" Well... Kinda. I wouldn't call it a BAD move as much as \"a good move for the other guy\". I'm not on the Kimber bashing bandwagon, but I do understand why there is such a bandwagon. Without trying to sound like a jerk about it, I don't think a Kimber is quite the 1911 the Colt is, either in quality or reputation. Kimbers can be fine 1911's, but Colt is known as the standard by which others are measured. That being said,  Kimber is under that standard the same as an Ed Brown or Doug Turnbull is above it (although not by the same amount). Hope this helps and doesn't come off as dickish. In short, you'd be doing the other guy a favor and not yourself. \n",
            "\n",
            "Edit: I forgot to mention something else. Get a photo of that Colt posted!\n",
            "Thanks for the input! I sort of share the same sentiment as you. Just wanted another opinion on it. I guess my real reason for asking is because I know the other guy's Kimber is almost mint and mine has quite a bit or range time. I don't know much about the longevity of these guns. Just assumed maybe newer would be better??? Also, here are pictures of both guns. Mine: http://imgur.com/6KQIvzY And his: http://imgur.com/L0PdiD7\n",
            "I've got nothing against Kimbers. As stated above they can be fine 1911s. I would rather have a Colt than a Kimber though. There is a reason why his is designed to look like yours.\n",
            "Good point! I think I'm keeping it :)\n",
            "You won't regret keeping it. As far as longevity goes, it is almost indefinite. You will wear out parts like springs, mags, and maybe even the grips. The slide, frame, and barrel will likely outlive you. Colt makes great guns. How many companies who produced pistols 100 years ago still have their products found on gun shop shelves in working and safe condition?\n",
            "I wouldn't trade for a Kimber \n",
            "Why don't ya both Go to the range w box o good ammo and boxes o cheap ammo shoot a few boxes and see what you think.  If you like it and functions decide then. If you don't like it you had a day at the range.\n",
            "I have always found Wildy Owns1 to be relatively entertaining, and he's stared a new series for 07, his channel is http://www.youtube.com/user/ReturnOfWilderness/ -\n",
            "Another one is b0aty, he's been livestreaming a lot of it too.\n",
            "Wondering this myself! Got: runeshark, suomi and so wreck3d\n",
            "This guy is great, he does quests.  http://www.youtube.com/user/ThirdAgeFilm\n",
            "Sigh...this guy Sd 3000 trys to make great commentaries and wants to help the community.\n",
            "\n",
            "http://www.youtube.com/user/thefinestof08\n",
            "\n",
            "Also, my buddy P1stols does epic livestreams, b0aty is amazing, and Gross Gore.\n",
            "chrisarchieRS\n",
            "Should I do NPCs with fast exp or do slayer?\n",
            "If you are a pure or zerker, fast xp is what you want. If a range tank or main, then do slayer\n",
            "Thanks\n",
            "NP, I like slayer so maybe I am a bit biased on it, but I think doing slayer with combats is better than not and then having to do slayer after maxing\n",
            "Yeah I was thinking the same, its what I've been doing.\n",
            "Im a zerker and even i find slayers effective tbh\n",
            "if you want slayer levels, train slayer. if you only want combat xp, nmz is probably best.\n",
            "Main account - Slayer\n",
            "\n",
            "Pure account - NPCs with fast xp gain\n",
            "That depends, do you want to train slayer? If you do, just train it instead, because you will get several combat 99s before getting 99 slayer. If you have no interest in anything you get from slayer, then just kill whatever you want and ignore the skill.\n",
            "For any build Zerker or above, I'd say do Slayer. It gets *really* profitable at like 80+, and as well most tasks are pretty great exp. Also, if you get 85 Slayer, Abby demons are great exp and amazing money.\n",
            "Well I did npcs with fast exp to max my strength out first, then I've been doing slayer to get my attack and defense up so far. Though I don't care about maxing it, but eventually getting 85 would be a nice goal on the way to maxing my combat.\n",
            "[deleted]\n",
            "Zzz. So sick of the slayer circlejerk.\n",
            "[deleted]\n",
            "Lawl I seriously hate this argument. Probably nobody will see this outside of you but I don't care, here it goes:\n",
            "\n",
            "People say slayer is more efficient for melee than just training for xp or farming something for money because it's decent at both. The problem is, the casual player is better off not slaying past a certain level if they only want to maximize exp and monetary gains.\n",
            "\n",
            "Slayer brings in good gp over time but it is inconsistent in the short term. The profitable tasks outside of the slayer item drops (whip, dboots, dbow) can be done with or without slayer. It takes an extremely long time to exceed 80 slayer, likely over 125 hours of play. That is ballpark two or three months for any non hardcore player that ONLY does slayer. Additionally, this time could be better spent doing other activities.\n",
            "\n",
            "With the update of NMZ, training combat stats is no longer like using dh at monkeys in ape atoll or killing thousands of yaks. It is extremely fast exp and often times brings a decent profit. Because of the ability to always keep piety on and target low def monsters, the xp/hour rates shoot through the roof and are always much much higher than slayer (outside of one or two tasks, probably dags come within 15k per hour). Purchasing herb boxes easily takes care of the cost of prayer potions used during the activity. With the time saved from doing combat stats, players can use their spare time to focus fiercely on making money.\n",
            "\n",
            "Making money through combat stats is not very difficult once the stats exceed 80+ (which doesn't take long using nmz). There are a range of options including, but not limited to: gwd, barrows, dks, colored dragons. All of these options double the gp/hour average of slayer at worst and at best offer drops exceeding 50M. \n",
            "\n",
            "All in all, I will concede if you want to max out on osrs, this is not even an argument. Slayer is certainly the way to go for that. However, considering only 2 players are maxed in osrs speaks to the difficulty and dedication required to do so. Such dedication that many players don't really have anymore, largely because we're an older player base with more responsibilities than our 13 year old selves. Because of this fact, I vehemently oppose telling everyone who wants to get exp and money as fast as possible to slay because factually speaking it is not the way to go. \n",
            "I know this is a whole other topic, but an argument towards doing slayer is that it can be fun. I actually enjoy doing slayer and getting clues.\n",
            "\n",
            "I also dislike NMZ because it's boring and repetitive.\n",
            "\n",
            "\n",
            "Obviously this depends on the player, but if you enjoy doing slayer, you should put the time into it.\n",
            "Oh certainly. I wouldn't be an advocate of anything that makes the game boring for the player because why play if that's the case? My argument is rooted in that it is better to do things other than slayer if you solely want combat exp and money but definitely not better if you want to max. For me (very casual player, 4-6 hours a week), I like nmz because I can study/write papers while I do it with very little lost exp.\n",
            "**Slayer**\n",
            "\n",
            "* Experience\n",
            "* Clues\n",
            "* Money\n",
            "* Farm Supplies\n",
            "* Herb Supplies\n",
            "* Runes for mage\n",
            "* Bones/Prayer (Big bones/dragon bones from a lot of tasks)\n",
            "* And more\n",
            "\n",
            "**Training**\n",
            "\n",
            "* Experience\n",
            "This morning I got my first RS account back after I had changed (and forgot) the pass of back in 2008. \n",
            "\n",
            "I had previously sent pass reset appeals, all of which kept getting denied. I even tried @Jagexsupport on Twitter and they just told me to do the pass reset appeal again. When I asked if I could get any personalized help they ignored me.\n",
            "\n",
            "As a last option I emailed accounthelp@jagex.com with everything I could possibly remember of my account, only to get denied again. At this point I was losing hope, obviously nothing I could remember was good enough. I emailed them back asking if there was anything else I could do and that I had already told them everything I could remember.\n",
            "\n",
            "Then this morning I woke up with two emails in my inbox, a pass reset link and an email from a different Jmod saying that my appeal was accepted.\n",
            "\n",
            "So now I have my first account back and yeah, it's nooby but it means a lot to me to have it back.\n",
            "\n",
            "My advice to all of you is to not give up when it comes to going through Jagex's support. They definitely make things frustrating, lol.\n",
            "\n",
            "Also major thanks to Mod Tallboy for accepting my appeal. I'm definitely grateful.\n",
            "I've played continuously on my first account since Dec 05. I could have come back to os with a friends much better account. But the sentiment is real. \n",
            "Sweet dude. When i recovered my first account it was last online like 2200 days ago.\n",
            "I had last logged in 2071 days ago from yesterday. Which was  Wednesday July 16, 2008.\n",
            "Some companies have terrible support.\n",
            "\n",
            "I've been trying to get an account back on another game for nearly a year now & the only reason I can't login is because of being forced to login with e-mails.\n",
            "\n",
            "Congrats on getting your account back though. :]\n",
            "Same thing happened to me. I tried like 10 times over 6 months to get my main back. One day, it finally went through....only to discover it had been charged to -2800 JCoins and Jagex will not remove the charges. Might as well still be hacked.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-c0913c91d893>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mfull_submission_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m't3_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfull_submission_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpost_id_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mpost_id_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfull_submission_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselftext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcomment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/praw/models/reddit/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attribute)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;34m\"\"\"Return the value of `attribute`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         raise AttributeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/praw/models/reddit/submission.py\u001b[0m in \u001b[0;36m_fetch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0msubmission_listing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomment_listing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0mcomment_listing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mListing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reddit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomment_listing\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/praw/models/reddit/submission.py\u001b[0m in \u001b[0;36m_fetch_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAPI_PATH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/praw/reddit.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, path, params, data, files)\u001b[0m\n\u001b[1;32m    625\u001b[0m         \"\"\"\n\u001b[1;32m    626\u001b[0m         return self._core.request(\n\u001b[0;32m--> 627\u001b[0;31m             \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m         )\n\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/prawcore/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, path, data, files, json, params)\u001b[0m\n\u001b[1;32m    183\u001b[0m         return self._request_with_retries(\n\u001b[1;32m    184\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             params=params, url=url)\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/prawcore/sessions.py\u001b[0m in \u001b[0;36m_request_with_retries\u001b[0;34m(self, data, files, json, method, params, url, retries)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         response, saved_exception = self._make_request(\n\u001b[0;32m--> 116\u001b[0;31m             data, files, json, method, params, retries, url)\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mdo_retry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/prawcore/sessions.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, data, files, json, method, params, retries, url)\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_requestor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_header_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 params=params)\n\u001b[0m\u001b[1;32m    102\u001b[0m             log.debug('Response: {} ({} bytes)'.format(\n\u001b[1;32m    103\u001b[0m                 response.status_code, response.headers.get('content-length')))\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/prawcore/rate_limit.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, request_function, set_header_callback, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'headers'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_header_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/prawcore/requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;34m\"\"\"Issue the HTTP request capturing any errors that may occur.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_http\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTIMEOUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRequestException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1346\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1347\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1010\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1012\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FacJEYMKzH9",
        "colab_type": "text"
      },
      "source": [
        "**1.3 clean up comments and save into another file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rf4YON-VBTwt",
        "colab_type": "code",
        "outputId": "b515a00a-a62f-40e4-c8cf-b96f4bfe8e14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# save the clean up data\n",
        "import pandas as pd\n",
        "import csv\n",
        "path = \"/content/drive/My Drive/GT_classes/CSE_6240 - Text Mining/Reddit-Roles-Identification/discourse/post_annotations_without_word_preprocess.csv\"\n",
        "discourse_file = pd.read_csv(path)\n",
        "\n",
        "preprocess_file = '/content/drive/My Drive/GT_classes/CSE_6240 - Text Mining/Reddit-Roles-Identification/discourse/dataset_only_letters.csv'\n",
        "posts_annotations = open(preprocess_file, 'w')\n",
        "fieldnames = ['post', 'post_type']\n",
        "writer = csv.DictWriter(posts_annotations, fieldnames=fieldnames)\n",
        "\n",
        "writer.writeheader()\n",
        "\n",
        "for i in range(discourse_file.shape[0]):\n",
        "    post = discourse_file['post'][i]\n",
        "    post_type = discourse_file['post_type'][i]\n",
        "\n",
        "    clean_text = clean_posts(post)\n",
        "    print(clean_text)\n",
        "    dict_to_store = {\"post_type\":post_type, \"post\":clean_text}\n",
        "    if clean_text:\n",
        "    # print(dict_to_store)\n",
        "        writer.writerow(dict_to_store)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgSd9c5sK_R2",
        "colab_type": "text"
      },
      "source": [
        "**1.4 Dataset Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjY3zig9mwCr",
        "colab_type": "code",
        "outputId": "47a07e88-3949-47ab-b662-dc8dd2962a26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "path = \"/content/drive/My Drive/GT_classes/CSE_6240 - Text Mining/Reddit-Roles-Identification/discourse/dataset_only_letters.csv\"\n",
        "discourse_file = pd.read_csv(path)\n",
        "\n",
        "total_rows = discourse_file.shape[0]\n",
        "print(\"Total rows of the dataset: \", total_rows)\n",
        "print()\n",
        "print(\"Rows of question discourse: \", len(discourse_file[discourse_file[\"post_type\"] == 'question']))\n",
        "print(\"Rows of answer discourse: \", len(discourse_file[discourse_file[\"post_type\"] == 'answer']))\n",
        "print(\"Rows of announcement discourse: \", len(discourse_file[discourse_file[\"post_type\"] == 'announcement']))\n",
        "print(\"Rows of agreement discourse: \", len(discourse_file[discourse_file[\"post_type\"] == 'agreement']))\n",
        "print(\"Rows of appreciation discourse: \", len(discourse_file[discourse_file[\"post_type\"] == 'appreciation']))\n",
        "print(\"Rows of disagreement discourse: \", len(discourse_file[discourse_file[\"post_type\"] == 'disagreement']))\n",
        "print(\"Rows of negative reaction discourse: \", len(discourse_file[discourse_file[\"post_type\"] == 'negativereaction']))\n",
        "print(\"Rows of elaboration discourse: \", len(discourse_file[discourse_file[\"post_type\"] == 'elaboration']))\n",
        "print(\"Rows of humor discourse: \", len(discourse_file[discourse_file[\"post_type\"] == 'humor']))\n",
        "print()\n",
        "print(\"Dataset sample: \", discourse_file.columns)\n",
        "print(discourse_file[discourse_file.index == 10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total rows of the dataset:  98705\n",
            "\n",
            "Rows of question discourse:  16098\n",
            "Rows of answer discourse:  39958\n",
            "Rows of announcement discourse:  1338\n",
            "Rows of agreement discourse:  4909\n",
            "Rows of appreciation discourse:  8481\n",
            "Rows of disagreement discourse:  3339\n",
            "Rows of negative reaction discourse:  1810\n",
            "Rows of elaboration discourse:  18647\n",
            "Rows of humor discourse:  2297\n",
            "\n",
            "Dataset sample:  Index(['post', 'post_type'], dtype='object')\n",
            "                                                 post     post_type\n",
            "10  ['i', 'love', 'cheese', 'cake', 'i', 'love', '...  announcement\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB7DGWyMLG08",
        "colab_type": "text"
      },
      "source": [
        "# **2. Attention-based BiLSTM model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWdqxncbLt5W",
        "colab_type": "text"
      },
      "source": [
        "**2.1 Set up for model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APbINIBNNs0v",
        "colab_type": "code",
        "outputId": "6dc79c01-0a12-44be-9dae-bbcf1d97e419",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# download and load the Google pretrained word2vec model\n",
        "from gensim.models import word2vec, KeyedVectors\n",
        "# !wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "# !gunzip GoogleNews-vectors-negative300.bin.gz\n",
        "\n",
        "pre_trained_word2vec_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
        "\n",
        "print(len(pre_trained_word2vec_model[\"hello\"]))\n",
        "embed_size = len(pre_trained_word2vec_model[\"hello\"])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXFw4YkMew10",
        "colab_type": "code",
        "outputId": "8393eb34-2d6e-4b1e-d7c8-a849d8319b53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# set up\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "torch.manual_seed(2020)\n",
        "\n",
        "path = \"/content/drive/My Drive/GT_classes/CSE_6240 - Text Mining/Reddit-Roles-Identification/discourse/dataset_only_letters.csv\"\n",
        "discourse_file = pd.read_csv(path)\n",
        "\n",
        "# Prepare for training the validation dataset\n",
        "train_size=0.8\n",
        "train_posts, test_posts, train_post_type, test_post_type = train_test_split(discourse_file['post'], \n",
        "                                                                            discourse_file['post_type'], \n",
        "                                                                            train_size=train_size)\n",
        "\n",
        "max_length_post = discourse_file['post'].str.len().max()\n",
        "\n",
        "print(\"Length of training posts: \", len(train_posts))\n",
        "print(\"Length of training post type: \", len(train_post_type))\n",
        "\n",
        "print(\"Length of testing posts: \", len(test_posts))\n",
        "print(\"Length of testing post type: \", len(test_post_type))\n",
        "print(\"max length post: \", max_length_post)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of training posts:  79015\n",
            "Length of training post type:  79015\n",
            "Length of testing posts:  19754\n",
            "Length of testing post type:  19754\n",
            "max length post:  27545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMNT6Ggv-Iwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert the posts to vector using the pretrained Google word2vec model\n",
        "discourse_types = {'agreement': 0, 'appreciation': 1, 'answer': 2, 'elaboration': 3, 'question': 4, \n",
        "                   'negativereaction': 5, 'other': 6, 'disagreement': 7, 'announcement': 8, 'humor': 9}\n",
        "\n",
        "def posts2vec(posts, word2vec_model):\n",
        "    posts_num = len(posts)\n",
        "    posts_vec = np.zeros((posts_num, max_length_post, embed_size))\n",
        "    for i in range(posts_num):\n",
        "        post = posts[i]\n",
        "        for j in range(len(post)):\n",
        "            word = post[j]\n",
        "            try:\n",
        "                posts_vec[i][j] = word2vec_model[word]\n",
        "            except:\n",
        "                continue\n",
        "    \n",
        "    return posts_vec.astype(float)\n",
        "\n",
        "def type2vec(posts_type):\n",
        "    return np.array([discourse_types[discourse] for discourse in posts_type])\n",
        "\n",
        "# train_posts = posts2vec(train_posts, pre_trained_word2vec_model)\n",
        "# test_posts = posts2vec(test_posts, pre_trained_word2vec_model)\n",
        "# print(\"Finish posts to vector\")\n",
        "# # print(\"train_posts sample: \", train_posts[0])\n",
        "# # print(\"train_posts sample type: \", train_post_type[0])\n",
        "# # print()\n",
        "# # print(\"test\")\n",
        "# # print(train_posts.dtype)\n",
        "# # print(train_posts.shape)\n",
        "# train_post_type = type2vec(train_post_type)\n",
        "# test_post_type = type2vec(test_post_type)\n",
        "# print(\"Finish discourse to vector\")\n",
        "\n",
        "# X, y = sample_dataset_minibatch(train_posts, train_post_type, 5)\n",
        "# print(X.shape, y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiz8CMJd5QwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_dataset_minibatch(X, y, batch_size):\n",
        "    split_size = len(X)\n",
        "    X, y = X.to_numpy(), y.to_numpy()\n",
        "    mask = np.random.choice(X.shape[0], batch_size)\n",
        "    X, y = X[mask], y[mask]\n",
        "    X = posts2vec(X, pre_trained_word2vec_model)\n",
        "    y = type2vec(y)\n",
        "    # max_len = max([len(post) for post in X])\n",
        "    # _, embed_size = X[0].shape\n",
        "    # new_X = np.zeros((batch_size, max_length_post, embed_size), dtype=np.float32)\n",
        "    # for i in range(batch_size):\n",
        "    #     x = X[i]\n",
        "    #     post_len, _ = x.shape\n",
        "    #     new_stack = np.zeros((max_length_post - post_len, embed_size))\n",
        "    #     X[i] = np.concatenate((x, new_stack), axis=0).astype(float)\n",
        "        # print(X[i].shape)\n",
        "    # print(X.shape)\n",
        "    # print(X.astype)\n",
        "    return X, y\n",
        "\n",
        "# X, y = sample_dataset_minibatch(train_posts, train_post_type, 10)\n",
        "# print(X.astype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7yNyrVMNc6Z",
        "colab_type": "text"
      },
      "source": [
        "**2.2 Model implementation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke_P4spwkY8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, hidden_dim, discourse_types, lstm_layers, bidirectional, device):\n",
        "        super(BiLSTM, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_dim\n",
        "        self.device = device\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "        self.num_layers = lstm_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(300, hidden_dim,\n",
        "                            num_layers=lstm_layers, bidirectional=bidirectional)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim * self.num_directions, discourse_types)\n",
        "\n",
        "    def attention_net(self, lstm_output, final_state):\n",
        "        hidden = final_state.squeeze(0)\n",
        "        if self.num_directions == 2:\n",
        "            hidden = torch.cat((hidden[0], hidden[1]), dim=1)\n",
        "            \n",
        "        # print(lstm_output.size(), hidden.size())\n",
        "        attn_weights = torch.bmm(lstm_output, hidden.unsqueeze(2)).squeeze(2)\n",
        "        # print(\"attn_weights size: \", attn_weights.size())\n",
        "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
        "        # print(\"soft_attn_weights size: \", soft_attn_weights.size())\n",
        "        # print(\"lstm output size: \", lstm_output.size())\n",
        "        new_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
        "\n",
        "        return new_hidden_state\n",
        "\n",
        "    def forward(self, sentences, batch_size):\n",
        "        # sentences is a vector of with index of embed model\n",
        "\n",
        "        inputs = sentences.permute(1, 0, 2)\n",
        "        # print(inputs.size())\n",
        "        h_0 = Variable(torch.zeros(self.num_directions * self.num_layers, batch_size, self.hidden_size).to(self.device))\n",
        "        c_0 = Variable(torch.zeros(self.num_directions * self.num_layers, batch_size, self.hidden_size).to(self.device))\n",
        "        # print(\"test\")\n",
        "        lstm_output, (final_hidden_state, final_cell_state) = self.lstm(inputs, (h_0, c_0)) # final_hidden_state.size() = (1, batch_size, hidden_size) \n",
        "        lstm_output = lstm_output.permute(1, 0, 2) # output.size() = (batch_size, num_seq, hidden_size)\n",
        "        # print(lstm_output.size())\n",
        "        attn_output = self.attention_net(lstm_output, final_hidden_state)\n",
        "        # print(\"attention size: \", attn_output.size())\n",
        "        output = self.hidden2tag(attn_output)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iwF9RuFjtAS",
        "colab_type": "text"
      },
      "source": [
        "2.3 Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vrn4NDIktAK8",
        "colab_type": "code",
        "outputId": "1645cc11-d30a-4046-eb45-9ee9d8f558b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "# Start training \n",
        "# vocab_size = len(word_to_idx)\n",
        "# embed_size = 100           # dimensionality of word embeddings\n",
        "hidden_size = 32          # number of features in hidden state of the RNN decoder\n",
        "num_layers = 1              # number of LSTM layers\n",
        "num_discourse_type = 10\n",
        "bidirectional = True\n",
        "dropout = 0.2\n",
        "\n",
        "batch_size = 16         # batch size\n",
        "num_epochs = 5             # number of training epochs\n",
        "learning_rate = 5e-4\n",
        "lr_decay_step_size = 500\n",
        "lr_decay = 0.995\n",
        "save_every = 1             # determines frequency of saving model weights\n",
        "print_every = 10          # determines window for printing average loss\n",
        "save_every = 500            #determine window for saving state point of model\n",
        "model_path = \"/content/drive/My Drive/GT_classes/CSE_6240 - Text Mining/Reddit-Roles-Identification/discourse/Model_32hsize_1layer_bidirectional_5epochs_16batch.pt\"\n",
        "training_path = \"/content/drive/My Drive/GT_classes/CSE_6240 - Text Mining/Reddit-Roles-Identification/discourse/training_process2.txt\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = BiLSTM(hidden_dim = hidden_size, \n",
        "               discourse_types = num_discourse_type,\n",
        "               lstm_layers = num_layers, \n",
        "               bidirectional = bidirectional,\n",
        "               device = device)\n",
        "if os.path.exists(model_path):\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# Move models to GPU if CUDA is available. \n",
        "model.to(device)\n",
        "\n",
        "# Define the loss function. \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "params = list(model.parameters())\n",
        "\n",
        "optimizer = torch.optim.Adam(params = model.parameters(), lr = learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_step_size, gamma=lr_decay)\n",
        "\n",
        "f1_average = 0    # average of every print_every iterations\n",
        "\n",
        "num_train = train_posts.size\n",
        "iterations_per_epoch = max(num_train // batch_size, 1)\n",
        "num_iterations = num_epochs * iterations_per_epoch\n",
        "\n",
        "training_file = open(training_path, 'w')\n",
        "for i in range(16150, num_iterations):\n",
        "    X, y = sample_dataset_minibatch(train_posts, train_post_type, batch_size)\n",
        "\n",
        "    model.zero_grad()\n",
        "    # print(X.dtype, y.dtype)\n",
        "    X, y = torch.FloatTensor(X).to(device), torch.LongTensor(y).to(device)\n",
        "\n",
        "    tag_score = model(X, batch_size)\n",
        "    # calculate f1 score\n",
        "    prediction = torch.argmax(tag_score, dim=1)\n",
        "\n",
        "    f1 = f1_score(prediction.cpu().numpy(), y.cpu().numpy(), average='micro')\n",
        "    f1_average += f1\n",
        "    \n",
        "    loss = criterion(tag_score, y)\n",
        "    # print(tag_score, post_type)\n",
        "    # print(\"loss of {} type discourse: {}\".format(tag, loss))\n",
        "    loss.backward()\n",
        "    \n",
        "    # Update the parameters in the optimizer.\n",
        "    optimizer.step()\n",
        "    \n",
        "    if i % print_every == 0:\n",
        "        with torch.no_grad():\n",
        "            # print(\"start testing...\")\n",
        "            X, y = sample_dataset_minibatch(test_posts, test_post_type, batch_size)\n",
        "            X, y = torch.FloatTensor(X).to(device), torch.LongTensor(y).to(device)\n",
        "            # print(\"X size: \", X.size())\n",
        "\n",
        "            tag_score = model(X, batch_size)\n",
        "            f1 = f1_score(prediction.cpu().numpy(), y.cpu().numpy(), average='micro')\n",
        "\n",
        "            loss = criterion(tag_score, y)\n",
        "\n",
        "            average = f1_average if i == 0 else f1_average / print_every\n",
        "\n",
        "            stats = 'Iterations [%d/%d], Loss: %f, Validation F1 score: %f, Average training f1 scores: %f '  \\\n",
        "                    % (i, num_iterations, loss, f1,  average)\n",
        "\n",
        "            training_file.writelines(stats + '\\n')\n",
        "            print(stats)\n",
        "            f1_average = 0\n",
        "\n",
        "\n",
        "    if i % save_every == 0:\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iterations [16150/24690], Loss: 1.325344, Validation F1 score: 0.250000, Average training f1 scores: 0.068750 \n",
            "Iterations [16160/24690], Loss: 1.745630, Validation F1 score: 0.125000, Average training f1 scores: 0.512500 \n",
            "Iterations [16170/24690], Loss: 1.502481, Validation F1 score: 0.437500, Average training f1 scores: 0.481250 \n",
            "Iterations [16180/24690], Loss: 1.947930, Validation F1 score: 0.375000, Average training f1 scores: 0.481250 \n",
            "Iterations [16190/24690], Loss: 1.782853, Validation F1 score: 0.437500, Average training f1 scores: 0.412500 \n",
            "Iterations [16200/24690], Loss: 1.164348, Validation F1 score: 0.437500, Average training f1 scores: 0.462500 \n",
            "Iterations [16210/24690], Loss: 1.261814, Validation F1 score: 0.250000, Average training f1 scores: 0.487500 \n",
            "Iterations [16220/24690], Loss: 1.721209, Validation F1 score: 0.437500, Average training f1 scores: 0.450000 \n",
            "Iterations [16230/24690], Loss: 1.721864, Validation F1 score: 0.250000, Average training f1 scores: 0.468750 \n",
            "Iterations [16240/24690], Loss: 1.785083, Validation F1 score: 0.437500, Average training f1 scores: 0.431250 \n",
            "Iterations [16250/24690], Loss: 1.429926, Validation F1 score: 0.187500, Average training f1 scores: 0.493750 \n",
            "Iterations [16260/24690], Loss: 1.537022, Validation F1 score: 0.375000, Average training f1 scores: 0.481250 \n",
            "Iterations [16270/24690], Loss: 1.476429, Validation F1 score: 0.437500, Average training f1 scores: 0.556250 \n",
            "Iterations [16280/24690], Loss: 1.355980, Validation F1 score: 0.375000, Average training f1 scores: 0.456250 \n",
            "Iterations [16290/24690], Loss: 1.474851, Validation F1 score: 0.375000, Average training f1 scores: 0.550000 \n",
            "Iterations [16300/24690], Loss: 1.874455, Validation F1 score: 0.312500, Average training f1 scores: 0.462500 \n",
            "Iterations [16310/24690], Loss: 1.697188, Validation F1 score: 0.250000, Average training f1 scores: 0.475000 \n",
            "Iterations [16320/24690], Loss: 1.758074, Validation F1 score: 0.250000, Average training f1 scores: 0.550000 \n",
            "Iterations [16330/24690], Loss: 1.873230, Validation F1 score: 0.500000, Average training f1 scores: 0.500000 \n",
            "Iterations [16340/24690], Loss: 1.905198, Validation F1 score: 0.250000, Average training f1 scores: 0.456250 \n",
            "Iterations [16350/24690], Loss: 0.979110, Validation F1 score: 0.625000, Average training f1 scores: 0.468750 \n",
            "Iterations [16360/24690], Loss: 1.719858, Validation F1 score: 0.312500, Average training f1 scores: 0.475000 \n",
            "Iterations [16370/24690], Loss: 1.031417, Validation F1 score: 0.562500, Average training f1 scores: 0.443750 \n",
            "Iterations [16380/24690], Loss: 1.746910, Validation F1 score: 0.312500, Average training f1 scores: 0.543750 \n",
            "Iterations [16390/24690], Loss: 1.934356, Validation F1 score: 0.250000, Average training f1 scores: 0.493750 \n",
            "Iterations [16400/24690], Loss: 1.672540, Validation F1 score: 0.312500, Average training f1 scores: 0.431250 \n",
            "Iterations [16410/24690], Loss: 1.320893, Validation F1 score: 0.375000, Average training f1 scores: 0.525000 \n",
            "Iterations [16420/24690], Loss: 1.340322, Validation F1 score: 0.437500, Average training f1 scores: 0.481250 \n",
            "Iterations [16430/24690], Loss: 1.446905, Validation F1 score: 0.312500, Average training f1 scores: 0.468750 \n",
            "Iterations [16440/24690], Loss: 2.082357, Validation F1 score: 0.187500, Average training f1 scores: 0.493750 \n",
            "Iterations [16450/24690], Loss: 1.676674, Validation F1 score: 0.312500, Average training f1 scores: 0.418750 \n",
            "Iterations [16460/24690], Loss: 1.395650, Validation F1 score: 0.125000, Average training f1 scores: 0.468750 \n",
            "Iterations [16470/24690], Loss: 1.556176, Validation F1 score: 0.625000, Average training f1 scores: 0.468750 \n",
            "Iterations [16480/24690], Loss: 1.840729, Validation F1 score: 0.375000, Average training f1 scores: 0.425000 \n",
            "Iterations [16490/24690], Loss: 1.739400, Validation F1 score: 0.312500, Average training f1 scores: 0.431250 \n",
            "Iterations [16500/24690], Loss: 1.543108, Validation F1 score: 0.312500, Average training f1 scores: 0.443750 \n",
            "Iterations [16510/24690], Loss: 1.062025, Validation F1 score: 0.437500, Average training f1 scores: 0.437500 \n",
            "Iterations [16520/24690], Loss: 1.252962, Validation F1 score: 0.375000, Average training f1 scores: 0.400000 \n",
            "Iterations [16530/24690], Loss: 1.658921, Validation F1 score: 0.375000, Average training f1 scores: 0.475000 \n",
            "Iterations [16540/24690], Loss: 1.105816, Validation F1 score: 0.437500, Average training f1 scores: 0.431250 \n",
            "Iterations [16550/24690], Loss: 1.517017, Validation F1 score: 0.375000, Average training f1 scores: 0.456250 \n",
            "Iterations [16560/24690], Loss: 1.614432, Validation F1 score: 0.312500, Average training f1 scores: 0.481250 \n",
            "Iterations [16570/24690], Loss: 1.696614, Validation F1 score: 0.250000, Average training f1 scores: 0.387500 \n",
            "Iterations [16580/24690], Loss: 1.325751, Validation F1 score: 0.437500, Average training f1 scores: 0.418750 \n",
            "Iterations [16590/24690], Loss: 1.487467, Validation F1 score: 0.437500, Average training f1 scores: 0.412500 \n",
            "Iterations [16600/24690], Loss: 1.816069, Validation F1 score: 0.312500, Average training f1 scores: 0.437500 \n",
            "Iterations [16610/24690], Loss: 1.572375, Validation F1 score: 0.312500, Average training f1 scores: 0.468750 \n",
            "Iterations [16620/24690], Loss: 1.229165, Validation F1 score: 0.312500, Average training f1 scores: 0.443750 \n",
            "Iterations [16630/24690], Loss: 1.977355, Validation F1 score: 0.187500, Average training f1 scores: 0.431250 \n",
            "Iterations [16640/24690], Loss: 1.734543, Validation F1 score: 0.375000, Average training f1 scores: 0.481250 \n",
            "Iterations [16650/24690], Loss: 1.096562, Validation F1 score: 0.375000, Average training f1 scores: 0.475000 \n",
            "Iterations [16660/24690], Loss: 1.205544, Validation F1 score: 0.500000, Average training f1 scores: 0.543750 \n",
            "Iterations [16670/24690], Loss: 1.833049, Validation F1 score: 0.250000, Average training f1 scores: 0.450000 \n",
            "Iterations [16680/24690], Loss: 1.422470, Validation F1 score: 0.500000, Average training f1 scores: 0.481250 \n",
            "Iterations [16690/24690], Loss: 1.291934, Validation F1 score: 0.625000, Average training f1 scores: 0.506250 \n",
            "Iterations [16700/24690], Loss: 1.479247, Validation F1 score: 0.250000, Average training f1 scores: 0.518750 \n",
            "Iterations [16710/24690], Loss: 1.519957, Validation F1 score: 0.437500, Average training f1 scores: 0.468750 \n",
            "Iterations [16720/24690], Loss: 1.451489, Validation F1 score: 0.312500, Average training f1 scores: 0.406250 \n",
            "Iterations [16730/24690], Loss: 2.194503, Validation F1 score: 0.187500, Average training f1 scores: 0.418750 \n",
            "Iterations [16740/24690], Loss: 1.132053, Validation F1 score: 0.375000, Average training f1 scores: 0.487500 \n",
            "Iterations [16750/24690], Loss: 1.488914, Validation F1 score: 0.187500, Average training f1 scores: 0.475000 \n",
            "Iterations [16760/24690], Loss: 1.420430, Validation F1 score: 0.312500, Average training f1 scores: 0.500000 \n",
            "Iterations [16770/24690], Loss: 1.726500, Validation F1 score: 0.312500, Average training f1 scores: 0.543750 \n",
            "Iterations [16780/24690], Loss: 1.595346, Validation F1 score: 0.250000, Average training f1 scores: 0.506250 \n",
            "Iterations [16790/24690], Loss: 1.944953, Validation F1 score: 0.187500, Average training f1 scores: 0.443750 \n",
            "Iterations [16800/24690], Loss: 1.729027, Validation F1 score: 0.187500, Average training f1 scores: 0.450000 \n",
            "Iterations [16810/24690], Loss: 1.542280, Validation F1 score: 0.312500, Average training f1 scores: 0.468750 \n",
            "Iterations [16820/24690], Loss: 1.555459, Validation F1 score: 0.312500, Average training f1 scores: 0.512500 \n",
            "Iterations [16830/24690], Loss: 2.273463, Validation F1 score: 0.312500, Average training f1 scores: 0.500000 \n",
            "Iterations [16840/24690], Loss: 1.460445, Validation F1 score: 0.500000, Average training f1 scores: 0.450000 \n",
            "Iterations [16850/24690], Loss: 2.146553, Validation F1 score: 0.250000, Average training f1 scores: 0.462500 \n",
            "Iterations [16860/24690], Loss: 1.102899, Validation F1 score: 0.437500, Average training f1 scores: 0.556250 \n",
            "Iterations [16870/24690], Loss: 1.595362, Validation F1 score: 0.250000, Average training f1 scores: 0.443750 \n",
            "Iterations [16880/24690], Loss: 1.591101, Validation F1 score: 0.437500, Average training f1 scores: 0.443750 \n",
            "Iterations [16890/24690], Loss: 1.912092, Validation F1 score: 0.250000, Average training f1 scores: 0.462500 \n",
            "Iterations [16900/24690], Loss: 1.700514, Validation F1 score: 0.437500, Average training f1 scores: 0.462500 \n",
            "Iterations [16910/24690], Loss: 1.547978, Validation F1 score: 0.437500, Average training f1 scores: 0.556250 \n",
            "Iterations [16920/24690], Loss: 1.713331, Validation F1 score: 0.250000, Average training f1 scores: 0.431250 \n",
            "Iterations [16930/24690], Loss: 1.288230, Validation F1 score: 0.250000, Average training f1 scores: 0.406250 \n",
            "Iterations [16940/24690], Loss: 1.738762, Validation F1 score: 0.312500, Average training f1 scores: 0.487500 \n",
            "Iterations [16950/24690], Loss: 1.190737, Validation F1 score: 0.500000, Average training f1 scores: 0.462500 \n",
            "Iterations [16960/24690], Loss: 1.284349, Validation F1 score: 0.437500, Average training f1 scores: 0.431250 \n",
            "Iterations [16970/24690], Loss: 1.684579, Validation F1 score: 0.375000, Average training f1 scores: 0.437500 \n",
            "Iterations [16980/24690], Loss: 1.335257, Validation F1 score: 0.437500, Average training f1 scores: 0.412500 \n",
            "Iterations [16990/24690], Loss: 1.426068, Validation F1 score: 0.500000, Average training f1 scores: 0.500000 \n",
            "Iterations [17000/24690], Loss: 1.558350, Validation F1 score: 0.312500, Average training f1 scores: 0.500000 \n",
            "Iterations [17010/24690], Loss: 1.218369, Validation F1 score: 0.312500, Average training f1 scores: 0.512500 \n",
            "Iterations [17020/24690], Loss: 1.333602, Validation F1 score: 0.562500, Average training f1 scores: 0.418750 \n",
            "Iterations [17030/24690], Loss: 1.286357, Validation F1 score: 0.312500, Average training f1 scores: 0.481250 \n",
            "Iterations [17040/24690], Loss: 1.588758, Validation F1 score: 0.312500, Average training f1 scores: 0.512500 \n",
            "Iterations [17050/24690], Loss: 1.653547, Validation F1 score: 0.375000, Average training f1 scores: 0.525000 \n",
            "Iterations [17060/24690], Loss: 1.236737, Validation F1 score: 0.375000, Average training f1 scores: 0.468750 \n",
            "Iterations [17070/24690], Loss: 1.711165, Validation F1 score: 0.375000, Average training f1 scores: 0.400000 \n",
            "Iterations [17080/24690], Loss: 1.144874, Validation F1 score: 0.375000, Average training f1 scores: 0.450000 \n",
            "Iterations [17090/24690], Loss: 1.467834, Validation F1 score: 0.500000, Average training f1 scores: 0.481250 \n",
            "Iterations [17100/24690], Loss: 1.449082, Validation F1 score: 0.437500, Average training f1 scores: 0.468750 \n",
            "Iterations [17110/24690], Loss: 1.607355, Validation F1 score: 0.187500, Average training f1 scores: 0.556250 \n",
            "Iterations [17120/24690], Loss: 1.421199, Validation F1 score: 0.437500, Average training f1 scores: 0.406250 \n",
            "Iterations [17130/24690], Loss: 1.416290, Validation F1 score: 0.500000, Average training f1 scores: 0.468750 \n",
            "Iterations [17140/24690], Loss: 1.222053, Validation F1 score: 0.500000, Average training f1 scores: 0.443750 \n",
            "Iterations [17150/24690], Loss: 1.713472, Validation F1 score: 0.250000, Average training f1 scores: 0.475000 \n",
            "Iterations [17160/24690], Loss: 1.602195, Validation F1 score: 0.062500, Average training f1 scores: 0.481250 \n",
            "Iterations [17170/24690], Loss: 1.893834, Validation F1 score: 0.250000, Average training f1 scores: 0.512500 \n",
            "Iterations [17180/24690], Loss: 2.238859, Validation F1 score: 0.125000, Average training f1 scores: 0.500000 \n",
            "Iterations [17190/24690], Loss: 1.274695, Validation F1 score: 0.375000, Average training f1 scores: 0.431250 \n",
            "Iterations [17200/24690], Loss: 1.597595, Validation F1 score: 0.375000, Average training f1 scores: 0.512500 \n",
            "Iterations [17210/24690], Loss: 1.736066, Validation F1 score: 0.312500, Average training f1 scores: 0.537500 \n",
            "Iterations [17220/24690], Loss: 1.308141, Validation F1 score: 0.437500, Average training f1 scores: 0.443750 \n",
            "Iterations [17230/24690], Loss: 1.409534, Validation F1 score: 0.375000, Average training f1 scores: 0.450000 \n",
            "Iterations [17240/24690], Loss: 1.606054, Validation F1 score: 0.250000, Average training f1 scores: 0.475000 \n",
            "Iterations [17250/24690], Loss: 1.971828, Validation F1 score: 0.187500, Average training f1 scores: 0.468750 \n",
            "Iterations [17260/24690], Loss: 1.389400, Validation F1 score: 0.375000, Average training f1 scores: 0.481250 \n",
            "Iterations [17270/24690], Loss: 1.547240, Validation F1 score: 0.437500, Average training f1 scores: 0.481250 \n",
            "Iterations [17280/24690], Loss: 1.192902, Validation F1 score: 0.500000, Average training f1 scores: 0.443750 \n",
            "Iterations [17290/24690], Loss: 1.289660, Validation F1 score: 0.312500, Average training f1 scores: 0.456250 \n",
            "Iterations [17300/24690], Loss: 1.266189, Validation F1 score: 0.437500, Average training f1 scores: 0.450000 \n",
            "Iterations [17310/24690], Loss: 1.484401, Validation F1 score: 0.500000, Average training f1 scores: 0.512500 \n",
            "Iterations [17320/24690], Loss: 1.394730, Validation F1 score: 0.187500, Average training f1 scores: 0.462500 \n",
            "Iterations [17330/24690], Loss: 1.780630, Validation F1 score: 0.250000, Average training f1 scores: 0.481250 \n",
            "Iterations [17340/24690], Loss: 1.657797, Validation F1 score: 0.312500, Average training f1 scores: 0.412500 \n",
            "Iterations [17350/24690], Loss: 1.257582, Validation F1 score: 0.125000, Average training f1 scores: 0.462500 \n",
            "Iterations [17360/24690], Loss: 1.185787, Validation F1 score: 0.500000, Average training f1 scores: 0.506250 \n",
            "Iterations [17370/24690], Loss: 1.961686, Validation F1 score: 0.312500, Average training f1 scores: 0.537500 \n",
            "Iterations [17380/24690], Loss: 1.529493, Validation F1 score: 0.500000, Average training f1 scores: 0.425000 \n",
            "Iterations [17390/24690], Loss: 1.271994, Validation F1 score: 0.562500, Average training f1 scores: 0.512500 \n",
            "Iterations [17400/24690], Loss: 1.818369, Validation F1 score: 0.312500, Average training f1 scores: 0.493750 \n",
            "Iterations [17410/24690], Loss: 1.100527, Validation F1 score: 0.625000, Average training f1 scores: 0.475000 \n",
            "Iterations [17420/24690], Loss: 1.080700, Validation F1 score: 0.625000, Average training f1 scores: 0.537500 \n",
            "Iterations [17430/24690], Loss: 1.504408, Validation F1 score: 0.562500, Average training f1 scores: 0.412500 \n",
            "Iterations [17440/24690], Loss: 1.539729, Validation F1 score: 0.375000, Average training f1 scores: 0.481250 \n",
            "Iterations [17450/24690], Loss: 1.571340, Validation F1 score: 0.250000, Average training f1 scores: 0.468750 \n",
            "Iterations [17460/24690], Loss: 1.432155, Validation F1 score: 0.375000, Average training f1 scores: 0.443750 \n",
            "Iterations [17470/24690], Loss: 1.422390, Validation F1 score: 0.250000, Average training f1 scores: 0.437500 \n",
            "Iterations [17480/24690], Loss: 1.521399, Validation F1 score: 0.500000, Average training f1 scores: 0.406250 \n",
            "Iterations [17490/24690], Loss: 1.263969, Validation F1 score: 0.312500, Average training f1 scores: 0.493750 \n",
            "Iterations [17500/24690], Loss: 1.533429, Validation F1 score: 0.312500, Average training f1 scores: 0.437500 \n",
            "Iterations [17510/24690], Loss: 1.535830, Validation F1 score: 0.437500, Average training f1 scores: 0.487500 \n",
            "Iterations [17520/24690], Loss: 1.136424, Validation F1 score: 0.500000, Average training f1 scores: 0.456250 \n",
            "Iterations [17530/24690], Loss: 2.074120, Validation F1 score: 0.312500, Average training f1 scores: 0.537500 \n",
            "Iterations [17540/24690], Loss: 1.521369, Validation F1 score: 0.437500, Average training f1 scores: 0.468750 \n",
            "Iterations [17550/24690], Loss: 1.486300, Validation F1 score: 0.250000, Average training f1 scores: 0.475000 \n",
            "Iterations [17560/24690], Loss: 1.448790, Validation F1 score: 0.250000, Average training f1 scores: 0.487500 \n",
            "Iterations [17570/24690], Loss: 1.471920, Validation F1 score: 0.312500, Average training f1 scores: 0.456250 \n",
            "Iterations [17580/24690], Loss: 1.901387, Validation F1 score: 0.187500, Average training f1 scores: 0.518750 \n",
            "Iterations [17590/24690], Loss: 1.525487, Validation F1 score: 0.375000, Average training f1 scores: 0.425000 \n",
            "Iterations [17600/24690], Loss: 1.653543, Validation F1 score: 0.375000, Average training f1 scores: 0.537500 \n",
            "Iterations [17610/24690], Loss: 1.255509, Validation F1 score: 0.437500, Average training f1 scores: 0.506250 \n",
            "Iterations [17620/24690], Loss: 1.624023, Validation F1 score: 0.187500, Average training f1 scores: 0.431250 \n",
            "Iterations [17630/24690], Loss: 1.665425, Validation F1 score: 0.437500, Average training f1 scores: 0.518750 \n",
            "Iterations [17640/24690], Loss: 1.377708, Validation F1 score: 0.375000, Average training f1 scores: 0.450000 \n",
            "Iterations [17650/24690], Loss: 1.620280, Validation F1 score: 0.375000, Average training f1 scores: 0.431250 \n",
            "Iterations [17660/24690], Loss: 1.553641, Validation F1 score: 0.437500, Average training f1 scores: 0.493750 \n",
            "Iterations [17670/24690], Loss: 1.805297, Validation F1 score: 0.312500, Average training f1 scores: 0.512500 \n",
            "Iterations [17680/24690], Loss: 1.498525, Validation F1 score: 0.375000, Average training f1 scores: 0.481250 \n",
            "Iterations [17690/24690], Loss: 1.732742, Validation F1 score: 0.250000, Average training f1 scores: 0.468750 \n",
            "Iterations [17700/24690], Loss: 1.225291, Validation F1 score: 0.437500, Average training f1 scores: 0.500000 \n",
            "Iterations [17710/24690], Loss: 1.344093, Validation F1 score: 0.375000, Average training f1 scores: 0.475000 \n",
            "Iterations [17720/24690], Loss: 1.704444, Validation F1 score: 0.125000, Average training f1 scores: 0.512500 \n",
            "Iterations [17730/24690], Loss: 1.401601, Validation F1 score: 0.500000, Average training f1 scores: 0.443750 \n",
            "Iterations [17740/24690], Loss: 1.751148, Validation F1 score: 0.375000, Average training f1 scores: 0.468750 \n",
            "Iterations [17750/24690], Loss: 1.620925, Validation F1 score: 0.312500, Average training f1 scores: 0.481250 \n",
            "Iterations [17760/24690], Loss: 1.793667, Validation F1 score: 0.437500, Average training f1 scores: 0.550000 \n",
            "Iterations [17770/24690], Loss: 1.014224, Validation F1 score: 0.500000, Average training f1 scores: 0.550000 \n",
            "Iterations [17780/24690], Loss: 1.337929, Validation F1 score: 0.375000, Average training f1 scores: 0.506250 \n",
            "Iterations [17790/24690], Loss: 1.846715, Validation F1 score: 0.250000, Average training f1 scores: 0.493750 \n",
            "Iterations [17800/24690], Loss: 1.995811, Validation F1 score: 0.250000, Average training f1 scores: 0.531250 \n",
            "Iterations [17810/24690], Loss: 1.470428, Validation F1 score: 0.437500, Average training f1 scores: 0.506250 \n",
            "Iterations [17820/24690], Loss: 1.260437, Validation F1 score: 0.250000, Average training f1 scores: 0.431250 \n",
            "Iterations [17830/24690], Loss: 1.124642, Validation F1 score: 0.437500, Average training f1 scores: 0.493750 \n",
            "Iterations [17840/24690], Loss: 1.526274, Validation F1 score: 0.437500, Average training f1 scores: 0.500000 \n",
            "Iterations [17850/24690], Loss: 1.542347, Validation F1 score: 0.312500, Average training f1 scores: 0.462500 \n",
            "Iterations [17860/24690], Loss: 1.563613, Validation F1 score: 0.375000, Average training f1 scores: 0.543750 \n",
            "Iterations [17870/24690], Loss: 1.214140, Validation F1 score: 0.375000, Average training f1 scores: 0.475000 \n",
            "Iterations [17880/24690], Loss: 1.622890, Validation F1 score: 0.312500, Average training f1 scores: 0.468750 \n",
            "Iterations [17890/24690], Loss: 1.565418, Validation F1 score: 0.437500, Average training f1 scores: 0.500000 \n",
            "Iterations [17900/24690], Loss: 1.515640, Validation F1 score: 0.250000, Average training f1 scores: 0.562500 \n",
            "Iterations [17910/24690], Loss: 1.955763, Validation F1 score: 0.312500, Average training f1 scores: 0.431250 \n",
            "Iterations [17920/24690], Loss: 1.540986, Validation F1 score: 0.250000, Average training f1 scores: 0.431250 \n",
            "Iterations [17930/24690], Loss: 1.963876, Validation F1 score: 0.500000, Average training f1 scores: 0.506250 \n",
            "Iterations [17940/24690], Loss: 1.959783, Validation F1 score: 0.250000, Average training f1 scores: 0.475000 \n",
            "Iterations [17950/24690], Loss: 1.349516, Validation F1 score: 0.437500, Average training f1 scores: 0.456250 \n",
            "Iterations [17960/24690], Loss: 1.338026, Validation F1 score: 0.250000, Average training f1 scores: 0.481250 \n",
            "Iterations [17970/24690], Loss: 1.522106, Validation F1 score: 0.312500, Average training f1 scores: 0.481250 \n",
            "Iterations [17980/24690], Loss: 1.684718, Validation F1 score: 0.375000, Average training f1 scores: 0.525000 \n",
            "Iterations [17990/24690], Loss: 1.540098, Validation F1 score: 0.375000, Average training f1 scores: 0.493750 \n",
            "Iterations [18000/24690], Loss: 1.627788, Validation F1 score: 0.437500, Average training f1 scores: 0.500000 \n",
            "Iterations [18010/24690], Loss: 1.230037, Validation F1 score: 0.437500, Average training f1 scores: 0.487500 \n",
            "Iterations [18020/24690], Loss: 1.471784, Validation F1 score: 0.500000, Average training f1 scores: 0.512500 \n",
            "Iterations [18030/24690], Loss: 0.835433, Validation F1 score: 0.625000, Average training f1 scores: 0.525000 \n",
            "Iterations [18040/24690], Loss: 1.622353, Validation F1 score: 0.312500, Average training f1 scores: 0.506250 \n",
            "Iterations [18050/24690], Loss: 1.533400, Validation F1 score: 0.375000, Average training f1 scores: 0.500000 \n",
            "Iterations [18060/24690], Loss: 2.040450, Validation F1 score: 0.187500, Average training f1 scores: 0.456250 \n",
            "Iterations [18070/24690], Loss: 1.851353, Validation F1 score: 0.187500, Average training f1 scores: 0.450000 \n",
            "Iterations [18080/24690], Loss: 1.148499, Validation F1 score: 0.500000, Average training f1 scores: 0.500000 \n",
            "Iterations [18090/24690], Loss: 1.155757, Validation F1 score: 0.562500, Average training f1 scores: 0.487500 \n",
            "Iterations [18100/24690], Loss: 2.454649, Validation F1 score: 0.187500, Average training f1 scores: 0.493750 \n",
            "Iterations [18110/24690], Loss: 1.502949, Validation F1 score: 0.375000, Average training f1 scores: 0.493750 \n",
            "Iterations [18120/24690], Loss: 1.551379, Validation F1 score: 0.250000, Average training f1 scores: 0.425000 \n",
            "Iterations [18130/24690], Loss: 1.411102, Validation F1 score: 0.375000, Average training f1 scores: 0.487500 \n",
            "Iterations [18140/24690], Loss: 1.461411, Validation F1 score: 0.500000, Average training f1 scores: 0.468750 \n",
            "Iterations [18150/24690], Loss: 1.116873, Validation F1 score: 0.375000, Average training f1 scores: 0.450000 \n",
            "Iterations [18160/24690], Loss: 1.514122, Validation F1 score: 0.250000, Average training f1 scores: 0.450000 \n",
            "Iterations [18170/24690], Loss: 1.232506, Validation F1 score: 0.437500, Average training f1 scores: 0.468750 \n",
            "Iterations [18180/24690], Loss: 1.665112, Validation F1 score: 0.375000, Average training f1 scores: 0.475000 \n",
            "Iterations [18190/24690], Loss: 1.466750, Validation F1 score: 0.125000, Average training f1 scores: 0.437500 \n",
            "Iterations [18200/24690], Loss: 1.444284, Validation F1 score: 0.312500, Average training f1 scores: 0.518750 \n",
            "Iterations [18210/24690], Loss: 1.905965, Validation F1 score: 0.125000, Average training f1 scores: 0.487500 \n",
            "Iterations [18220/24690], Loss: 1.774623, Validation F1 score: 0.125000, Average training f1 scores: 0.456250 \n",
            "Iterations [18230/24690], Loss: 1.117234, Validation F1 score: 0.500000, Average training f1 scores: 0.531250 \n",
            "Iterations [18240/24690], Loss: 1.421295, Validation F1 score: 0.312500, Average training f1 scores: 0.487500 \n",
            "Iterations [18250/24690], Loss: 1.512028, Validation F1 score: 0.250000, Average training f1 scores: 0.493750 \n",
            "Iterations [18260/24690], Loss: 1.642681, Validation F1 score: 0.187500, Average training f1 scores: 0.456250 \n",
            "Iterations [18270/24690], Loss: 2.053436, Validation F1 score: 0.250000, Average training f1 scores: 0.487500 \n",
            "Iterations [18280/24690], Loss: 1.646878, Validation F1 score: 0.312500, Average training f1 scores: 0.506250 \n",
            "Iterations [18290/24690], Loss: 1.232573, Validation F1 score: 0.437500, Average training f1 scores: 0.481250 \n",
            "Iterations [18300/24690], Loss: 2.001471, Validation F1 score: 0.312500, Average training f1 scores: 0.500000 \n",
            "Iterations [18310/24690], Loss: 1.383524, Validation F1 score: 0.437500, Average training f1 scores: 0.475000 \n",
            "Iterations [18320/24690], Loss: 1.349570, Validation F1 score: 0.375000, Average training f1 scores: 0.512500 \n",
            "Iterations [18330/24690], Loss: 1.942359, Validation F1 score: 0.250000, Average training f1 scores: 0.462500 \n",
            "Iterations [18340/24690], Loss: 1.631299, Validation F1 score: 0.437500, Average training f1 scores: 0.400000 \n",
            "Iterations [18350/24690], Loss: 1.660462, Validation F1 score: 0.375000, Average training f1 scores: 0.418750 \n",
            "Iterations [18360/24690], Loss: 1.457207, Validation F1 score: 0.437500, Average training f1 scores: 0.525000 \n",
            "Iterations [18370/24690], Loss: 1.291959, Validation F1 score: 0.375000, Average training f1 scores: 0.518750 \n",
            "Iterations [18380/24690], Loss: 1.798339, Validation F1 score: 0.062500, Average training f1 scores: 0.456250 \n",
            "Iterations [18390/24690], Loss: 1.554301, Validation F1 score: 0.187500, Average training f1 scores: 0.531250 \n",
            "Iterations [18400/24690], Loss: 1.837195, Validation F1 score: 0.250000, Average training f1 scores: 0.412500 \n",
            "Iterations [18410/24690], Loss: 1.172565, Validation F1 score: 0.500000, Average training f1 scores: 0.481250 \n",
            "Iterations [18420/24690], Loss: 1.214440, Validation F1 score: 0.312500, Average training f1 scores: 0.481250 \n",
            "Iterations [18430/24690], Loss: 1.755257, Validation F1 score: 0.250000, Average training f1 scores: 0.506250 \n",
            "Iterations [18440/24690], Loss: 1.339145, Validation F1 score: 0.562500, Average training f1 scores: 0.387500 \n",
            "Iterations [18450/24690], Loss: 1.551011, Validation F1 score: 0.500000, Average training f1 scores: 0.531250 \n",
            "Iterations [18460/24690], Loss: 1.334898, Validation F1 score: 0.437500, Average training f1 scores: 0.518750 \n",
            "Iterations [18470/24690], Loss: 1.884936, Validation F1 score: 0.187500, Average training f1 scores: 0.475000 \n",
            "Iterations [18480/24690], Loss: 1.425084, Validation F1 score: 0.375000, Average training f1 scores: 0.400000 \n",
            "Iterations [18490/24690], Loss: 1.561602, Validation F1 score: 0.437500, Average training f1 scores: 0.512500 \n",
            "Iterations [18500/24690], Loss: 1.939269, Validation F1 score: 0.375000, Average training f1 scores: 0.518750 \n",
            "Iterations [18510/24690], Loss: 1.420524, Validation F1 score: 0.562500, Average training f1 scores: 0.512500 \n",
            "Iterations [18520/24690], Loss: 1.438196, Validation F1 score: 0.562500, Average training f1 scores: 0.468750 \n",
            "Iterations [18530/24690], Loss: 2.086406, Validation F1 score: 0.125000, Average training f1 scores: 0.412500 \n",
            "Iterations [18540/24690], Loss: 1.283203, Validation F1 score: 0.375000, Average training f1 scores: 0.500000 \n",
            "Iterations [18550/24690], Loss: 1.520389, Validation F1 score: 0.375000, Average training f1 scores: 0.462500 \n",
            "Iterations [18560/24690], Loss: 1.464055, Validation F1 score: 0.500000, Average training f1 scores: 0.475000 \n",
            "Iterations [18570/24690], Loss: 1.850806, Validation F1 score: 0.250000, Average training f1 scores: 0.425000 \n",
            "Iterations [18580/24690], Loss: 1.697091, Validation F1 score: 0.187500, Average training f1 scores: 0.518750 \n",
            "Iterations [18590/24690], Loss: 1.576971, Validation F1 score: 0.187500, Average training f1 scores: 0.512500 \n",
            "Iterations [18600/24690], Loss: 1.373533, Validation F1 score: 0.375000, Average training f1 scores: 0.562500 \n",
            "Iterations [18610/24690], Loss: 1.196042, Validation F1 score: 0.625000, Average training f1 scores: 0.462500 \n",
            "Iterations [18620/24690], Loss: 1.600664, Validation F1 score: 0.375000, Average training f1 scores: 0.475000 \n",
            "Iterations [18630/24690], Loss: 1.827173, Validation F1 score: 0.250000, Average training f1 scores: 0.437500 \n",
            "Iterations [18640/24690], Loss: 1.670789, Validation F1 score: 0.187500, Average training f1 scores: 0.475000 \n",
            "Iterations [18650/24690], Loss: 1.336993, Validation F1 score: 0.187500, Average training f1 scores: 0.462500 \n",
            "Iterations [18660/24690], Loss: 1.364463, Validation F1 score: 0.375000, Average training f1 scores: 0.512500 \n",
            "Iterations [18670/24690], Loss: 1.457754, Validation F1 score: 0.437500, Average training f1 scores: 0.500000 \n",
            "Iterations [18680/24690], Loss: 1.282019, Validation F1 score: 0.375000, Average training f1 scores: 0.481250 \n",
            "Iterations [18690/24690], Loss: 1.430103, Validation F1 score: 0.312500, Average training f1 scores: 0.500000 \n",
            "Iterations [18700/24690], Loss: 1.818052, Validation F1 score: 0.312500, Average training f1 scores: 0.562500 \n",
            "Iterations [18710/24690], Loss: 1.488998, Validation F1 score: 0.187500, Average training f1 scores: 0.512500 \n",
            "Iterations [18720/24690], Loss: 1.582560, Validation F1 score: 0.187500, Average training f1 scores: 0.493750 \n",
            "Iterations [18730/24690], Loss: 1.398980, Validation F1 score: 0.437500, Average training f1 scores: 0.475000 \n",
            "Iterations [18740/24690], Loss: 1.709169, Validation F1 score: 0.312500, Average training f1 scores: 0.412500 \n",
            "Iterations [18750/24690], Loss: 1.898541, Validation F1 score: 0.312500, Average training f1 scores: 0.487500 \n",
            "Iterations [18760/24690], Loss: 1.677653, Validation F1 score: 0.250000, Average training f1 scores: 0.518750 \n",
            "Iterations [18770/24690], Loss: 1.433015, Validation F1 score: 0.500000, Average training f1 scores: 0.487500 \n",
            "Iterations [18780/24690], Loss: 0.964860, Validation F1 score: 0.500000, Average training f1 scores: 0.450000 \n",
            "Iterations [18790/24690], Loss: 1.313053, Validation F1 score: 0.375000, Average training f1 scores: 0.412500 \n",
            "Iterations [18800/24690], Loss: 1.329723, Validation F1 score: 0.375000, Average training f1 scores: 0.418750 \n",
            "Iterations [18810/24690], Loss: 1.323816, Validation F1 score: 0.437500, Average training f1 scores: 0.456250 \n",
            "Iterations [18820/24690], Loss: 1.804199, Validation F1 score: 0.312500, Average training f1 scores: 0.506250 \n",
            "Iterations [18830/24690], Loss: 1.291924, Validation F1 score: 0.562500, Average training f1 scores: 0.518750 \n",
            "Iterations [18840/24690], Loss: 1.470383, Validation F1 score: 0.250000, Average training f1 scores: 0.475000 \n",
            "Iterations [18850/24690], Loss: 2.091517, Validation F1 score: 0.312500, Average training f1 scores: 0.543750 \n",
            "Iterations [18860/24690], Loss: 1.262941, Validation F1 score: 0.500000, Average training f1 scores: 0.556250 \n",
            "Iterations [18870/24690], Loss: 1.266379, Validation F1 score: 0.312500, Average training f1 scores: 0.518750 \n",
            "Iterations [18880/24690], Loss: 1.533627, Validation F1 score: 0.375000, Average training f1 scores: 0.462500 \n",
            "Iterations [18890/24690], Loss: 1.184891, Validation F1 score: 0.500000, Average training f1 scores: 0.581250 \n",
            "Iterations [18900/24690], Loss: 1.263433, Validation F1 score: 0.500000, Average training f1 scores: 0.462500 \n",
            "Iterations [18910/24690], Loss: 1.273038, Validation F1 score: 0.312500, Average training f1 scores: 0.493750 \n",
            "Iterations [18920/24690], Loss: 1.718880, Validation F1 score: 0.500000, Average training f1 scores: 0.512500 \n",
            "Iterations [18930/24690], Loss: 1.369322, Validation F1 score: 0.437500, Average training f1 scores: 0.493750 \n",
            "Iterations [18940/24690], Loss: 1.186355, Validation F1 score: 0.437500, Average training f1 scores: 0.512500 \n",
            "Iterations [18950/24690], Loss: 2.029322, Validation F1 score: 0.187500, Average training f1 scores: 0.506250 \n",
            "Iterations [18960/24690], Loss: 1.413960, Validation F1 score: 0.250000, Average training f1 scores: 0.462500 \n",
            "Iterations [18970/24690], Loss: 1.319130, Validation F1 score: 0.437500, Average training f1 scores: 0.512500 \n",
            "Iterations [18980/24690], Loss: 1.797772, Validation F1 score: 0.250000, Average training f1 scores: 0.450000 \n",
            "Iterations [18990/24690], Loss: 1.240086, Validation F1 score: 0.437500, Average training f1 scores: 0.537500 \n",
            "Iterations [19000/24690], Loss: 1.577350, Validation F1 score: 0.187500, Average training f1 scores: 0.475000 \n",
            "Iterations [19010/24690], Loss: 1.232641, Validation F1 score: 0.375000, Average training f1 scores: 0.481250 \n",
            "Iterations [19020/24690], Loss: 1.310523, Validation F1 score: 0.187500, Average training f1 scores: 0.475000 \n",
            "Iterations [19030/24690], Loss: 1.669759, Validation F1 score: 0.312500, Average training f1 scores: 0.475000 \n",
            "Iterations [19040/24690], Loss: 1.280225, Validation F1 score: 0.500000, Average training f1 scores: 0.462500 \n",
            "Iterations [19050/24690], Loss: 1.310393, Validation F1 score: 0.437500, Average training f1 scores: 0.487500 \n",
            "Iterations [19060/24690], Loss: 1.134264, Validation F1 score: 0.562500, Average training f1 scores: 0.512500 \n",
            "Iterations [19070/24690], Loss: 1.916672, Validation F1 score: 0.187500, Average training f1 scores: 0.500000 \n",
            "Iterations [19080/24690], Loss: 1.609447, Validation F1 score: 0.375000, Average training f1 scores: 0.493750 \n",
            "Iterations [19090/24690], Loss: 1.391543, Validation F1 score: 0.500000, Average training f1 scores: 0.406250 \n",
            "Iterations [19100/24690], Loss: 1.832372, Validation F1 score: 0.312500, Average training f1 scores: 0.500000 \n",
            "Iterations [19110/24690], Loss: 1.071194, Validation F1 score: 0.375000, Average training f1 scores: 0.587500 \n",
            "Iterations [19120/24690], Loss: 1.768265, Validation F1 score: 0.312500, Average training f1 scores: 0.462500 \n",
            "Iterations [19130/24690], Loss: 1.388908, Validation F1 score: 0.250000, Average training f1 scores: 0.418750 \n",
            "Iterations [19140/24690], Loss: 1.511894, Validation F1 score: 0.250000, Average training f1 scores: 0.487500 \n",
            "Iterations [19150/24690], Loss: 1.353597, Validation F1 score: 0.437500, Average training f1 scores: 0.550000 \n",
            "Iterations [19160/24690], Loss: 1.596997, Validation F1 score: 0.375000, Average training f1 scores: 0.481250 \n",
            "Iterations [19170/24690], Loss: 1.633201, Validation F1 score: 0.187500, Average training f1 scores: 0.475000 \n",
            "Iterations [19180/24690], Loss: 1.515756, Validation F1 score: 0.312500, Average training f1 scores: 0.493750 \n",
            "Iterations [19190/24690], Loss: 1.509121, Validation F1 score: 0.437500, Average training f1 scores: 0.462500 \n",
            "Iterations [19200/24690], Loss: 1.172101, Validation F1 score: 0.312500, Average training f1 scores: 0.450000 \n",
            "Iterations [19210/24690], Loss: 1.274861, Validation F1 score: 0.250000, Average training f1 scores: 0.450000 \n",
            "Iterations [19220/24690], Loss: 1.308613, Validation F1 score: 0.375000, Average training f1 scores: 0.543750 \n",
            "Iterations [19230/24690], Loss: 0.791670, Validation F1 score: 0.500000, Average training f1 scores: 0.418750 \n",
            "Iterations [19240/24690], Loss: 1.377504, Validation F1 score: 0.500000, Average training f1 scores: 0.468750 \n",
            "Iterations [19250/24690], Loss: 1.310360, Validation F1 score: 0.500000, Average training f1 scores: 0.543750 \n",
            "Iterations [19260/24690], Loss: 1.506025, Validation F1 score: 0.375000, Average training f1 scores: 0.531250 \n",
            "Iterations [19270/24690], Loss: 1.004668, Validation F1 score: 0.312500, Average training f1 scores: 0.512500 \n",
            "Iterations [19280/24690], Loss: 0.905174, Validation F1 score: 0.375000, Average training f1 scores: 0.487500 \n",
            "Iterations [19290/24690], Loss: 1.768471, Validation F1 score: 0.312500, Average training f1 scores: 0.518750 \n",
            "Iterations [19300/24690], Loss: 1.376575, Validation F1 score: 0.375000, Average training f1 scores: 0.481250 \n",
            "Iterations [19310/24690], Loss: 1.749586, Validation F1 score: 0.312500, Average training f1 scores: 0.437500 \n",
            "Iterations [19320/24690], Loss: 1.368228, Validation F1 score: 0.375000, Average training f1 scores: 0.456250 \n",
            "Iterations [19330/24690], Loss: 0.919411, Validation F1 score: 0.312500, Average training f1 scores: 0.518750 \n",
            "Iterations [19340/24690], Loss: 1.955004, Validation F1 score: 0.187500, Average training f1 scores: 0.518750 \n",
            "Iterations [19350/24690], Loss: 1.754263, Validation F1 score: 0.312500, Average training f1 scores: 0.506250 \n",
            "Iterations [19360/24690], Loss: 1.580451, Validation F1 score: 0.250000, Average training f1 scores: 0.543750 \n",
            "Iterations [19370/24690], Loss: 1.701152, Validation F1 score: 0.312500, Average training f1 scores: 0.512500 \n",
            "Iterations [19380/24690], Loss: 1.671471, Validation F1 score: 0.375000, Average training f1 scores: 0.437500 \n",
            "Iterations [19390/24690], Loss: 1.371699, Validation F1 score: 0.500000, Average training f1 scores: 0.487500 \n",
            "Iterations [19400/24690], Loss: 1.503417, Validation F1 score: 0.500000, Average training f1 scores: 0.493750 \n",
            "Iterations [19410/24690], Loss: 1.270786, Validation F1 score: 0.312500, Average training f1 scores: 0.475000 \n",
            "Iterations [19420/24690], Loss: 1.153166, Validation F1 score: 0.562500, Average training f1 scores: 0.450000 \n",
            "Iterations [19430/24690], Loss: 1.532355, Validation F1 score: 0.187500, Average training f1 scores: 0.431250 \n",
            "Iterations [19440/24690], Loss: 1.709914, Validation F1 score: 0.312500, Average training f1 scores: 0.450000 \n",
            "Iterations [19450/24690], Loss: 1.930742, Validation F1 score: 0.375000, Average training f1 scores: 0.468750 \n",
            "Iterations [19460/24690], Loss: 1.914988, Validation F1 score: 0.187500, Average training f1 scores: 0.500000 \n",
            "Iterations [19470/24690], Loss: 1.884781, Validation F1 score: 0.437500, Average training f1 scores: 0.537500 \n",
            "Iterations [19480/24690], Loss: 1.672648, Validation F1 score: 0.187500, Average training f1 scores: 0.462500 \n",
            "Iterations [19490/24690], Loss: 1.499788, Validation F1 score: 0.250000, Average training f1 scores: 0.525000 \n",
            "Iterations [19500/24690], Loss: 1.697335, Validation F1 score: 0.312500, Average training f1 scores: 0.475000 \n",
            "Iterations [19510/24690], Loss: 1.641041, Validation F1 score: 0.250000, Average training f1 scores: 0.506250 \n",
            "Iterations [19520/24690], Loss: 2.042877, Validation F1 score: 0.312500, Average training f1 scores: 0.506250 \n",
            "Iterations [19530/24690], Loss: 1.401720, Validation F1 score: 0.562500, Average training f1 scores: 0.487500 \n",
            "Iterations [19540/24690], Loss: 1.840073, Validation F1 score: 0.250000, Average training f1 scores: 0.431250 \n",
            "Iterations [19550/24690], Loss: 1.796788, Validation F1 score: 0.312500, Average training f1 scores: 0.450000 \n",
            "Iterations [19560/24690], Loss: 1.278839, Validation F1 score: 0.312500, Average training f1 scores: 0.437500 \n",
            "Iterations [19570/24690], Loss: 2.322058, Validation F1 score: 0.312500, Average training f1 scores: 0.462500 \n",
            "Iterations [19580/24690], Loss: 1.683444, Validation F1 score: 0.375000, Average training f1 scores: 0.437500 \n",
            "Iterations [19590/24690], Loss: 1.110752, Validation F1 score: 0.562500, Average training f1 scores: 0.406250 \n",
            "Iterations [19600/24690], Loss: 1.551456, Validation F1 score: 0.312500, Average training f1 scores: 0.381250 \n",
            "Iterations [19610/24690], Loss: 1.784494, Validation F1 score: 0.187500, Average training f1 scores: 0.450000 \n",
            "Iterations [19620/24690], Loss: 1.505879, Validation F1 score: 0.312500, Average training f1 scores: 0.412500 \n",
            "Iterations [19630/24690], Loss: 1.708436, Validation F1 score: 0.375000, Average training f1 scores: 0.500000 \n",
            "Iterations [19640/24690], Loss: 1.762185, Validation F1 score: 0.187500, Average training f1 scores: 0.425000 \n",
            "Iterations [19650/24690], Loss: 1.606413, Validation F1 score: 0.500000, Average training f1 scores: 0.487500 \n",
            "Iterations [19660/24690], Loss: 1.318647, Validation F1 score: 0.500000, Average training f1 scores: 0.456250 \n",
            "Iterations [19670/24690], Loss: 1.881295, Validation F1 score: 0.062500, Average training f1 scores: 0.512500 \n",
            "Iterations [19680/24690], Loss: 1.928428, Validation F1 score: 0.312500, Average training f1 scores: 0.456250 \n",
            "Iterations [19690/24690], Loss: 1.610277, Validation F1 score: 0.312500, Average training f1 scores: 0.487500 \n",
            "Iterations [19700/24690], Loss: 1.301904, Validation F1 score: 0.312500, Average training f1 scores: 0.468750 \n",
            "Iterations [19710/24690], Loss: 1.458524, Validation F1 score: 0.375000, Average training f1 scores: 0.500000 \n",
            "Iterations [19720/24690], Loss: 1.482464, Validation F1 score: 0.187500, Average training f1 scores: 0.512500 \n",
            "Iterations [19730/24690], Loss: 1.251342, Validation F1 score: 0.500000, Average training f1 scores: 0.468750 \n",
            "Iterations [19740/24690], Loss: 1.203741, Validation F1 score: 0.437500, Average training f1 scores: 0.443750 \n",
            "Iterations [19750/24690], Loss: 1.610907, Validation F1 score: 0.375000, Average training f1 scores: 0.406250 \n",
            "Iterations [19760/24690], Loss: 1.381256, Validation F1 score: 0.312500, Average training f1 scores: 0.506250 \n",
            "Iterations [19770/24690], Loss: 2.688528, Validation F1 score: 0.187500, Average training f1 scores: 0.456250 \n",
            "Iterations [19780/24690], Loss: 1.796270, Validation F1 score: 0.312500, Average training f1 scores: 0.418750 \n",
            "Iterations [19790/24690], Loss: 1.118619, Validation F1 score: 0.375000, Average training f1 scores: 0.431250 \n",
            "Iterations [19800/24690], Loss: 1.561933, Validation F1 score: 0.312500, Average training f1 scores: 0.462500 \n",
            "Iterations [19810/24690], Loss: 1.717527, Validation F1 score: 0.312500, Average training f1 scores: 0.543750 \n",
            "Iterations [19820/24690], Loss: 1.429319, Validation F1 score: 0.437500, Average training f1 scores: 0.481250 \n",
            "Iterations [19830/24690], Loss: 1.541326, Validation F1 score: 0.562500, Average training f1 scores: 0.543750 \n",
            "Iterations [19840/24690], Loss: 1.884080, Validation F1 score: 0.375000, Average training f1 scores: 0.481250 \n",
            "Iterations [19850/24690], Loss: 1.750879, Validation F1 score: 0.375000, Average training f1 scores: 0.431250 \n",
            "Iterations [19860/24690], Loss: 1.869278, Validation F1 score: 0.375000, Average training f1 scores: 0.575000 \n",
            "Iterations [19870/24690], Loss: 1.349591, Validation F1 score: 0.437500, Average training f1 scores: 0.468750 \n",
            "Iterations [19880/24690], Loss: 1.559218, Validation F1 score: 0.375000, Average training f1 scores: 0.462500 \n",
            "Iterations [19890/24690], Loss: 1.410209, Validation F1 score: 0.250000, Average training f1 scores: 0.437500 \n",
            "Iterations [19900/24690], Loss: 1.710698, Validation F1 score: 0.437500, Average training f1 scores: 0.550000 \n",
            "Iterations [19910/24690], Loss: 1.895676, Validation F1 score: 0.312500, Average training f1 scores: 0.500000 \n",
            "Iterations [19920/24690], Loss: 1.589492, Validation F1 score: 0.500000, Average training f1 scores: 0.487500 \n",
            "Iterations [19930/24690], Loss: 1.506791, Validation F1 score: 0.375000, Average training f1 scores: 0.506250 \n",
            "Iterations [19940/24690], Loss: 1.428421, Validation F1 score: 0.562500, Average training f1 scores: 0.506250 \n",
            "Iterations [19950/24690], Loss: 1.183888, Validation F1 score: 0.312500, Average training f1 scores: 0.537500 \n",
            "Iterations [19960/24690], Loss: 1.586811, Validation F1 score: 0.375000, Average training f1 scores: 0.543750 \n",
            "Iterations [19970/24690], Loss: 1.829524, Validation F1 score: 0.312500, Average training f1 scores: 0.525000 \n",
            "Iterations [19980/24690], Loss: 1.544424, Validation F1 score: 0.375000, Average training f1 scores: 0.506250 \n",
            "Iterations [19990/24690], Loss: 1.766874, Validation F1 score: 0.437500, Average training f1 scores: 0.418750 \n",
            "Iterations [20000/24690], Loss: 1.480558, Validation F1 score: 0.312500, Average training f1 scores: 0.481250 \n",
            "Iterations [20010/24690], Loss: 1.645940, Validation F1 score: 0.375000, Average training f1 scores: 0.481250 \n",
            "Iterations [20020/24690], Loss: 0.946373, Validation F1 score: 0.375000, Average training f1 scores: 0.431250 \n",
            "Iterations [20030/24690], Loss: 1.696827, Validation F1 score: 0.375000, Average training f1 scores: 0.468750 \n",
            "Iterations [20040/24690], Loss: 1.733508, Validation F1 score: 0.312500, Average training f1 scores: 0.487500 \n",
            "Iterations [20050/24690], Loss: 1.018221, Validation F1 score: 0.500000, Average training f1 scores: 0.443750 \n",
            "Iterations [20060/24690], Loss: 1.113835, Validation F1 score: 0.500000, Average training f1 scores: 0.493750 \n",
            "Iterations [20070/24690], Loss: 1.325741, Validation F1 score: 0.500000, Average training f1 scores: 0.500000 \n",
            "Iterations [20080/24690], Loss: 1.473698, Validation F1 score: 0.250000, Average training f1 scores: 0.456250 \n",
            "Iterations [20090/24690], Loss: 1.430711, Validation F1 score: 0.312500, Average training f1 scores: 0.481250 \n",
            "Iterations [20100/24690], Loss: 1.714788, Validation F1 score: 0.250000, Average training f1 scores: 0.393750 \n",
            "Iterations [20110/24690], Loss: 1.668618, Validation F1 score: 0.250000, Average training f1 scores: 0.531250 \n",
            "Iterations [20120/24690], Loss: 1.366809, Validation F1 score: 0.187500, Average training f1 scores: 0.468750 \n",
            "Iterations [20130/24690], Loss: 1.902044, Validation F1 score: 0.500000, Average training f1 scores: 0.468750 \n",
            "Iterations [20140/24690], Loss: 1.810175, Validation F1 score: 0.125000, Average training f1 scores: 0.493750 \n",
            "Iterations [20150/24690], Loss: 1.015122, Validation F1 score: 0.437500, Average training f1 scores: 0.450000 \n",
            "Iterations [20160/24690], Loss: 1.713757, Validation F1 score: 0.312500, Average training f1 scores: 0.418750 \n",
            "Iterations [20170/24690], Loss: 1.292429, Validation F1 score: 0.375000, Average training f1 scores: 0.506250 \n",
            "Iterations [20180/24690], Loss: 1.534798, Validation F1 score: 0.375000, Average training f1 scores: 0.481250 \n",
            "Iterations [20190/24690], Loss: 1.842352, Validation F1 score: 0.312500, Average training f1 scores: 0.562500 \n",
            "Iterations [20200/24690], Loss: 1.560682, Validation F1 score: 0.312500, Average training f1 scores: 0.468750 \n",
            "Iterations [20210/24690], Loss: 1.765326, Validation F1 score: 0.250000, Average training f1 scores: 0.600000 \n",
            "Iterations [20220/24690], Loss: 1.702302, Validation F1 score: 0.312500, Average training f1 scores: 0.518750 \n",
            "Iterations [20230/24690], Loss: 1.423511, Validation F1 score: 0.250000, Average training f1 scores: 0.412500 \n",
            "Iterations [20240/24690], Loss: 1.854393, Validation F1 score: 0.250000, Average training f1 scores: 0.462500 \n",
            "Iterations [20250/24690], Loss: 1.728927, Validation F1 score: 0.500000, Average training f1 scores: 0.506250 \n",
            "Iterations [20260/24690], Loss: 1.283265, Validation F1 score: 0.562500, Average training f1 scores: 0.481250 \n",
            "Iterations [20270/24690], Loss: 2.007379, Validation F1 score: 0.312500, Average training f1 scores: 0.475000 \n",
            "Iterations [20280/24690], Loss: 1.471605, Validation F1 score: 0.375000, Average training f1 scores: 0.468750 \n",
            "Iterations [20290/24690], Loss: 1.380603, Validation F1 score: 0.312500, Average training f1 scores: 0.462500 \n",
            "Iterations [20300/24690], Loss: 1.509327, Validation F1 score: 0.250000, Average training f1 scores: 0.493750 \n",
            "Iterations [20310/24690], Loss: 1.802624, Validation F1 score: 0.375000, Average training f1 scores: 0.425000 \n",
            "Iterations [20320/24690], Loss: 1.326813, Validation F1 score: 0.437500, Average training f1 scores: 0.462500 \n",
            "Iterations [20330/24690], Loss: 1.707170, Validation F1 score: 0.187500, Average training f1 scores: 0.506250 \n",
            "Iterations [20340/24690], Loss: 1.864278, Validation F1 score: 0.375000, Average training f1 scores: 0.450000 \n",
            "Iterations [20350/24690], Loss: 1.472599, Validation F1 score: 0.375000, Average training f1 scores: 0.518750 \n",
            "Iterations [20360/24690], Loss: 1.185061, Validation F1 score: 0.187500, Average training f1 scores: 0.468750 \n",
            "Iterations [20370/24690], Loss: 1.238332, Validation F1 score: 0.500000, Average training f1 scores: 0.481250 \n",
            "Iterations [20380/24690], Loss: 1.378572, Validation F1 score: 0.375000, Average training f1 scores: 0.462500 \n",
            "Iterations [20390/24690], Loss: 1.618514, Validation F1 score: 0.187500, Average training f1 scores: 0.468750 \n",
            "Iterations [20400/24690], Loss: 1.652950, Validation F1 score: 0.562500, Average training f1 scores: 0.518750 \n",
            "Iterations [20410/24690], Loss: 1.690890, Validation F1 score: 0.437500, Average training f1 scores: 0.487500 \n",
            "Iterations [20420/24690], Loss: 1.753670, Validation F1 score: 0.312500, Average training f1 scores: 0.443750 \n",
            "Iterations [20430/24690], Loss: 1.822253, Validation F1 score: 0.250000, Average training f1 scores: 0.393750 \n",
            "Iterations [20440/24690], Loss: 1.529640, Validation F1 score: 0.375000, Average training f1 scores: 0.450000 \n",
            "Iterations [20450/24690], Loss: 1.640907, Validation F1 score: 0.437500, Average training f1 scores: 0.487500 \n",
            "Iterations [20460/24690], Loss: 1.733391, Validation F1 score: 0.312500, Average training f1 scores: 0.425000 \n",
            "Iterations [20470/24690], Loss: 1.990591, Validation F1 score: 0.125000, Average training f1 scores: 0.531250 \n",
            "Iterations [20480/24690], Loss: 2.161438, Validation F1 score: 0.312500, Average training f1 scores: 0.562500 \n",
            "Iterations [20490/24690], Loss: 1.436713, Validation F1 score: 0.375000, Average training f1 scores: 0.468750 \n",
            "Iterations [20500/24690], Loss: 1.346568, Validation F1 score: 0.500000, Average training f1 scores: 0.450000 \n",
            "Iterations [20510/24690], Loss: 1.543165, Validation F1 score: 0.312500, Average training f1 scores: 0.462500 \n",
            "Iterations [20520/24690], Loss: 1.240054, Validation F1 score: 0.250000, Average training f1 scores: 0.512500 \n",
            "Iterations [20530/24690], Loss: 1.465294, Validation F1 score: 0.250000, Average training f1 scores: 0.512500 \n",
            "Iterations [20540/24690], Loss: 1.932560, Validation F1 score: 0.312500, Average training f1 scores: 0.387500 \n",
            "Iterations [20550/24690], Loss: 1.819479, Validation F1 score: 0.375000, Average training f1 scores: 0.468750 \n",
            "Iterations [20560/24690], Loss: 1.979032, Validation F1 score: 0.312500, Average training f1 scores: 0.406250 \n",
            "Iterations [20570/24690], Loss: 1.932478, Validation F1 score: 0.187500, Average training f1 scores: 0.500000 \n",
            "Iterations [20580/24690], Loss: 1.168169, Validation F1 score: 0.562500, Average training f1 scores: 0.381250 \n",
            "Iterations [20590/24690], Loss: 1.274083, Validation F1 score: 0.187500, Average training f1 scores: 0.487500 \n",
            "Iterations [20600/24690], Loss: 2.155832, Validation F1 score: 0.125000, Average training f1 scores: 0.437500 \n",
            "Iterations [20610/24690], Loss: 1.319536, Validation F1 score: 0.250000, Average training f1 scores: 0.468750 \n",
            "Iterations [20620/24690], Loss: 1.350426, Validation F1 score: 0.375000, Average training f1 scores: 0.462500 \n",
            "Iterations [20630/24690], Loss: 1.427014, Validation F1 score: 0.187500, Average training f1 scores: 0.506250 \n",
            "Iterations [20640/24690], Loss: 1.733731, Validation F1 score: 0.437500, Average training f1 scores: 0.468750 \n",
            "Iterations [20650/24690], Loss: 1.503516, Validation F1 score: 0.250000, Average training f1 scores: 0.487500 \n",
            "Iterations [20660/24690], Loss: 2.210192, Validation F1 score: 0.125000, Average training f1 scores: 0.487500 \n",
            "Iterations [20670/24690], Loss: 1.514854, Validation F1 score: 0.312500, Average training f1 scores: 0.406250 \n",
            "Iterations [20680/24690], Loss: 1.442542, Validation F1 score: 0.437500, Average training f1 scores: 0.412500 \n",
            "Iterations [20690/24690], Loss: 1.280410, Validation F1 score: 0.312500, Average training f1 scores: 0.481250 \n",
            "Iterations [20700/24690], Loss: 0.969580, Validation F1 score: 0.500000, Average training f1 scores: 0.481250 \n",
            "Iterations [20710/24690], Loss: 1.549102, Validation F1 score: 0.375000, Average training f1 scores: 0.500000 \n",
            "Iterations [20720/24690], Loss: 1.214352, Validation F1 score: 0.437500, Average training f1 scores: 0.468750 \n",
            "Iterations [20730/24690], Loss: 1.835738, Validation F1 score: 0.375000, Average training f1 scores: 0.475000 \n",
            "Iterations [20740/24690], Loss: 1.579296, Validation F1 score: 0.375000, Average training f1 scores: 0.462500 \n",
            "Iterations [20750/24690], Loss: 1.533893, Validation F1 score: 0.375000, Average training f1 scores: 0.425000 \n",
            "Iterations [20760/24690], Loss: 1.456863, Validation F1 score: 0.312500, Average training f1 scores: 0.412500 \n",
            "Iterations [20770/24690], Loss: 1.380391, Validation F1 score: 0.312500, Average training f1 scores: 0.400000 \n",
            "Iterations [20780/24690], Loss: 1.833227, Validation F1 score: 0.312500, Average training f1 scores: 0.531250 \n",
            "Iterations [20790/24690], Loss: 1.550337, Validation F1 score: 0.375000, Average training f1 scores: 0.481250 \n",
            "Iterations [20800/24690], Loss: 1.370792, Validation F1 score: 0.375000, Average training f1 scores: 0.468750 \n",
            "Iterations [20810/24690], Loss: 1.467348, Validation F1 score: 0.375000, Average training f1 scores: 0.456250 \n",
            "Iterations [20820/24690], Loss: 1.810531, Validation F1 score: 0.312500, Average training f1 scores: 0.475000 \n",
            "Iterations [20830/24690], Loss: 1.901143, Validation F1 score: 0.687500, Average training f1 scores: 0.537500 \n",
            "Iterations [20840/24690], Loss: 1.695808, Validation F1 score: 0.250000, Average training f1 scores: 0.443750 \n",
            "Iterations [20850/24690], Loss: 1.808062, Validation F1 score: 0.250000, Average training f1 scores: 0.462500 \n",
            "Iterations [20860/24690], Loss: 1.583048, Validation F1 score: 0.312500, Average training f1 scores: 0.506250 \n",
            "Iterations [20870/24690], Loss: 2.333163, Validation F1 score: 0.187500, Average training f1 scores: 0.487500 \n",
            "Iterations [20880/24690], Loss: 1.322370, Validation F1 score: 0.625000, Average training f1 scores: 0.493750 \n",
            "Iterations [20890/24690], Loss: 1.817238, Validation F1 score: 0.375000, Average training f1 scores: 0.437500 \n",
            "Iterations [20900/24690], Loss: 1.839812, Validation F1 score: 0.437500, Average training f1 scores: 0.456250 \n",
            "Iterations [20910/24690], Loss: 1.313632, Validation F1 score: 0.562500, Average training f1 scores: 0.456250 \n",
            "Iterations [20920/24690], Loss: 1.547472, Validation F1 score: 0.375000, Average training f1 scores: 0.487500 \n",
            "Iterations [20930/24690], Loss: 1.390260, Validation F1 score: 0.375000, Average training f1 scores: 0.537500 \n",
            "Iterations [20940/24690], Loss: 1.294482, Validation F1 score: 0.375000, Average training f1 scores: 0.506250 \n",
            "Iterations [20950/24690], Loss: 1.884428, Validation F1 score: 0.375000, Average training f1 scores: 0.556250 \n",
            "Iterations [20960/24690], Loss: 1.155349, Validation F1 score: 0.500000, Average training f1 scores: 0.437500 \n",
            "Iterations [20970/24690], Loss: 1.380933, Validation F1 score: 0.562500, Average training f1 scores: 0.475000 \n",
            "Iterations [20980/24690], Loss: 1.404463, Validation F1 score: 0.187500, Average training f1 scores: 0.537500 \n",
            "Iterations [20990/24690], Loss: 1.350957, Validation F1 score: 0.500000, Average training f1 scores: 0.443750 \n",
            "Iterations [21000/24690], Loss: 1.289182, Validation F1 score: 0.500000, Average training f1 scores: 0.493750 \n",
            "Iterations [21010/24690], Loss: 1.852897, Validation F1 score: 0.250000, Average training f1 scores: 0.581250 \n",
            "Iterations [21020/24690], Loss: 1.333743, Validation F1 score: 0.375000, Average training f1 scores: 0.481250 \n",
            "Iterations [21030/24690], Loss: 1.410157, Validation F1 score: 0.312500, Average training f1 scores: 0.506250 \n",
            "Iterations [21040/24690], Loss: 1.638602, Validation F1 score: 0.437500, Average training f1 scores: 0.506250 \n",
            "Iterations [21050/24690], Loss: 1.189143, Validation F1 score: 0.562500, Average training f1 scores: 0.512500 \n",
            "Iterations [21060/24690], Loss: 1.423549, Validation F1 score: 0.312500, Average training f1 scores: 0.481250 \n",
            "Iterations [21070/24690], Loss: 1.596045, Validation F1 score: 0.250000, Average training f1 scores: 0.518750 \n",
            "Iterations [21080/24690], Loss: 1.434734, Validation F1 score: 0.312500, Average training f1 scores: 0.487500 \n",
            "Iterations [21090/24690], Loss: 1.561426, Validation F1 score: 0.375000, Average training f1 scores: 0.500000 \n",
            "Iterations [21100/24690], Loss: 1.460754, Validation F1 score: 0.375000, Average training f1 scores: 0.481250 \n",
            "Iterations [21110/24690], Loss: 1.806952, Validation F1 score: 0.437500, Average training f1 scores: 0.462500 \n",
            "Iterations [21120/24690], Loss: 1.745426, Validation F1 score: 0.250000, Average training f1 scores: 0.531250 \n",
            "Iterations [21130/24690], Loss: 1.199052, Validation F1 score: 0.500000, Average training f1 scores: 0.500000 \n",
            "Iterations [21140/24690], Loss: 1.809208, Validation F1 score: 0.500000, Average training f1 scores: 0.475000 \n",
            "Iterations [21150/24690], Loss: 1.500958, Validation F1 score: 0.437500, Average training f1 scores: 0.468750 \n",
            "Iterations [21160/24690], Loss: 1.559484, Validation F1 score: 0.437500, Average training f1 scores: 0.481250 \n",
            "Iterations [21170/24690], Loss: 1.682213, Validation F1 score: 0.312500, Average training f1 scores: 0.487500 \n",
            "Iterations [21180/24690], Loss: 1.418189, Validation F1 score: 0.250000, Average training f1 scores: 0.493750 \n",
            "Iterations [21190/24690], Loss: 1.427920, Validation F1 score: 0.437500, Average training f1 scores: 0.456250 \n",
            "Iterations [21200/24690], Loss: 1.326896, Validation F1 score: 0.437500, Average training f1 scores: 0.437500 \n",
            "Iterations [21210/24690], Loss: 1.326205, Validation F1 score: 0.250000, Average training f1 scores: 0.431250 \n",
            "Iterations [21220/24690], Loss: 1.362318, Validation F1 score: 0.375000, Average training f1 scores: 0.437500 \n",
            "Iterations [21230/24690], Loss: 1.431194, Validation F1 score: 0.250000, Average training f1 scores: 0.431250 \n",
            "Iterations [21240/24690], Loss: 1.890711, Validation F1 score: 0.250000, Average training f1 scores: 0.381250 \n",
            "Iterations [21250/24690], Loss: 1.153091, Validation F1 score: 0.437500, Average training f1 scores: 0.437500 \n",
            "Iterations [21260/24690], Loss: 1.386150, Validation F1 score: 0.250000, Average training f1 scores: 0.500000 \n",
            "Iterations [21270/24690], Loss: 1.600076, Validation F1 score: 0.437500, Average training f1 scores: 0.493750 \n",
            "Iterations [21280/24690], Loss: 0.947342, Validation F1 score: 0.500000, Average training f1 scores: 0.475000 \n",
            "Iterations [21290/24690], Loss: 2.082409, Validation F1 score: 0.312500, Average training f1 scores: 0.512500 \n",
            "Iterations [21300/24690], Loss: 1.743190, Validation F1 score: 0.250000, Average training f1 scores: 0.512500 \n",
            "Iterations [21310/24690], Loss: 1.140039, Validation F1 score: 0.375000, Average training f1 scores: 0.450000 \n",
            "Iterations [21320/24690], Loss: 1.296892, Validation F1 score: 0.437500, Average training f1 scores: 0.481250 \n",
            "Iterations [21330/24690], Loss: 1.622212, Validation F1 score: 0.125000, Average training f1 scores: 0.512500 \n",
            "Iterations [21340/24690], Loss: 1.672718, Validation F1 score: 0.250000, Average training f1 scores: 0.468750 \n",
            "Iterations [21350/24690], Loss: 1.465775, Validation F1 score: 0.375000, Average training f1 scores: 0.525000 \n",
            "Iterations [21360/24690], Loss: 1.327348, Validation F1 score: 0.437500, Average training f1 scores: 0.462500 \n",
            "Iterations [21370/24690], Loss: 1.326841, Validation F1 score: 0.375000, Average training f1 scores: 0.512500 \n",
            "Iterations [21380/24690], Loss: 1.724141, Validation F1 score: 0.375000, Average training f1 scores: 0.443750 \n",
            "Iterations [21390/24690], Loss: 1.702885, Validation F1 score: 0.437500, Average training f1 scores: 0.537500 \n",
            "Iterations [21400/24690], Loss: 1.556597, Validation F1 score: 0.312500, Average training f1 scores: 0.543750 \n",
            "Iterations [21410/24690], Loss: 1.479023, Validation F1 score: 0.375000, Average training f1 scores: 0.450000 \n",
            "Iterations [21420/24690], Loss: 1.800388, Validation F1 score: 0.187500, Average training f1 scores: 0.512500 \n",
            "Iterations [21430/24690], Loss: 1.713466, Validation F1 score: 0.312500, Average training f1 scores: 0.493750 \n",
            "Iterations [21440/24690], Loss: 1.379364, Validation F1 score: 0.250000, Average training f1 scores: 0.512500 \n",
            "Iterations [21450/24690], Loss: 1.331070, Validation F1 score: 0.125000, Average training f1 scores: 0.506250 \n",
            "Iterations [21460/24690], Loss: 1.947077, Validation F1 score: 0.312500, Average training f1 scores: 0.500000 \n",
            "Iterations [21470/24690], Loss: 1.037780, Validation F1 score: 0.375000, Average training f1 scores: 0.437500 \n",
            "Iterations [21480/24690], Loss: 1.715019, Validation F1 score: 0.312500, Average training f1 scores: 0.437500 \n",
            "Iterations [21490/24690], Loss: 1.890503, Validation F1 score: 0.312500, Average training f1 scores: 0.543750 \n",
            "Iterations [21500/24690], Loss: 1.450896, Validation F1 score: 0.312500, Average training f1 scores: 0.481250 \n",
            "Iterations [21510/24690], Loss: 1.702565, Validation F1 score: 0.437500, Average training f1 scores: 0.431250 \n",
            "Iterations [21520/24690], Loss: 1.578358, Validation F1 score: 0.250000, Average training f1 scores: 0.450000 \n",
            "Iterations [21530/24690], Loss: 1.458212, Validation F1 score: 0.312500, Average training f1 scores: 0.443750 \n",
            "Iterations [21540/24690], Loss: 1.416239, Validation F1 score: 0.375000, Average training f1 scores: 0.481250 \n",
            "Iterations [21550/24690], Loss: 1.321853, Validation F1 score: 0.437500, Average training f1 scores: 0.425000 \n",
            "Iterations [21560/24690], Loss: 1.137183, Validation F1 score: 0.562500, Average training f1 scores: 0.437500 \n",
            "Iterations [21570/24690], Loss: 1.601436, Validation F1 score: 0.250000, Average training f1 scores: 0.456250 \n",
            "Iterations [21580/24690], Loss: 1.232866, Validation F1 score: 0.312500, Average training f1 scores: 0.556250 \n",
            "Iterations [21590/24690], Loss: 1.604599, Validation F1 score: 0.187500, Average training f1 scores: 0.406250 \n",
            "Iterations [21600/24690], Loss: 1.403821, Validation F1 score: 0.500000, Average training f1 scores: 0.462500 \n",
            "Iterations [21610/24690], Loss: 1.577381, Validation F1 score: 0.375000, Average training f1 scores: 0.462500 \n",
            "Iterations [21620/24690], Loss: 1.494223, Validation F1 score: 0.437500, Average training f1 scores: 0.493750 \n",
            "Iterations [21630/24690], Loss: 1.071849, Validation F1 score: 0.375000, Average training f1 scores: 0.481250 \n",
            "Iterations [21640/24690], Loss: 1.277375, Validation F1 score: 0.187500, Average training f1 scores: 0.500000 \n",
            "Iterations [21650/24690], Loss: 1.556645, Validation F1 score: 0.375000, Average training f1 scores: 0.487500 \n",
            "Iterations [21660/24690], Loss: 1.494753, Validation F1 score: 0.375000, Average training f1 scores: 0.506250 \n",
            "Iterations [21670/24690], Loss: 1.962792, Validation F1 score: 0.125000, Average training f1 scores: 0.425000 \n",
            "Iterations [21680/24690], Loss: 1.683864, Validation F1 score: 0.312500, Average training f1 scores: 0.506250 \n",
            "Iterations [21690/24690], Loss: 1.693309, Validation F1 score: 0.312500, Average training f1 scores: 0.562500 \n",
            "Iterations [21700/24690], Loss: 1.250810, Validation F1 score: 0.500000, Average training f1 scores: 0.481250 \n",
            "Iterations [21710/24690], Loss: 1.015543, Validation F1 score: 0.437500, Average training f1 scores: 0.487500 \n",
            "Iterations [21720/24690], Loss: 1.422168, Validation F1 score: 0.250000, Average training f1 scores: 0.425000 \n",
            "Iterations [21730/24690], Loss: 1.854591, Validation F1 score: 0.125000, Average training f1 scores: 0.412500 \n",
            "Iterations [21740/24690], Loss: 1.568334, Validation F1 score: 0.062500, Average training f1 scores: 0.518750 \n",
            "Iterations [21750/24690], Loss: 1.409429, Validation F1 score: 0.437500, Average training f1 scores: 0.512500 \n",
            "Iterations [21760/24690], Loss: 1.802367, Validation F1 score: 0.375000, Average training f1 scores: 0.512500 \n",
            "Iterations [21770/24690], Loss: 1.181425, Validation F1 score: 0.500000, Average training f1 scores: 0.450000 \n",
            "Iterations [21780/24690], Loss: 1.496390, Validation F1 score: 0.375000, Average training f1 scores: 0.462500 \n",
            "Iterations [21790/24690], Loss: 1.496172, Validation F1 score: 0.125000, Average training f1 scores: 0.481250 \n",
            "Iterations [21800/24690], Loss: 1.550745, Validation F1 score: 0.562500, Average training f1 scores: 0.500000 \n",
            "Iterations [21810/24690], Loss: 1.338629, Validation F1 score: 0.250000, Average training f1 scores: 0.456250 \n",
            "Iterations [21820/24690], Loss: 1.112946, Validation F1 score: 0.437500, Average training f1 scores: 0.481250 \n",
            "Iterations [21830/24690], Loss: 1.628888, Validation F1 score: 0.250000, Average training f1 scores: 0.437500 \n",
            "Iterations [21840/24690], Loss: 1.510814, Validation F1 score: 0.250000, Average training f1 scores: 0.456250 \n",
            "Iterations [21850/24690], Loss: 1.143311, Validation F1 score: 0.437500, Average training f1 scores: 0.518750 \n",
            "Iterations [21860/24690], Loss: 1.441661, Validation F1 score: 0.375000, Average training f1 scores: 0.481250 \n",
            "Iterations [21870/24690], Loss: 1.551672, Validation F1 score: 0.437500, Average training f1 scores: 0.462500 \n",
            "Iterations [21880/24690], Loss: 1.664367, Validation F1 score: 0.250000, Average training f1 scores: 0.400000 \n",
            "Iterations [21890/24690], Loss: 1.278914, Validation F1 score: 0.437500, Average training f1 scores: 0.475000 \n",
            "Iterations [21900/24690], Loss: 1.788255, Validation F1 score: 0.375000, Average training f1 scores: 0.493750 \n",
            "Iterations [21910/24690], Loss: 1.577819, Validation F1 score: 0.312500, Average training f1 scores: 0.475000 \n",
            "Iterations [21920/24690], Loss: 1.233236, Validation F1 score: 0.187500, Average training f1 scores: 0.506250 \n",
            "Iterations [21930/24690], Loss: 1.100670, Validation F1 score: 0.437500, Average training f1 scores: 0.481250 \n",
            "Iterations [21940/24690], Loss: 2.196592, Validation F1 score: 0.187500, Average training f1 scores: 0.481250 \n",
            "Iterations [21950/24690], Loss: 1.459943, Validation F1 score: 0.312500, Average training f1 scores: 0.512500 \n",
            "Iterations [21960/24690], Loss: 1.561815, Validation F1 score: 0.062500, Average training f1 scores: 0.512500 \n",
            "Iterations [21970/24690], Loss: 1.716805, Validation F1 score: 0.437500, Average training f1 scores: 0.418750 \n",
            "Iterations [21980/24690], Loss: 1.580333, Validation F1 score: 0.187500, Average training f1 scores: 0.468750 \n",
            "Iterations [21990/24690], Loss: 1.935247, Validation F1 score: 0.187500, Average training f1 scores: 0.487500 \n",
            "Iterations [22000/24690], Loss: 1.972683, Validation F1 score: 0.437500, Average training f1 scores: 0.481250 \n",
            "Iterations [22010/24690], Loss: 1.353175, Validation F1 score: 0.312500, Average training f1 scores: 0.475000 \n",
            "Iterations [22020/24690], Loss: 1.734893, Validation F1 score: 0.187500, Average training f1 scores: 0.475000 \n",
            "Iterations [22030/24690], Loss: 1.590213, Validation F1 score: 0.437500, Average training f1 scores: 0.606250 \n",
            "Iterations [22040/24690], Loss: 1.328283, Validation F1 score: 0.562500, Average training f1 scores: 0.518750 \n",
            "Iterations [22050/24690], Loss: 1.510842, Validation F1 score: 0.312500, Average training f1 scores: 0.518750 \n",
            "Iterations [22060/24690], Loss: 1.560770, Validation F1 score: 0.312500, Average training f1 scores: 0.443750 \n",
            "Iterations [22070/24690], Loss: 1.347166, Validation F1 score: 0.250000, Average training f1 scores: 0.518750 \n",
            "Iterations [22080/24690], Loss: 1.538719, Validation F1 score: 0.250000, Average training f1 scores: 0.556250 \n",
            "Iterations [22090/24690], Loss: 1.604040, Validation F1 score: 0.250000, Average training f1 scores: 0.481250 \n",
            "Iterations [22100/24690], Loss: 1.282977, Validation F1 score: 0.187500, Average training f1 scores: 0.450000 \n",
            "Iterations [22110/24690], Loss: 1.579513, Validation F1 score: 0.437500, Average training f1 scores: 0.475000 \n",
            "Iterations [22120/24690], Loss: 1.347977, Validation F1 score: 0.250000, Average training f1 scores: 0.462500 \n",
            "Iterations [22130/24690], Loss: 1.707330, Validation F1 score: 0.312500, Average training f1 scores: 0.468750 \n",
            "Iterations [22140/24690], Loss: 1.428893, Validation F1 score: 0.375000, Average training f1 scores: 0.475000 \n",
            "Iterations [22150/24690], Loss: 1.985052, Validation F1 score: 0.187500, Average training f1 scores: 0.543750 \n",
            "Iterations [22160/24690], Loss: 1.582433, Validation F1 score: 0.375000, Average training f1 scores: 0.456250 \n",
            "Iterations [22170/24690], Loss: 1.733779, Validation F1 score: 0.250000, Average training f1 scores: 0.400000 \n",
            "Iterations [22180/24690], Loss: 0.951265, Validation F1 score: 0.312500, Average training f1 scores: 0.512500 \n",
            "Iterations [22190/24690], Loss: 1.534522, Validation F1 score: 0.375000, Average training f1 scores: 0.493750 \n",
            "Iterations [22200/24690], Loss: 2.024827, Validation F1 score: 0.062500, Average training f1 scores: 0.462500 \n",
            "Iterations [22210/24690], Loss: 1.430393, Validation F1 score: 0.500000, Average training f1 scores: 0.418750 \n",
            "Iterations [22220/24690], Loss: 1.700423, Validation F1 score: 0.250000, Average training f1 scores: 0.493750 \n",
            "Iterations [22230/24690], Loss: 1.125156, Validation F1 score: 0.562500, Average training f1 scores: 0.487500 \n",
            "Iterations [22240/24690], Loss: 1.629813, Validation F1 score: 0.312500, Average training f1 scores: 0.462500 \n",
            "Iterations [22250/24690], Loss: 1.770494, Validation F1 score: 0.250000, Average training f1 scores: 0.443750 \n",
            "Iterations [22260/24690], Loss: 1.298616, Validation F1 score: 0.562500, Average training f1 scores: 0.500000 \n",
            "Iterations [22270/24690], Loss: 1.751705, Validation F1 score: 0.250000, Average training f1 scores: 0.406250 \n",
            "Iterations [22280/24690], Loss: 1.024581, Validation F1 score: 0.500000, Average training f1 scores: 0.581250 \n",
            "Iterations [22290/24690], Loss: 1.436756, Validation F1 score: 0.312500, Average training f1 scores: 0.493750 \n",
            "Iterations [22300/24690], Loss: 1.408173, Validation F1 score: 0.625000, Average training f1 scores: 0.412500 \n",
            "Iterations [22310/24690], Loss: 1.391671, Validation F1 score: 0.375000, Average training f1 scores: 0.462500 \n",
            "Iterations [22320/24690], Loss: 1.126501, Validation F1 score: 0.375000, Average training f1 scores: 0.400000 \n",
            "Iterations [22330/24690], Loss: 1.559382, Validation F1 score: 0.312500, Average training f1 scores: 0.450000 \n",
            "Iterations [22340/24690], Loss: 1.599708, Validation F1 score: 0.250000, Average training f1 scores: 0.431250 \n",
            "Iterations [22350/24690], Loss: 1.321307, Validation F1 score: 0.437500, Average training f1 scores: 0.506250 \n",
            "Iterations [22360/24690], Loss: 1.440470, Validation F1 score: 0.500000, Average training f1 scores: 0.475000 \n",
            "Iterations [22370/24690], Loss: 1.737206, Validation F1 score: 0.375000, Average training f1 scores: 0.450000 \n",
            "Iterations [22380/24690], Loss: 1.806325, Validation F1 score: 0.125000, Average training f1 scores: 0.562500 \n",
            "Iterations [22390/24690], Loss: 1.866701, Validation F1 score: 0.187500, Average training f1 scores: 0.493750 \n",
            "Iterations [22400/24690], Loss: 1.386410, Validation F1 score: 0.500000, Average training f1 scores: 0.487500 \n",
            "Iterations [22410/24690], Loss: 1.545243, Validation F1 score: 0.250000, Average training f1 scores: 0.500000 \n",
            "Iterations [22420/24690], Loss: 1.549829, Validation F1 score: 0.312500, Average training f1 scores: 0.500000 \n",
            "Iterations [22430/24690], Loss: 1.643081, Validation F1 score: 0.312500, Average training f1 scores: 0.450000 \n",
            "Iterations [22440/24690], Loss: 1.719829, Validation F1 score: 0.312500, Average training f1 scores: 0.525000 \n",
            "Iterations [22450/24690], Loss: 1.660218, Validation F1 score: 0.312500, Average training f1 scores: 0.481250 \n",
            "Iterations [22460/24690], Loss: 1.914923, Validation F1 score: 0.250000, Average training f1 scores: 0.518750 \n",
            "Iterations [22470/24690], Loss: 1.490192, Validation F1 score: 0.187500, Average training f1 scores: 0.462500 \n",
            "Iterations [22480/24690], Loss: 1.679505, Validation F1 score: 0.500000, Average training f1 scores: 0.537500 \n",
            "Iterations [22490/24690], Loss: 1.493549, Validation F1 score: 0.375000, Average training f1 scores: 0.556250 \n",
            "Iterations [22500/24690], Loss: 1.712240, Validation F1 score: 0.187500, Average training f1 scores: 0.456250 \n",
            "Iterations [22510/24690], Loss: 1.427916, Validation F1 score: 0.312500, Average training f1 scores: 0.481250 \n",
            "Iterations [22520/24690], Loss: 1.299421, Validation F1 score: 0.312500, Average training f1 scores: 0.487500 \n",
            "Iterations [22530/24690], Loss: 1.877219, Validation F1 score: 0.250000, Average training f1 scores: 0.450000 \n",
            "Iterations [22540/24690], Loss: 1.568702, Validation F1 score: 0.187500, Average training f1 scores: 0.462500 \n",
            "Iterations [22550/24690], Loss: 1.882859, Validation F1 score: 0.125000, Average training f1 scores: 0.487500 \n",
            "Iterations [22560/24690], Loss: 1.339197, Validation F1 score: 0.375000, Average training f1 scores: 0.468750 \n",
            "Iterations [22570/24690], Loss: 1.982236, Validation F1 score: 0.187500, Average training f1 scores: 0.550000 \n",
            "Iterations [22580/24690], Loss: 1.365873, Validation F1 score: 0.312500, Average training f1 scores: 0.443750 \n",
            "Iterations [22590/24690], Loss: 1.017336, Validation F1 score: 0.500000, Average training f1 scores: 0.500000 \n",
            "Iterations [22600/24690], Loss: 1.388804, Validation F1 score: 0.125000, Average training f1 scores: 0.456250 \n",
            "Iterations [22610/24690], Loss: 1.603102, Validation F1 score: 0.375000, Average training f1 scores: 0.512500 \n",
            "Iterations [22620/24690], Loss: 1.375917, Validation F1 score: 0.375000, Average training f1 scores: 0.481250 \n",
            "Iterations [22630/24690], Loss: 1.392838, Validation F1 score: 0.437500, Average training f1 scores: 0.437500 \n",
            "Iterations [22640/24690], Loss: 1.481573, Validation F1 score: 0.375000, Average training f1 scores: 0.456250 \n",
            "Iterations [22650/24690], Loss: 1.593756, Validation F1 score: 0.250000, Average training f1 scores: 0.462500 \n",
            "Iterations [22660/24690], Loss: 1.750849, Validation F1 score: 0.062500, Average training f1 scores: 0.468750 \n",
            "Iterations [22670/24690], Loss: 1.439918, Validation F1 score: 0.375000, Average training f1 scores: 0.475000 \n",
            "Iterations [22680/24690], Loss: 1.523516, Validation F1 score: 0.375000, Average training f1 scores: 0.475000 \n",
            "Iterations [22690/24690], Loss: 1.280795, Validation F1 score: 0.437500, Average training f1 scores: 0.525000 \n",
            "Iterations [22700/24690], Loss: 1.215759, Validation F1 score: 0.375000, Average training f1 scores: 0.500000 \n",
            "Iterations [22710/24690], Loss: 1.917452, Validation F1 score: 0.312500, Average training f1 scores: 0.443750 \n",
            "Iterations [22720/24690], Loss: 1.589554, Validation F1 score: 0.312500, Average training f1 scores: 0.525000 \n",
            "Iterations [22730/24690], Loss: 1.477538, Validation F1 score: 0.437500, Average training f1 scores: 0.487500 \n",
            "Iterations [22740/24690], Loss: 1.516508, Validation F1 score: 0.312500, Average training f1 scores: 0.456250 \n",
            "Iterations [22750/24690], Loss: 1.190602, Validation F1 score: 0.375000, Average training f1 scores: 0.475000 \n",
            "Iterations [22760/24690], Loss: 1.694867, Validation F1 score: 0.312500, Average training f1 scores: 0.481250 \n",
            "Iterations [22770/24690], Loss: 1.162857, Validation F1 score: 0.375000, Average training f1 scores: 0.493750 \n",
            "Iterations [22780/24690], Loss: 1.122453, Validation F1 score: 0.625000, Average training f1 scores: 0.512500 \n",
            "Iterations [22790/24690], Loss: 1.406702, Validation F1 score: 0.250000, Average training f1 scores: 0.443750 \n",
            "Iterations [22800/24690], Loss: 1.456482, Validation F1 score: 0.437500, Average training f1 scores: 0.506250 \n",
            "Iterations [22810/24690], Loss: 1.471771, Validation F1 score: 0.500000, Average training f1 scores: 0.506250 \n",
            "Iterations [22820/24690], Loss: 1.559750, Validation F1 score: 0.375000, Average training f1 scores: 0.500000 \n",
            "Iterations [22830/24690], Loss: 1.496921, Validation F1 score: 0.375000, Average training f1 scores: 0.500000 \n",
            "Iterations [22840/24690], Loss: 1.472062, Validation F1 score: 0.375000, Average training f1 scores: 0.450000 \n",
            "Iterations [22850/24690], Loss: 1.646684, Validation F1 score: 0.437500, Average training f1 scores: 0.468750 \n",
            "Iterations [22860/24690], Loss: 1.568361, Validation F1 score: 0.312500, Average training f1 scores: 0.475000 \n",
            "Iterations [22870/24690], Loss: 1.120597, Validation F1 score: 0.250000, Average training f1 scores: 0.468750 \n",
            "Iterations [22880/24690], Loss: 1.496126, Validation F1 score: 0.375000, Average training f1 scores: 0.506250 \n",
            "Iterations [22890/24690], Loss: 1.444366, Validation F1 score: 0.375000, Average training f1 scores: 0.518750 \n",
            "Iterations [22900/24690], Loss: 1.616768, Validation F1 score: 0.125000, Average training f1 scores: 0.462500 \n",
            "Iterations [22910/24690], Loss: 2.012214, Validation F1 score: 0.375000, Average training f1 scores: 0.493750 \n",
            "Iterations [22920/24690], Loss: 1.600391, Validation F1 score: 0.437500, Average training f1 scores: 0.493750 \n",
            "Iterations [22930/24690], Loss: 1.330512, Validation F1 score: 0.437500, Average training f1 scores: 0.475000 \n",
            "Iterations [22940/24690], Loss: 1.314681, Validation F1 score: 0.312500, Average training f1 scores: 0.450000 \n",
            "Iterations [22950/24690], Loss: 1.327047, Validation F1 score: 0.437500, Average training f1 scores: 0.525000 \n",
            "Iterations [22960/24690], Loss: 1.194168, Validation F1 score: 0.375000, Average training f1 scores: 0.481250 \n",
            "Iterations [22970/24690], Loss: 1.944522, Validation F1 score: 0.250000, Average training f1 scores: 0.525000 \n",
            "Iterations [22980/24690], Loss: 1.583167, Validation F1 score: 0.312500, Average training f1 scores: 0.475000 \n",
            "Iterations [22990/24690], Loss: 1.639579, Validation F1 score: 0.437500, Average training f1 scores: 0.550000 \n",
            "Iterations [23000/24690], Loss: 1.066201, Validation F1 score: 0.500000, Average training f1 scores: 0.525000 \n",
            "Iterations [23010/24690], Loss: 1.391054, Validation F1 score: 0.312500, Average training f1 scores: 0.500000 \n",
            "Iterations [23020/24690], Loss: 1.804204, Validation F1 score: 0.375000, Average training f1 scores: 0.506250 \n",
            "Iterations [23030/24690], Loss: 1.910961, Validation F1 score: 0.062500, Average training f1 scores: 0.525000 \n",
            "Iterations [23040/24690], Loss: 1.671321, Validation F1 score: 0.375000, Average training f1 scores: 0.475000 \n",
            "Iterations [23050/24690], Loss: 0.998849, Validation F1 score: 0.437500, Average training f1 scores: 0.525000 \n",
            "Iterations [23060/24690], Loss: 1.581282, Validation F1 score: 0.250000, Average training f1 scores: 0.493750 \n",
            "Iterations [23070/24690], Loss: 1.440096, Validation F1 score: 0.312500, Average training f1 scores: 0.481250 \n",
            "Iterations [23080/24690], Loss: 1.277625, Validation F1 score: 0.500000, Average training f1 scores: 0.456250 \n",
            "Iterations [23090/24690], Loss: 1.398755, Validation F1 score: 0.250000, Average training f1 scores: 0.500000 \n",
            "Iterations [23100/24690], Loss: 1.754589, Validation F1 score: 0.250000, Average training f1 scores: 0.393750 \n",
            "Iterations [23110/24690], Loss: 1.477290, Validation F1 score: 0.250000, Average training f1 scores: 0.525000 \n",
            "Iterations [23120/24690], Loss: 2.085875, Validation F1 score: 0.125000, Average training f1 scores: 0.450000 \n",
            "Iterations [23130/24690], Loss: 1.893896, Validation F1 score: 0.375000, Average training f1 scores: 0.506250 \n",
            "Iterations [23140/24690], Loss: 1.782512, Validation F1 score: 0.437500, Average training f1 scores: 0.537500 \n",
            "Iterations [23150/24690], Loss: 1.728170, Validation F1 score: 0.312500, Average training f1 scores: 0.481250 \n",
            "Iterations [23160/24690], Loss: 1.765995, Validation F1 score: 0.187500, Average training f1 scores: 0.468750 \n",
            "Iterations [23170/24690], Loss: 1.265626, Validation F1 score: 0.500000, Average training f1 scores: 0.512500 \n",
            "Iterations [23180/24690], Loss: 1.261874, Validation F1 score: 0.375000, Average training f1 scores: 0.493750 \n",
            "Iterations [23190/24690], Loss: 1.316845, Validation F1 score: 0.250000, Average training f1 scores: 0.481250 \n",
            "Iterations [23200/24690], Loss: 1.674831, Validation F1 score: 0.312500, Average training f1 scores: 0.475000 \n",
            "Iterations [23210/24690], Loss: 1.073608, Validation F1 score: 0.437500, Average training f1 scores: 0.568750 \n",
            "Iterations [23220/24690], Loss: 1.377918, Validation F1 score: 0.375000, Average training f1 scores: 0.481250 \n",
            "Iterations [23230/24690], Loss: 1.662586, Validation F1 score: 0.187500, Average training f1 scores: 0.493750 \n",
            "Iterations [23240/24690], Loss: 1.193924, Validation F1 score: 0.375000, Average training f1 scores: 0.406250 \n",
            "Iterations [23250/24690], Loss: 1.639879, Validation F1 score: 0.375000, Average training f1 scores: 0.475000 \n",
            "Iterations [23260/24690], Loss: 1.396381, Validation F1 score: 0.437500, Average training f1 scores: 0.518750 \n",
            "Iterations [23270/24690], Loss: 1.163170, Validation F1 score: 0.375000, Average training f1 scores: 0.537500 \n",
            "Iterations [23280/24690], Loss: 1.095653, Validation F1 score: 0.375000, Average training f1 scores: 0.525000 \n",
            "Iterations [23290/24690], Loss: 1.272980, Validation F1 score: 0.312500, Average training f1 scores: 0.481250 \n",
            "Iterations [23300/24690], Loss: 1.733809, Validation F1 score: 0.187500, Average training f1 scores: 0.537500 \n",
            "Iterations [23310/24690], Loss: 2.122467, Validation F1 score: 0.250000, Average training f1 scores: 0.425000 \n",
            "Iterations [23320/24690], Loss: 1.751945, Validation F1 score: 0.375000, Average training f1 scores: 0.531250 \n",
            "Iterations [23330/24690], Loss: 1.369257, Validation F1 score: 0.375000, Average training f1 scores: 0.537500 \n",
            "Iterations [23340/24690], Loss: 1.651758, Validation F1 score: 0.125000, Average training f1 scores: 0.500000 \n",
            "Iterations [23350/24690], Loss: 1.159210, Validation F1 score: 0.375000, Average training f1 scores: 0.487500 \n",
            "Iterations [23360/24690], Loss: 1.651665, Validation F1 score: 0.375000, Average training f1 scores: 0.437500 \n",
            "Iterations [23370/24690], Loss: 1.248873, Validation F1 score: 0.562500, Average training f1 scores: 0.406250 \n",
            "Iterations [23380/24690], Loss: 1.304863, Validation F1 score: 0.500000, Average training f1 scores: 0.525000 \n",
            "Iterations [23390/24690], Loss: 1.551129, Validation F1 score: 0.250000, Average training f1 scores: 0.518750 \n",
            "Iterations [23400/24690], Loss: 1.596184, Validation F1 score: 0.437500, Average training f1 scores: 0.518750 \n",
            "Iterations [23410/24690], Loss: 1.822704, Validation F1 score: 0.437500, Average training f1 scores: 0.437500 \n",
            "Iterations [23420/24690], Loss: 2.609576, Validation F1 score: 0.250000, Average training f1 scores: 0.468750 \n",
            "Iterations [23430/24690], Loss: 1.610283, Validation F1 score: 0.375000, Average training f1 scores: 0.456250 \n",
            "Iterations [23440/24690], Loss: 1.446183, Validation F1 score: 0.250000, Average training f1 scores: 0.493750 \n",
            "Iterations [23450/24690], Loss: 1.451306, Validation F1 score: 0.437500, Average training f1 scores: 0.475000 \n",
            "Iterations [23460/24690], Loss: 1.700455, Validation F1 score: 0.562500, Average training f1 scores: 0.518750 \n",
            "Iterations [23470/24690], Loss: 2.062073, Validation F1 score: 0.562500, Average training f1 scores: 0.506250 \n",
            "Iterations [23480/24690], Loss: 1.506433, Validation F1 score: 0.437500, Average training f1 scores: 0.468750 \n",
            "Iterations [23490/24690], Loss: 1.658311, Validation F1 score: 0.187500, Average training f1 scores: 0.543750 \n",
            "Iterations [23500/24690], Loss: 1.378332, Validation F1 score: 0.500000, Average training f1 scores: 0.443750 \n",
            "Iterations [23510/24690], Loss: 1.489894, Validation F1 score: 0.312500, Average training f1 scores: 0.456250 \n",
            "Iterations [23520/24690], Loss: 1.150997, Validation F1 score: 0.375000, Average training f1 scores: 0.487500 \n",
            "Iterations [23530/24690], Loss: 1.891116, Validation F1 score: 0.250000, Average training f1 scores: 0.462500 \n",
            "Iterations [23540/24690], Loss: 1.433165, Validation F1 score: 0.437500, Average training f1 scores: 0.437500 \n",
            "Iterations [23550/24690], Loss: 1.261332, Validation F1 score: 0.375000, Average training f1 scores: 0.506250 \n",
            "Iterations [23560/24690], Loss: 1.485304, Validation F1 score: 0.437500, Average training f1 scores: 0.456250 \n",
            "Iterations [23570/24690], Loss: 1.062536, Validation F1 score: 0.562500, Average training f1 scores: 0.537500 \n",
            "Iterations [23580/24690], Loss: 1.648557, Validation F1 score: 0.312500, Average training f1 scores: 0.462500 \n",
            "Iterations [23590/24690], Loss: 1.354773, Validation F1 score: 0.312500, Average training f1 scores: 0.437500 \n",
            "Iterations [23600/24690], Loss: 1.865296, Validation F1 score: 0.187500, Average training f1 scores: 0.487500 \n",
            "Iterations [23610/24690], Loss: 1.446369, Validation F1 score: 0.375000, Average training f1 scores: 0.525000 \n",
            "Iterations [23620/24690], Loss: 1.648231, Validation F1 score: 0.375000, Average training f1 scores: 0.543750 \n",
            "Iterations [23630/24690], Loss: 1.044945, Validation F1 score: 0.250000, Average training f1 scores: 0.462500 \n",
            "Iterations [23640/24690], Loss: 1.390249, Validation F1 score: 0.437500, Average training f1 scores: 0.537500 \n",
            "Iterations [23650/24690], Loss: 1.595138, Validation F1 score: 0.500000, Average training f1 scores: 0.568750 \n",
            "Iterations [23660/24690], Loss: 0.950583, Validation F1 score: 0.312500, Average training f1 scores: 0.493750 \n",
            "Iterations [23670/24690], Loss: 1.918693, Validation F1 score: 0.437500, Average training f1 scores: 0.500000 \n",
            "Iterations [23680/24690], Loss: 1.701350, Validation F1 score: 0.312500, Average training f1 scores: 0.437500 \n",
            "Iterations [23690/24690], Loss: 1.018657, Validation F1 score: 0.500000, Average training f1 scores: 0.543750 \n",
            "Iterations [23700/24690], Loss: 1.153005, Validation F1 score: 0.187500, Average training f1 scores: 0.443750 \n",
            "Iterations [23710/24690], Loss: 1.404459, Validation F1 score: 0.312500, Average training f1 scores: 0.412500 \n",
            "Iterations [23720/24690], Loss: 1.826352, Validation F1 score: 0.437500, Average training f1 scores: 0.468750 \n",
            "Iterations [23730/24690], Loss: 1.261113, Validation F1 score: 0.500000, Average training f1 scores: 0.506250 \n",
            "Iterations [23740/24690], Loss: 1.666045, Validation F1 score: 0.250000, Average training f1 scores: 0.512500 \n",
            "Iterations [23750/24690], Loss: 1.702836, Validation F1 score: 0.250000, Average training f1 scores: 0.468750 \n",
            "Iterations [23760/24690], Loss: 1.371090, Validation F1 score: 0.375000, Average training f1 scores: 0.500000 \n",
            "Iterations [23770/24690], Loss: 1.461323, Validation F1 score: 0.312500, Average training f1 scores: 0.518750 \n",
            "Iterations [23780/24690], Loss: 1.851274, Validation F1 score: 0.125000, Average training f1 scores: 0.475000 \n",
            "Iterations [23790/24690], Loss: 1.170254, Validation F1 score: 0.437500, Average training f1 scores: 0.531250 \n",
            "Iterations [23800/24690], Loss: 1.761473, Validation F1 score: 0.062500, Average training f1 scores: 0.468750 \n",
            "Iterations [23810/24690], Loss: 1.648584, Validation F1 score: 0.437500, Average training f1 scores: 0.518750 \n",
            "Iterations [23820/24690], Loss: 1.469107, Validation F1 score: 0.187500, Average training f1 scores: 0.468750 \n",
            "Iterations [23830/24690], Loss: 1.866867, Validation F1 score: 0.250000, Average training f1 scores: 0.468750 \n",
            "Iterations [23840/24690], Loss: 1.373479, Validation F1 score: 0.375000, Average training f1 scores: 0.443750 \n",
            "Iterations [23850/24690], Loss: 1.613784, Validation F1 score: 0.312500, Average training f1 scores: 0.487500 \n",
            "Iterations [23860/24690], Loss: 1.899180, Validation F1 score: 0.062500, Average training f1 scores: 0.493750 \n",
            "Iterations [23870/24690], Loss: 1.291263, Validation F1 score: 0.562500, Average training f1 scores: 0.437500 \n",
            "Iterations [23880/24690], Loss: 1.664921, Validation F1 score: 0.125000, Average training f1 scores: 0.537500 \n",
            "Iterations [23890/24690], Loss: 1.576909, Validation F1 score: 0.437500, Average training f1 scores: 0.525000 \n",
            "Iterations [23900/24690], Loss: 1.819670, Validation F1 score: 0.187500, Average training f1 scores: 0.437500 \n",
            "Iterations [23910/24690], Loss: 1.558522, Validation F1 score: 0.312500, Average training f1 scores: 0.518750 \n",
            "Iterations [23920/24690], Loss: 1.723153, Validation F1 score: 0.250000, Average training f1 scores: 0.518750 \n",
            "Iterations [23930/24690], Loss: 1.777686, Validation F1 score: 0.375000, Average training f1 scores: 0.412500 \n",
            "Iterations [23940/24690], Loss: 1.528679, Validation F1 score: 0.187500, Average training f1 scores: 0.500000 \n",
            "Iterations [23950/24690], Loss: 1.624688, Validation F1 score: 0.437500, Average training f1 scores: 0.500000 \n",
            "Iterations [23960/24690], Loss: 1.514625, Validation F1 score: 0.437500, Average training f1 scores: 0.500000 \n",
            "Iterations [23970/24690], Loss: 1.436045, Validation F1 score: 0.312500, Average training f1 scores: 0.531250 \n",
            "Iterations [23980/24690], Loss: 1.577191, Validation F1 score: 0.250000, Average training f1 scores: 0.537500 \n",
            "Iterations [23990/24690], Loss: 1.281140, Validation F1 score: 0.312500, Average training f1 scores: 0.543750 \n",
            "Iterations [24000/24690], Loss: 1.682129, Validation F1 score: 0.500000, Average training f1 scores: 0.468750 \n",
            "Iterations [24010/24690], Loss: 1.107303, Validation F1 score: 0.562500, Average training f1 scores: 0.531250 \n",
            "Iterations [24020/24690], Loss: 1.284465, Validation F1 score: 0.312500, Average training f1 scores: 0.450000 \n",
            "Iterations [24030/24690], Loss: 1.581867, Validation F1 score: 0.312500, Average training f1 scores: 0.525000 \n",
            "Iterations [24040/24690], Loss: 1.585363, Validation F1 score: 0.312500, Average training f1 scores: 0.468750 \n",
            "Iterations [24050/24690], Loss: 1.097168, Validation F1 score: 0.625000, Average training f1 scores: 0.437500 \n",
            "Iterations [24060/24690], Loss: 1.396126, Validation F1 score: 0.187500, Average training f1 scores: 0.462500 \n",
            "Iterations [24070/24690], Loss: 1.443581, Validation F1 score: 0.500000, Average training f1 scores: 0.562500 \n",
            "Iterations [24080/24690], Loss: 1.625863, Validation F1 score: 0.250000, Average training f1 scores: 0.437500 \n",
            "Iterations [24090/24690], Loss: 1.375609, Validation F1 score: 0.500000, Average training f1 scores: 0.462500 \n",
            "Iterations [24100/24690], Loss: 1.278770, Validation F1 score: 0.375000, Average training f1 scores: 0.450000 \n",
            "Iterations [24110/24690], Loss: 1.495600, Validation F1 score: 0.375000, Average training f1 scores: 0.575000 \n",
            "Iterations [24120/24690], Loss: 1.728744, Validation F1 score: 0.187500, Average training f1 scores: 0.437500 \n",
            "Iterations [24130/24690], Loss: 1.385504, Validation F1 score: 0.250000, Average training f1 scores: 0.543750 \n",
            "Iterations [24140/24690], Loss: 1.680840, Validation F1 score: 0.312500, Average training f1 scores: 0.518750 \n",
            "Iterations [24150/24690], Loss: 1.062450, Validation F1 score: 0.500000, Average training f1 scores: 0.556250 \n",
            "Iterations [24160/24690], Loss: 1.264299, Validation F1 score: 0.500000, Average training f1 scores: 0.568750 \n",
            "Iterations [24170/24690], Loss: 1.029493, Validation F1 score: 0.437500, Average training f1 scores: 0.556250 \n",
            "Iterations [24180/24690], Loss: 1.573331, Validation F1 score: 0.437500, Average training f1 scores: 0.468750 \n",
            "Iterations [24190/24690], Loss: 1.539933, Validation F1 score: 0.375000, Average training f1 scores: 0.500000 \n",
            "Iterations [24200/24690], Loss: 1.493968, Validation F1 score: 0.187500, Average training f1 scores: 0.525000 \n",
            "Iterations [24210/24690], Loss: 1.663394, Validation F1 score: 0.437500, Average training f1 scores: 0.462500 \n",
            "Iterations [24220/24690], Loss: 1.103029, Validation F1 score: 0.500000, Average training f1 scores: 0.512500 \n",
            "Iterations [24230/24690], Loss: 1.521832, Validation F1 score: 0.312500, Average training f1 scores: 0.531250 \n",
            "Iterations [24240/24690], Loss: 1.498262, Validation F1 score: 0.125000, Average training f1 scores: 0.475000 \n",
            "Iterations [24250/24690], Loss: 1.076885, Validation F1 score: 0.500000, Average training f1 scores: 0.556250 \n",
            "Iterations [24260/24690], Loss: 1.047296, Validation F1 score: 0.437500, Average training f1 scores: 0.493750 \n",
            "Iterations [24270/24690], Loss: 1.556116, Validation F1 score: 0.312500, Average training f1 scores: 0.412500 \n",
            "Iterations [24280/24690], Loss: 1.163949, Validation F1 score: 0.437500, Average training f1 scores: 0.500000 \n",
            "Iterations [24290/24690], Loss: 2.002623, Validation F1 score: 0.312500, Average training f1 scores: 0.518750 \n",
            "Iterations [24300/24690], Loss: 1.591472, Validation F1 score: 0.312500, Average training f1 scores: 0.462500 \n",
            "Iterations [24310/24690], Loss: 1.468408, Validation F1 score: 0.250000, Average training f1 scores: 0.518750 \n",
            "Iterations [24320/24690], Loss: 1.501203, Validation F1 score: 0.500000, Average training f1 scores: 0.475000 \n",
            "Iterations [24330/24690], Loss: 1.676022, Validation F1 score: 0.312500, Average training f1 scores: 0.512500 \n",
            "Iterations [24340/24690], Loss: 1.781323, Validation F1 score: 0.312500, Average training f1 scores: 0.481250 \n",
            "Iterations [24350/24690], Loss: 1.494129, Validation F1 score: 0.437500, Average training f1 scores: 0.381250 \n",
            "Iterations [24360/24690], Loss: 1.504463, Validation F1 score: 0.187500, Average training f1 scores: 0.506250 \n",
            "Iterations [24370/24690], Loss: 1.900995, Validation F1 score: 0.375000, Average training f1 scores: 0.475000 \n",
            "Iterations [24380/24690], Loss: 1.658506, Validation F1 score: 0.437500, Average training f1 scores: 0.393750 \n",
            "Iterations [24390/24690], Loss: 1.280447, Validation F1 score: 0.625000, Average training f1 scores: 0.468750 \n",
            "Iterations [24400/24690], Loss: 1.656733, Validation F1 score: 0.437500, Average training f1 scores: 0.493750 \n",
            "Iterations [24410/24690], Loss: 1.970175, Validation F1 score: 0.375000, Average training f1 scores: 0.512500 \n",
            "Iterations [24420/24690], Loss: 1.556902, Validation F1 score: 0.375000, Average training f1 scores: 0.506250 \n",
            "Iterations [24430/24690], Loss: 1.188852, Validation F1 score: 0.250000, Average training f1 scores: 0.506250 \n",
            "Iterations [24440/24690], Loss: 1.731975, Validation F1 score: 0.250000, Average training f1 scores: 0.456250 \n",
            "Iterations [24450/24690], Loss: 1.661798, Validation F1 score: 0.187500, Average training f1 scores: 0.506250 \n",
            "Iterations [24460/24690], Loss: 1.721529, Validation F1 score: 0.437500, Average training f1 scores: 0.443750 \n",
            "Iterations [24470/24690], Loss: 1.247263, Validation F1 score: 0.312500, Average training f1 scores: 0.537500 \n",
            "Iterations [24480/24690], Loss: 1.572274, Validation F1 score: 0.250000, Average training f1 scores: 0.500000 \n",
            "Iterations [24490/24690], Loss: 1.879737, Validation F1 score: 0.250000, Average training f1 scores: 0.512500 \n",
            "Iterations [24500/24690], Loss: 1.750611, Validation F1 score: 0.312500, Average training f1 scores: 0.393750 \n",
            "Iterations [24510/24690], Loss: 1.175836, Validation F1 score: 0.500000, Average training f1 scores: 0.525000 \n",
            "Iterations [24520/24690], Loss: 1.245586, Validation F1 score: 0.437500, Average training f1 scores: 0.525000 \n",
            "Iterations [24530/24690], Loss: 1.456924, Validation F1 score: 0.312500, Average training f1 scores: 0.462500 \n",
            "Iterations [24540/24690], Loss: 1.199004, Validation F1 score: 0.562500, Average training f1 scores: 0.493750 \n",
            "Iterations [24550/24690], Loss: 1.225412, Validation F1 score: 0.375000, Average training f1 scores: 0.468750 \n",
            "Iterations [24560/24690], Loss: 1.199969, Validation F1 score: 0.312500, Average training f1 scores: 0.493750 \n",
            "Iterations [24570/24690], Loss: 1.391338, Validation F1 score: 0.437500, Average training f1 scores: 0.437500 \n",
            "Iterations [24580/24690], Loss: 1.445351, Validation F1 score: 0.312500, Average training f1 scores: 0.406250 \n",
            "Iterations [24590/24690], Loss: 1.683427, Validation F1 score: 0.375000, Average training f1 scores: 0.406250 \n",
            "Iterations [24600/24690], Loss: 1.497368, Validation F1 score: 0.500000, Average training f1 scores: 0.525000 \n",
            "Iterations [24610/24690], Loss: 1.530401, Validation F1 score: 0.187500, Average training f1 scores: 0.493750 \n",
            "Iterations [24620/24690], Loss: 1.252723, Validation F1 score: 0.562500, Average training f1 scores: 0.487500 \n",
            "Iterations [24630/24690], Loss: 1.465160, Validation F1 score: 0.375000, Average training f1 scores: 0.518750 \n",
            "Iterations [24640/24690], Loss: 1.517374, Validation F1 score: 0.250000, Average training f1 scores: 0.506250 \n",
            "Iterations [24650/24690], Loss: 1.543788, Validation F1 score: 0.500000, Average training f1 scores: 0.531250 \n",
            "Iterations [24660/24690], Loss: 1.562816, Validation F1 score: 0.437500, Average training f1 scores: 0.500000 \n",
            "Iterations [24670/24690], Loss: 1.246171, Validation F1 score: 0.437500, Average training f1 scores: 0.493750 \n",
            "Iterations [24680/24690], Loss: 1.188842, Validation F1 score: 0.250000, Average training f1 scores: 0.468750 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxEhpTFSllTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the final trained model\n",
        "torch.save(model.state_dict(), model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZoicVCWkbzX",
        "colab_type": "text"
      },
      "source": [
        "2.4 Analysis the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRCtK9FSkhOq",
        "colab_type": "code",
        "outputId": "f4a11283-e7c9-4f15-ec68-4549ecf76dd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Run model through whole testing dataset and find the final F1 scores\n",
        "# load the model\n",
        "model_path = \"/content/drive/My Drive/GT_classes/CSE_6240 - Text Mining/Reddit-Roles-Identification/discourse/Model_32hsize_1layer_bidirectional_5epochs_16batch.pt\"\n",
        "hidden_size = 32          # number of features in hidden state of the RNN decoder\n",
        "num_layers = 1              # number of LSTM layers\n",
        "num_discourse_type = 10\n",
        "bidirectional = True\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = BiLSTM(hidden_dim = hidden_size, \n",
        "               discourse_types = num_discourse_type,\n",
        "               lstm_layers = num_layers, \n",
        "               bidirectional = bidirectional,\n",
        "               device = device)\n",
        "\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.to(device)\n",
        "\n",
        "batch_size = 16\n",
        "iterations = int(len(test_posts) / batch_size + 0.5) \n",
        "\n",
        "predictions = []\n",
        "\n",
        "for i in range(iterations):\n",
        "    X = test_posts.to_numpy() \n",
        "    mask = np.array(range(i*batch_size,(i+1)*batch_size))\n",
        "    X = X[mask]\n",
        "\n",
        "    X = posts2vec(X, pre_trained_word2vec_model)\n",
        "    X = torch.FloatTensor(X).to(device)\n",
        "\n",
        "    tag_score = model(X, batch_size)\n",
        "    # calculate f1 score\n",
        "    prediction = torch.argmax(tag_score, dim=1).cpu().numpy().tolist()\n",
        "    predictions += prediction\n",
        "    print('Iterations [%d/%d]' % (len(predictions), len(test_posts)))\n",
        "\n",
        "test_post_type = type2vec(test_post_type)\n",
        "predictions = np.array(predictions)\n",
        "test_f1 = f1_score(predictions, test_post_type, average='micro')\n",
        "print(\"Final f1 score for whole testing dataset: {}\".format(test_f1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iterations [16/19754]\n",
            "Iterations [32/19754]\n",
            "Iterations [48/19754]\n",
            "Iterations [64/19754]\n",
            "Iterations [80/19754]\n",
            "Iterations [96/19754]\n",
            "Iterations [112/19754]\n",
            "Iterations [128/19754]\n",
            "Iterations [144/19754]\n",
            "Iterations [160/19754]\n",
            "Iterations [176/19754]\n",
            "Iterations [192/19754]\n",
            "Iterations [208/19754]\n",
            "Iterations [224/19754]\n",
            "Iterations [240/19754]\n",
            "Iterations [256/19754]\n",
            "Iterations [272/19754]\n",
            "Iterations [288/19754]\n",
            "Iterations [304/19754]\n",
            "Iterations [320/19754]\n",
            "Iterations [336/19754]\n",
            "Iterations [352/19754]\n",
            "Iterations [368/19754]\n",
            "Iterations [384/19754]\n",
            "Iterations [400/19754]\n",
            "Iterations [416/19754]\n",
            "Iterations [432/19754]\n",
            "Iterations [448/19754]\n",
            "Iterations [464/19754]\n",
            "Iterations [480/19754]\n",
            "Iterations [496/19754]\n",
            "Iterations [512/19754]\n",
            "Iterations [528/19754]\n",
            "Iterations [544/19754]\n",
            "Iterations [560/19754]\n",
            "Iterations [576/19754]\n",
            "Iterations [592/19754]\n",
            "Iterations [608/19754]\n",
            "Iterations [624/19754]\n",
            "Iterations [640/19754]\n",
            "Iterations [656/19754]\n",
            "Iterations [672/19754]\n",
            "Iterations [688/19754]\n",
            "Iterations [704/19754]\n",
            "Iterations [720/19754]\n",
            "Iterations [736/19754]\n",
            "Iterations [752/19754]\n",
            "Iterations [768/19754]\n",
            "Iterations [784/19754]\n",
            "Iterations [800/19754]\n",
            "Iterations [816/19754]\n",
            "Iterations [832/19754]\n",
            "Iterations [848/19754]\n",
            "Iterations [864/19754]\n",
            "Iterations [880/19754]\n",
            "Iterations [896/19754]\n",
            "Iterations [912/19754]\n",
            "Iterations [928/19754]\n",
            "Iterations [944/19754]\n",
            "Iterations [960/19754]\n",
            "Iterations [976/19754]\n",
            "Iterations [992/19754]\n",
            "Iterations [1008/19754]\n",
            "Iterations [1024/19754]\n",
            "Iterations [1040/19754]\n",
            "Iterations [1056/19754]\n",
            "Iterations [1072/19754]\n",
            "Iterations [1088/19754]\n",
            "Iterations [1104/19754]\n",
            "Iterations [1120/19754]\n",
            "Iterations [1136/19754]\n",
            "Iterations [1152/19754]\n",
            "Iterations [1168/19754]\n",
            "Iterations [1184/19754]\n",
            "Iterations [1200/19754]\n",
            "Iterations [1216/19754]\n",
            "Iterations [1232/19754]\n",
            "Iterations [1248/19754]\n",
            "Iterations [1264/19754]\n",
            "Iterations [1280/19754]\n",
            "Iterations [1296/19754]\n",
            "Iterations [1312/19754]\n",
            "Iterations [1328/19754]\n",
            "Iterations [1344/19754]\n",
            "Iterations [1360/19754]\n",
            "Iterations [1376/19754]\n",
            "Iterations [1392/19754]\n",
            "Iterations [1408/19754]\n",
            "Iterations [1424/19754]\n",
            "Iterations [1440/19754]\n",
            "Iterations [1456/19754]\n",
            "Iterations [1472/19754]\n",
            "Iterations [1488/19754]\n",
            "Iterations [1504/19754]\n",
            "Iterations [1520/19754]\n",
            "Iterations [1536/19754]\n",
            "Iterations [1552/19754]\n",
            "Iterations [1568/19754]\n",
            "Iterations [1584/19754]\n",
            "Iterations [1600/19754]\n",
            "Iterations [1616/19754]\n",
            "Iterations [1632/19754]\n",
            "Iterations [1648/19754]\n",
            "Iterations [1664/19754]\n",
            "Iterations [1680/19754]\n",
            "Iterations [1696/19754]\n",
            "Iterations [1712/19754]\n",
            "Iterations [1728/19754]\n",
            "Iterations [1744/19754]\n",
            "Iterations [1760/19754]\n",
            "Iterations [1776/19754]\n",
            "Iterations [1792/19754]\n",
            "Iterations [1808/19754]\n",
            "Iterations [1824/19754]\n",
            "Iterations [1840/19754]\n",
            "Iterations [1856/19754]\n",
            "Iterations [1872/19754]\n",
            "Iterations [1888/19754]\n",
            "Iterations [1904/19754]\n",
            "Iterations [1920/19754]\n",
            "Iterations [1936/19754]\n",
            "Iterations [1952/19754]\n",
            "Iterations [1968/19754]\n",
            "Iterations [1984/19754]\n",
            "Iterations [2000/19754]\n",
            "Iterations [2016/19754]\n",
            "Iterations [2032/19754]\n",
            "Iterations [2048/19754]\n",
            "Iterations [2064/19754]\n",
            "Iterations [2080/19754]\n",
            "Iterations [2096/19754]\n",
            "Iterations [2112/19754]\n",
            "Iterations [2128/19754]\n",
            "Iterations [2144/19754]\n",
            "Iterations [2160/19754]\n",
            "Iterations [2176/19754]\n",
            "Iterations [2192/19754]\n",
            "Iterations [2208/19754]\n",
            "Iterations [2224/19754]\n",
            "Iterations [2240/19754]\n",
            "Iterations [2256/19754]\n",
            "Iterations [2272/19754]\n",
            "Iterations [2288/19754]\n",
            "Iterations [2304/19754]\n",
            "Iterations [2320/19754]\n",
            "Iterations [2336/19754]\n",
            "Iterations [2352/19754]\n",
            "Iterations [2368/19754]\n",
            "Iterations [2384/19754]\n",
            "Iterations [2400/19754]\n",
            "Iterations [2416/19754]\n",
            "Iterations [2432/19754]\n",
            "Iterations [2448/19754]\n",
            "Iterations [2464/19754]\n",
            "Iterations [2480/19754]\n",
            "Iterations [2496/19754]\n",
            "Iterations [2512/19754]\n",
            "Iterations [2528/19754]\n",
            "Iterations [2544/19754]\n",
            "Iterations [2560/19754]\n",
            "Iterations [2576/19754]\n",
            "Iterations [2592/19754]\n",
            "Iterations [2608/19754]\n",
            "Iterations [2624/19754]\n",
            "Iterations [2640/19754]\n",
            "Iterations [2656/19754]\n",
            "Iterations [2672/19754]\n",
            "Iterations [2688/19754]\n",
            "Iterations [2704/19754]\n",
            "Iterations [2720/19754]\n",
            "Iterations [2736/19754]\n",
            "Iterations [2752/19754]\n",
            "Iterations [2768/19754]\n",
            "Iterations [2784/19754]\n",
            "Iterations [2800/19754]\n",
            "Iterations [2816/19754]\n",
            "Iterations [2832/19754]\n",
            "Iterations [2848/19754]\n",
            "Iterations [2864/19754]\n",
            "Iterations [2880/19754]\n",
            "Iterations [2896/19754]\n",
            "Iterations [2912/19754]\n",
            "Iterations [2928/19754]\n",
            "Iterations [2944/19754]\n",
            "Iterations [2960/19754]\n",
            "Iterations [2976/19754]\n",
            "Iterations [2992/19754]\n",
            "Iterations [3008/19754]\n",
            "Iterations [3024/19754]\n",
            "Iterations [3040/19754]\n",
            "Iterations [3056/19754]\n",
            "Iterations [3072/19754]\n",
            "Iterations [3088/19754]\n",
            "Iterations [3104/19754]\n",
            "Iterations [3120/19754]\n",
            "Iterations [3136/19754]\n",
            "Iterations [3152/19754]\n",
            "Iterations [3168/19754]\n",
            "Iterations [3184/19754]\n",
            "Iterations [3200/19754]\n",
            "Iterations [3216/19754]\n",
            "Iterations [3232/19754]\n",
            "Iterations [3248/19754]\n",
            "Iterations [3264/19754]\n",
            "Iterations [3280/19754]\n",
            "Iterations [3296/19754]\n",
            "Iterations [3312/19754]\n",
            "Iterations [3328/19754]\n",
            "Iterations [3344/19754]\n",
            "Iterations [3360/19754]\n",
            "Iterations [3376/19754]\n",
            "Iterations [3392/19754]\n",
            "Iterations [3408/19754]\n",
            "Iterations [3424/19754]\n",
            "Iterations [3440/19754]\n",
            "Iterations [3456/19754]\n",
            "Iterations [3472/19754]\n",
            "Iterations [3488/19754]\n",
            "Iterations [3504/19754]\n",
            "Iterations [3520/19754]\n",
            "Iterations [3536/19754]\n",
            "Iterations [3552/19754]\n",
            "Iterations [3568/19754]\n",
            "Iterations [3584/19754]\n",
            "Iterations [3600/19754]\n",
            "Iterations [3616/19754]\n",
            "Iterations [3632/19754]\n",
            "Iterations [3648/19754]\n",
            "Iterations [3664/19754]\n",
            "Iterations [3680/19754]\n",
            "Iterations [3696/19754]\n",
            "Iterations [3712/19754]\n",
            "Iterations [3728/19754]\n",
            "Iterations [3744/19754]\n",
            "Iterations [3760/19754]\n",
            "Iterations [3776/19754]\n",
            "Iterations [3792/19754]\n",
            "Iterations [3808/19754]\n",
            "Iterations [3824/19754]\n",
            "Iterations [3840/19754]\n",
            "Iterations [3856/19754]\n",
            "Iterations [3872/19754]\n",
            "Iterations [3888/19754]\n",
            "Iterations [3904/19754]\n",
            "Iterations [3920/19754]\n",
            "Iterations [3936/19754]\n",
            "Iterations [3952/19754]\n",
            "Iterations [3968/19754]\n",
            "Iterations [3984/19754]\n",
            "Iterations [4000/19754]\n",
            "Iterations [4016/19754]\n",
            "Iterations [4032/19754]\n",
            "Iterations [4048/19754]\n",
            "Iterations [4064/19754]\n",
            "Iterations [4080/19754]\n",
            "Iterations [4096/19754]\n",
            "Iterations [4112/19754]\n",
            "Iterations [4128/19754]\n",
            "Iterations [4144/19754]\n",
            "Iterations [4160/19754]\n",
            "Iterations [4176/19754]\n",
            "Iterations [4192/19754]\n",
            "Iterations [4208/19754]\n",
            "Iterations [4224/19754]\n",
            "Iterations [4240/19754]\n",
            "Iterations [4256/19754]\n",
            "Iterations [4272/19754]\n",
            "Iterations [4288/19754]\n",
            "Iterations [4304/19754]\n",
            "Iterations [4320/19754]\n",
            "Iterations [4336/19754]\n",
            "Iterations [4352/19754]\n",
            "Iterations [4368/19754]\n",
            "Iterations [4384/19754]\n",
            "Iterations [4400/19754]\n",
            "Iterations [4416/19754]\n",
            "Iterations [4432/19754]\n",
            "Iterations [4448/19754]\n",
            "Iterations [4464/19754]\n",
            "Iterations [4480/19754]\n",
            "Iterations [4496/19754]\n",
            "Iterations [4512/19754]\n",
            "Iterations [4528/19754]\n",
            "Iterations [4544/19754]\n",
            "Iterations [4560/19754]\n",
            "Iterations [4576/19754]\n",
            "Iterations [4592/19754]\n",
            "Iterations [4608/19754]\n",
            "Iterations [4624/19754]\n",
            "Iterations [4640/19754]\n",
            "Iterations [4656/19754]\n",
            "Iterations [4672/19754]\n",
            "Iterations [4688/19754]\n",
            "Iterations [4704/19754]\n",
            "Iterations [4720/19754]\n",
            "Iterations [4736/19754]\n",
            "Iterations [4752/19754]\n",
            "Iterations [4768/19754]\n",
            "Iterations [4784/19754]\n",
            "Iterations [4800/19754]\n",
            "Iterations [4816/19754]\n",
            "Iterations [4832/19754]\n",
            "Iterations [4848/19754]\n",
            "Iterations [4864/19754]\n",
            "Iterations [4880/19754]\n",
            "Iterations [4896/19754]\n",
            "Iterations [4912/19754]\n",
            "Iterations [4928/19754]\n",
            "Iterations [4944/19754]\n",
            "Iterations [4960/19754]\n",
            "Iterations [4976/19754]\n",
            "Iterations [4992/19754]\n",
            "Iterations [5008/19754]\n",
            "Iterations [5024/19754]\n",
            "Iterations [5040/19754]\n",
            "Iterations [5056/19754]\n",
            "Iterations [5072/19754]\n",
            "Iterations [5088/19754]\n",
            "Iterations [5104/19754]\n",
            "Iterations [5120/19754]\n",
            "Iterations [5136/19754]\n",
            "Iterations [5152/19754]\n",
            "Iterations [5168/19754]\n",
            "Iterations [5184/19754]\n",
            "Iterations [5200/19754]\n",
            "Iterations [5216/19754]\n",
            "Iterations [5232/19754]\n",
            "Iterations [5248/19754]\n",
            "Iterations [5264/19754]\n",
            "Iterations [5280/19754]\n",
            "Iterations [5296/19754]\n",
            "Iterations [5312/19754]\n",
            "Iterations [5328/19754]\n",
            "Iterations [5344/19754]\n",
            "Iterations [5360/19754]\n",
            "Iterations [5376/19754]\n",
            "Iterations [5392/19754]\n",
            "Iterations [5408/19754]\n",
            "Iterations [5424/19754]\n",
            "Iterations [5440/19754]\n",
            "Iterations [5456/19754]\n",
            "Iterations [5472/19754]\n",
            "Iterations [5488/19754]\n",
            "Iterations [5504/19754]\n",
            "Iterations [5520/19754]\n",
            "Iterations [5536/19754]\n",
            "Iterations [5552/19754]\n",
            "Iterations [5568/19754]\n",
            "Iterations [5584/19754]\n",
            "Iterations [5600/19754]\n",
            "Iterations [5616/19754]\n",
            "Iterations [5632/19754]\n",
            "Iterations [5648/19754]\n",
            "Iterations [5664/19754]\n",
            "Iterations [5680/19754]\n",
            "Iterations [5696/19754]\n",
            "Iterations [5712/19754]\n",
            "Iterations [5728/19754]\n",
            "Iterations [5744/19754]\n",
            "Iterations [5760/19754]\n",
            "Iterations [5776/19754]\n",
            "Iterations [5792/19754]\n",
            "Iterations [5808/19754]\n",
            "Iterations [5824/19754]\n",
            "Iterations [5840/19754]\n",
            "Iterations [5856/19754]\n",
            "Iterations [5872/19754]\n",
            "Iterations [5888/19754]\n",
            "Iterations [5904/19754]\n",
            "Iterations [5920/19754]\n",
            "Iterations [5936/19754]\n",
            "Iterations [5952/19754]\n",
            "Iterations [5968/19754]\n",
            "Iterations [5984/19754]\n",
            "Iterations [6000/19754]\n",
            "Iterations [6016/19754]\n",
            "Iterations [6032/19754]\n",
            "Iterations [6048/19754]\n",
            "Iterations [6064/19754]\n",
            "Iterations [6080/19754]\n",
            "Iterations [6096/19754]\n",
            "Iterations [6112/19754]\n",
            "Iterations [6128/19754]\n",
            "Iterations [6144/19754]\n",
            "Iterations [6160/19754]\n",
            "Iterations [6176/19754]\n",
            "Iterations [6192/19754]\n",
            "Iterations [6208/19754]\n",
            "Iterations [6224/19754]\n",
            "Iterations [6240/19754]\n",
            "Iterations [6256/19754]\n",
            "Iterations [6272/19754]\n",
            "Iterations [6288/19754]\n",
            "Iterations [6304/19754]\n",
            "Iterations [6320/19754]\n",
            "Iterations [6336/19754]\n",
            "Iterations [6352/19754]\n",
            "Iterations [6368/19754]\n",
            "Iterations [6384/19754]\n",
            "Iterations [6400/19754]\n",
            "Iterations [6416/19754]\n",
            "Iterations [6432/19754]\n",
            "Iterations [6448/19754]\n",
            "Iterations [6464/19754]\n",
            "Iterations [6480/19754]\n",
            "Iterations [6496/19754]\n",
            "Iterations [6512/19754]\n",
            "Iterations [6528/19754]\n",
            "Iterations [6544/19754]\n",
            "Iterations [6560/19754]\n",
            "Iterations [6576/19754]\n",
            "Iterations [6592/19754]\n",
            "Iterations [6608/19754]\n",
            "Iterations [6624/19754]\n",
            "Iterations [6640/19754]\n",
            "Iterations [6656/19754]\n",
            "Iterations [6672/19754]\n",
            "Iterations [6688/19754]\n",
            "Iterations [6704/19754]\n",
            "Iterations [6720/19754]\n",
            "Iterations [6736/19754]\n",
            "Iterations [6752/19754]\n",
            "Iterations [6768/19754]\n",
            "Iterations [6784/19754]\n",
            "Iterations [6800/19754]\n",
            "Iterations [6816/19754]\n",
            "Iterations [6832/19754]\n",
            "Iterations [6848/19754]\n",
            "Iterations [6864/19754]\n",
            "Iterations [6880/19754]\n",
            "Iterations [6896/19754]\n",
            "Iterations [6912/19754]\n",
            "Iterations [6928/19754]\n",
            "Iterations [6944/19754]\n",
            "Iterations [6960/19754]\n",
            "Iterations [6976/19754]\n",
            "Iterations [6992/19754]\n",
            "Iterations [7008/19754]\n",
            "Iterations [7024/19754]\n",
            "Iterations [7040/19754]\n",
            "Iterations [7056/19754]\n",
            "Iterations [7072/19754]\n",
            "Iterations [7088/19754]\n",
            "Iterations [7104/19754]\n",
            "Iterations [7120/19754]\n",
            "Iterations [7136/19754]\n",
            "Iterations [7152/19754]\n",
            "Iterations [7168/19754]\n",
            "Iterations [7184/19754]\n",
            "Iterations [7200/19754]\n",
            "Iterations [7216/19754]\n",
            "Iterations [7232/19754]\n",
            "Iterations [7248/19754]\n",
            "Iterations [7264/19754]\n",
            "Iterations [7280/19754]\n",
            "Iterations [7296/19754]\n",
            "Iterations [7312/19754]\n",
            "Iterations [7328/19754]\n",
            "Iterations [7344/19754]\n",
            "Iterations [7360/19754]\n",
            "Iterations [7376/19754]\n",
            "Iterations [7392/19754]\n",
            "Iterations [7408/19754]\n",
            "Iterations [7424/19754]\n",
            "Iterations [7440/19754]\n",
            "Iterations [7456/19754]\n",
            "Iterations [7472/19754]\n",
            "Iterations [7488/19754]\n",
            "Iterations [7504/19754]\n",
            "Iterations [7520/19754]\n",
            "Iterations [7536/19754]\n",
            "Iterations [7552/19754]\n",
            "Iterations [7568/19754]\n",
            "Iterations [7584/19754]\n",
            "Iterations [7600/19754]\n",
            "Iterations [7616/19754]\n",
            "Iterations [7632/19754]\n",
            "Iterations [7648/19754]\n",
            "Iterations [7664/19754]\n",
            "Iterations [7680/19754]\n",
            "Iterations [7696/19754]\n",
            "Iterations [7712/19754]\n",
            "Iterations [7728/19754]\n",
            "Iterations [7744/19754]\n",
            "Iterations [7760/19754]\n",
            "Iterations [7776/19754]\n",
            "Iterations [7792/19754]\n",
            "Iterations [7808/19754]\n",
            "Iterations [7824/19754]\n",
            "Iterations [7840/19754]\n",
            "Iterations [7856/19754]\n",
            "Iterations [7872/19754]\n",
            "Iterations [7888/19754]\n",
            "Iterations [7904/19754]\n",
            "Iterations [7920/19754]\n",
            "Iterations [7936/19754]\n",
            "Iterations [7952/19754]\n",
            "Iterations [7968/19754]\n",
            "Iterations [7984/19754]\n",
            "Iterations [8000/19754]\n",
            "Iterations [8016/19754]\n",
            "Iterations [8032/19754]\n",
            "Iterations [8048/19754]\n",
            "Iterations [8064/19754]\n",
            "Iterations [8080/19754]\n",
            "Iterations [8096/19754]\n",
            "Iterations [8112/19754]\n",
            "Iterations [8128/19754]\n",
            "Iterations [8144/19754]\n",
            "Iterations [8160/19754]\n",
            "Iterations [8176/19754]\n",
            "Iterations [8192/19754]\n",
            "Iterations [8208/19754]\n",
            "Iterations [8224/19754]\n",
            "Iterations [8240/19754]\n",
            "Iterations [8256/19754]\n",
            "Iterations [8272/19754]\n",
            "Iterations [8288/19754]\n",
            "Iterations [8304/19754]\n",
            "Iterations [8320/19754]\n",
            "Iterations [8336/19754]\n",
            "Iterations [8352/19754]\n",
            "Iterations [8368/19754]\n",
            "Iterations [8384/19754]\n",
            "Iterations [8400/19754]\n",
            "Iterations [8416/19754]\n",
            "Iterations [8432/19754]\n",
            "Iterations [8448/19754]\n",
            "Iterations [8464/19754]\n",
            "Iterations [8480/19754]\n",
            "Iterations [8496/19754]\n",
            "Iterations [8512/19754]\n",
            "Iterations [8528/19754]\n",
            "Iterations [8544/19754]\n",
            "Iterations [8560/19754]\n",
            "Iterations [8576/19754]\n",
            "Iterations [8592/19754]\n",
            "Iterations [8608/19754]\n",
            "Iterations [8624/19754]\n",
            "Iterations [8640/19754]\n",
            "Iterations [8656/19754]\n",
            "Iterations [8672/19754]\n",
            "Iterations [8688/19754]\n",
            "Iterations [8704/19754]\n",
            "Iterations [8720/19754]\n",
            "Iterations [8736/19754]\n",
            "Iterations [8752/19754]\n",
            "Iterations [8768/19754]\n",
            "Iterations [8784/19754]\n",
            "Iterations [8800/19754]\n",
            "Iterations [8816/19754]\n",
            "Iterations [8832/19754]\n",
            "Iterations [8848/19754]\n",
            "Iterations [8864/19754]\n",
            "Iterations [8880/19754]\n",
            "Iterations [8896/19754]\n",
            "Iterations [8912/19754]\n",
            "Iterations [8928/19754]\n",
            "Iterations [8944/19754]\n",
            "Iterations [8960/19754]\n",
            "Iterations [8976/19754]\n",
            "Iterations [8992/19754]\n",
            "Iterations [9008/19754]\n",
            "Iterations [9024/19754]\n",
            "Iterations [9040/19754]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-94339e6bc305>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mtag_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;31m# calculate f1 score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-1a77f5784577>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentences, batch_size)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mc_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_directions\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# print(\"test\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mlstm_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfinal_hidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_cell_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# final_hidden_state.size() = (1, batch_size, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mlstm_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# output.size() = (batch_size, num_seq, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# print(lstm_output.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 559\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cj7fmgLSvDjH",
        "colab_type": "code",
        "outputId": "c045639e-25cd-4524-938d-cab154b7f7b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 804
        }
      },
      "source": [
        "# Plot the training process\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "training_path1 = \"/content/drive/My Drive/GT_classes/CSE_6240 - Text Mining/Reddit-Roles-Identification/discourse/training_process.txt\"\n",
        "training_path2 = \"/content/drive/My Drive/GT_classes/CSE_6240 - Text Mining/Reddit-Roles-Identification/discourse/training_process2.txt\"\n",
        "def read_file(path):\n",
        "    iterations = []\n",
        "    losses = []\n",
        "    validation_f1 = []\n",
        "    average_training_f1 = []\n",
        "    with open(path, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            line_list = line.split()\n",
        "            if len(line_list) < 13:\n",
        "                continue\n",
        "            iteration = int(line_list[1].split('/')[0][1:])\n",
        "            loss = float(line_list[3][:-1])\n",
        "            valid_f1 = float(line_list[7][:-1])\n",
        "            ave_train_f1 = float(line_list[-1][:-1])\n",
        "            # print(iteration, loss, valid_f1, ave_train_f1)\n",
        "\n",
        "            iterations.append(iteration)\n",
        "            losses.append(loss)\n",
        "            validation_f1.append(valid_f1)\n",
        "            average_training_f1.append(ave_train_f1)\n",
        "\n",
        "    return iterations, losses, validation_f1, average_training_f1\n",
        "\n",
        "def plot(x, y, x_label, y_label):\n",
        "    # plot x and y\n",
        "    plt.plot(x, y)\n",
        "\n",
        "    # use linear regression to find a line that best fit the plot\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    x, y = x.reshape(-1,1), y.reshape(-1,1)\n",
        "\n",
        "    plt.plot( x, LinearRegression().fit(x, y).predict(x), 'r-' )\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "    plt.show()\n",
        "\n",
        "iteration1, loss1, validation1, average_training1 = read_file(training_path1)\n",
        "iteration2, loss2, validation2, average_training2 = read_file(training_path2)\n",
        "\n",
        "iterations = iteration1 + iteration2\n",
        "losses = loss1 + loss2\n",
        "validation_f1 = validation1 + validation2\n",
        "average_training_f1 = average_training1 + average_training2\n",
        "\n",
        "plot(iterations, losses, \"Iterations\", \"Losses\")\n",
        "plot(iterations, validation_f1, \"Iterations\", \"validation f1 scores\")\n",
        "plot(iterations, average_training_f1, \"Iterations\", \"average training f1 scores\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfI9SizzIolX",
        "colab_type": "text"
      },
      "source": [
        "#**3. Run the model on unlabeled dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5DbYeZ09TMz",
        "colab_type": "code",
        "outputId": "23a618c8-99ce-421c-f867-d13a015a2f50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# load unlabeled dataset\n",
        "import pandas as pd\n",
        "\n",
        "path = \"/content/drive/My Drive/GT_classes/CSE_6240 - Text Mining/Reddit-Roles-Identification/discourse/mergedComments_2019_02.csv\"\n",
        "unlabelled_discourse_file = pd.read_csv(path)\n",
        "\n",
        "print(discourse_file.columns)\n",
        "print(discourse_file[:5])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['post', 'post_type'], dtype='object')\n",
            "                                                post     post_type\n",
            "0  ['it', 'was', 'only', 'a', 'few', 'minutes', '...  announcement\n",
            "1  ['i', 've', 'wanted', 'to', 'watch', 'this', '...   elaboration\n",
            "2  ['you', 'strike', 'me', 'as', 'the', 'type', '...   elaboration\n",
            "3  ['yeah', 'i', 've', 'always', 'heard', 'that',...   elaboration\n",
            "4  ['alright', 'guys', 'little', 'background', 'a...  announcement\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raGJU1nZm2ZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clean up the unlabelled data\n",
        "import csv\n",
        "\n",
        "posts = []\n",
        "clean_unlabel_text = []\n",
        "for post in unlabelled_discourse_file[\"mergedText\"].apply(str):\n",
        "    clean_text = clean_posts(post)\n",
        "\n",
        "    posts.append(post)\n",
        "    clean_unlabel_text.append(clean_text)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XclTeKBEkymN",
        "colab_type": "code",
        "outputId": "62268b6b-ad82-40c6-96d5-e643989b5cbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# load the model\n",
        "model_path = \"/content/drive/My Drive/GT_classes/CSE_6240 - Text Mining/Reddit-Roles-Identification/discourse/Model_32hsize_1layer_bidirectional_5epochs_16batch.pt\"\n",
        "hidden_size = 32          # number of features in hidden state of the RNN decoder\n",
        "num_layers = 1              # number of LSTM layers\n",
        "num_discourse_type = 10\n",
        "bidirectional = True\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = BiLSTM(hidden_dim = hidden_size, \n",
        "               discourse_types = num_discourse_type,\n",
        "               lstm_layers = num_layers, \n",
        "               bidirectional = bidirectional,\n",
        "               device = device)\n",
        "               \n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.to(device)\n",
        "\n",
        "batch_size = 32\n",
        "iterations = int(len(clean_unlabel_text) / batch_size + 0.5) \n",
        "\n",
        "unlabelled_predictions = []\n",
        "\n",
        "for i in range(iterations):\n",
        "    if i != iterations - 1: \n",
        "        X = clean_unlabel_text[i*batch_size:(i+1)*batch_size] \n",
        "    else:\n",
        "        X = clean_unlabel_text[i*batch_size:]\n",
        "    \n",
        "    X = posts2vec(X, pre_trained_word2vec_model)\n",
        "    X = torch.FloatTensor(X).to(device)\n",
        "\n",
        "    tag_score = model(X, batch_size)\n",
        "    # calculate f1 score\n",
        "    prediction = torch.argmax(tag_score, dim=1).cpu().numpy().tolist()\n",
        "    unlabelled_predictions += prediction\n",
        "    print('Iterations [%d/%d]' % (len(unlabelled_predictions), len(clean_unlabel_text)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iterations [32/199482]\n",
            "Iterations [64/199482]\n",
            "Iterations [96/199482]\n",
            "Iterations [128/199482]\n",
            "Iterations [160/199482]\n",
            "Iterations [192/199482]\n",
            "Iterations [224/199482]\n",
            "Iterations [256/199482]\n",
            "Iterations [288/199482]\n",
            "Iterations [320/199482]\n",
            "Iterations [352/199482]\n",
            "Iterations [384/199482]\n",
            "Iterations [416/199482]\n",
            "Iterations [448/199482]\n",
            "Iterations [480/199482]\n",
            "Iterations [512/199482]\n",
            "Iterations [544/199482]\n",
            "Iterations [576/199482]\n",
            "Iterations [608/199482]\n",
            "Iterations [640/199482]\n",
            "Iterations [672/199482]\n",
            "Iterations [704/199482]\n",
            "Iterations [736/199482]\n",
            "Iterations [768/199482]\n",
            "Iterations [800/199482]\n",
            "Iterations [832/199482]\n",
            "Iterations [864/199482]\n",
            "Iterations [896/199482]\n",
            "Iterations [928/199482]\n",
            "Iterations [960/199482]\n",
            "Iterations [992/199482]\n",
            "Iterations [1024/199482]\n",
            "Iterations [1056/199482]\n",
            "Iterations [1088/199482]\n",
            "Iterations [1120/199482]\n",
            "Iterations [1152/199482]\n",
            "Iterations [1184/199482]\n",
            "Iterations [1216/199482]\n",
            "Iterations [1248/199482]\n",
            "Iterations [1280/199482]\n",
            "Iterations [1312/199482]\n",
            "Iterations [1344/199482]\n",
            "Iterations [1376/199482]\n",
            "Iterations [1408/199482]\n",
            "Iterations [1440/199482]\n",
            "Iterations [1472/199482]\n",
            "Iterations [1504/199482]\n",
            "Iterations [1536/199482]\n",
            "Iterations [1568/199482]\n",
            "Iterations [1600/199482]\n",
            "Iterations [1632/199482]\n",
            "Iterations [1664/199482]\n",
            "Iterations [1696/199482]\n",
            "Iterations [1728/199482]\n",
            "Iterations [1760/199482]\n",
            "Iterations [1792/199482]\n",
            "Iterations [1824/199482]\n",
            "Iterations [1856/199482]\n",
            "Iterations [1888/199482]\n",
            "Iterations [1920/199482]\n",
            "Iterations [1952/199482]\n",
            "Iterations [1984/199482]\n",
            "Iterations [2016/199482]\n",
            "Iterations [2048/199482]\n",
            "Iterations [2080/199482]\n",
            "Iterations [2112/199482]\n",
            "Iterations [2144/199482]\n",
            "Iterations [2176/199482]\n",
            "Iterations [2208/199482]\n",
            "Iterations [2240/199482]\n",
            "Iterations [2272/199482]\n",
            "Iterations [2304/199482]\n",
            "Iterations [2336/199482]\n",
            "Iterations [2368/199482]\n",
            "Iterations [2400/199482]\n",
            "Iterations [2432/199482]\n",
            "Iterations [2464/199482]\n",
            "Iterations [2496/199482]\n",
            "Iterations [2528/199482]\n",
            "Iterations [2560/199482]\n",
            "Iterations [2592/199482]\n",
            "Iterations [2624/199482]\n",
            "Iterations [2656/199482]\n",
            "Iterations [2688/199482]\n",
            "Iterations [2720/199482]\n",
            "Iterations [2752/199482]\n",
            "Iterations [2784/199482]\n",
            "Iterations [2816/199482]\n",
            "Iterations [2848/199482]\n",
            "Iterations [2880/199482]\n",
            "Iterations [2912/199482]\n",
            "Iterations [2944/199482]\n",
            "Iterations [2976/199482]\n",
            "Iterations [3008/199482]\n",
            "Iterations [3040/199482]\n",
            "Iterations [3072/199482]\n",
            "Iterations [3104/199482]\n",
            "Iterations [3136/199482]\n",
            "Iterations [3168/199482]\n",
            "Iterations [3200/199482]\n",
            "Iterations [3232/199482]\n",
            "Iterations [3264/199482]\n",
            "Iterations [3296/199482]\n",
            "Iterations [3328/199482]\n",
            "Iterations [3360/199482]\n",
            "Iterations [3392/199482]\n",
            "Iterations [3424/199482]\n",
            "Iterations [3456/199482]\n",
            "Iterations [3488/199482]\n",
            "Iterations [3520/199482]\n",
            "Iterations [3552/199482]\n",
            "Iterations [3584/199482]\n",
            "Iterations [3616/199482]\n",
            "Iterations [3648/199482]\n",
            "Iterations [3680/199482]\n",
            "Iterations [3712/199482]\n",
            "Iterations [3744/199482]\n",
            "Iterations [3776/199482]\n",
            "Iterations [3808/199482]\n",
            "Iterations [3840/199482]\n",
            "Iterations [3872/199482]\n",
            "Iterations [3904/199482]\n",
            "Iterations [3936/199482]\n",
            "Iterations [3968/199482]\n",
            "Iterations [4000/199482]\n",
            "Iterations [4032/199482]\n",
            "Iterations [4064/199482]\n",
            "Iterations [4096/199482]\n",
            "Iterations [4128/199482]\n",
            "Iterations [4160/199482]\n",
            "Iterations [4192/199482]\n",
            "Iterations [4224/199482]\n",
            "Iterations [4256/199482]\n",
            "Iterations [4288/199482]\n",
            "Iterations [4320/199482]\n",
            "Iterations [4352/199482]\n",
            "Iterations [4384/199482]\n",
            "Iterations [4416/199482]\n",
            "Iterations [4448/199482]\n",
            "Iterations [4480/199482]\n",
            "Iterations [4512/199482]\n",
            "Iterations [4544/199482]\n",
            "Iterations [4576/199482]\n",
            "Iterations [4608/199482]\n",
            "Iterations [4640/199482]\n",
            "Iterations [4672/199482]\n",
            "Iterations [4704/199482]\n",
            "Iterations [4736/199482]\n",
            "Iterations [4768/199482]\n",
            "Iterations [4800/199482]\n",
            "Iterations [4832/199482]\n",
            "Iterations [4864/199482]\n",
            "Iterations [4896/199482]\n",
            "Iterations [4928/199482]\n",
            "Iterations [4960/199482]\n",
            "Iterations [4992/199482]\n",
            "Iterations [5024/199482]\n",
            "Iterations [5056/199482]\n",
            "Iterations [5088/199482]\n",
            "Iterations [5120/199482]\n",
            "Iterations [5152/199482]\n",
            "Iterations [5184/199482]\n",
            "Iterations [5216/199482]\n",
            "Iterations [5248/199482]\n",
            "Iterations [5280/199482]\n",
            "Iterations [5312/199482]\n",
            "Iterations [5344/199482]\n",
            "Iterations [5376/199482]\n",
            "Iterations [5408/199482]\n",
            "Iterations [5440/199482]\n",
            "Iterations [5472/199482]\n",
            "Iterations [5504/199482]\n",
            "Iterations [5536/199482]\n",
            "Iterations [5568/199482]\n",
            "Iterations [5600/199482]\n",
            "Iterations [5632/199482]\n",
            "Iterations [5664/199482]\n",
            "Iterations [5696/199482]\n",
            "Iterations [5728/199482]\n",
            "Iterations [5760/199482]\n",
            "Iterations [5792/199482]\n",
            "Iterations [5824/199482]\n",
            "Iterations [5856/199482]\n",
            "Iterations [5888/199482]\n",
            "Iterations [5920/199482]\n",
            "Iterations [5952/199482]\n",
            "Iterations [5984/199482]\n",
            "Iterations [6016/199482]\n",
            "Iterations [6048/199482]\n",
            "Iterations [6080/199482]\n",
            "Iterations [6112/199482]\n",
            "Iterations [6144/199482]\n",
            "Iterations [6176/199482]\n",
            "Iterations [6208/199482]\n",
            "Iterations [6240/199482]\n",
            "Iterations [6272/199482]\n",
            "Iterations [6304/199482]\n",
            "Iterations [6336/199482]\n",
            "Iterations [6368/199482]\n",
            "Iterations [6400/199482]\n",
            "Iterations [6432/199482]\n",
            "Iterations [6464/199482]\n",
            "Iterations [6496/199482]\n",
            "Iterations [6528/199482]\n",
            "Iterations [6560/199482]\n",
            "Iterations [6592/199482]\n",
            "Iterations [6624/199482]\n",
            "Iterations [6656/199482]\n",
            "Iterations [6688/199482]\n",
            "Iterations [6720/199482]\n",
            "Iterations [6752/199482]\n",
            "Iterations [6784/199482]\n",
            "Iterations [6816/199482]\n",
            "Iterations [6848/199482]\n",
            "Iterations [6880/199482]\n",
            "Iterations [6912/199482]\n",
            "Iterations [6944/199482]\n",
            "Iterations [6976/199482]\n",
            "Iterations [7008/199482]\n",
            "Iterations [7040/199482]\n",
            "Iterations [7072/199482]\n",
            "Iterations [7104/199482]\n",
            "Iterations [7136/199482]\n",
            "Iterations [7168/199482]\n",
            "Iterations [7200/199482]\n",
            "Iterations [7232/199482]\n",
            "Iterations [7264/199482]\n",
            "Iterations [7296/199482]\n",
            "Iterations [7328/199482]\n",
            "Iterations [7360/199482]\n",
            "Iterations [7392/199482]\n",
            "Iterations [7424/199482]\n",
            "Iterations [7456/199482]\n",
            "Iterations [7488/199482]\n",
            "Iterations [7520/199482]\n",
            "Iterations [7552/199482]\n",
            "Iterations [7584/199482]\n",
            "Iterations [7616/199482]\n",
            "Iterations [7648/199482]\n",
            "Iterations [7680/199482]\n",
            "Iterations [7712/199482]\n",
            "Iterations [7744/199482]\n",
            "Iterations [7776/199482]\n",
            "Iterations [7808/199482]\n",
            "Iterations [7840/199482]\n",
            "Iterations [7872/199482]\n",
            "Iterations [7904/199482]\n",
            "Iterations [7936/199482]\n",
            "Iterations [7968/199482]\n",
            "Iterations [8000/199482]\n",
            "Iterations [8032/199482]\n",
            "Iterations [8064/199482]\n",
            "Iterations [8096/199482]\n",
            "Iterations [8128/199482]\n",
            "Iterations [8160/199482]\n",
            "Iterations [8192/199482]\n",
            "Iterations [8224/199482]\n",
            "Iterations [8256/199482]\n",
            "Iterations [8288/199482]\n",
            "Iterations [8320/199482]\n",
            "Iterations [8352/199482]\n",
            "Iterations [8384/199482]\n",
            "Iterations [8416/199482]\n",
            "Iterations [8448/199482]\n",
            "Iterations [8480/199482]\n",
            "Iterations [8512/199482]\n",
            "Iterations [8544/199482]\n",
            "Iterations [8576/199482]\n",
            "Iterations [8608/199482]\n",
            "Iterations [8640/199482]\n",
            "Iterations [8672/199482]\n",
            "Iterations [8704/199482]\n",
            "Iterations [8736/199482]\n",
            "Iterations [8768/199482]\n",
            "Iterations [8800/199482]\n",
            "Iterations [8832/199482]\n",
            "Iterations [8864/199482]\n",
            "Iterations [8896/199482]\n",
            "Iterations [8928/199482]\n",
            "Iterations [8960/199482]\n",
            "Iterations [8992/199482]\n",
            "Iterations [9024/199482]\n",
            "Iterations [9056/199482]\n",
            "Iterations [9088/199482]\n",
            "Iterations [9120/199482]\n",
            "Iterations [9152/199482]\n",
            "Iterations [9184/199482]\n",
            "Iterations [9216/199482]\n",
            "Iterations [9248/199482]\n",
            "Iterations [9280/199482]\n",
            "Iterations [9312/199482]\n",
            "Iterations [9344/199482]\n",
            "Iterations [9376/199482]\n",
            "Iterations [9408/199482]\n",
            "Iterations [9440/199482]\n",
            "Iterations [9472/199482]\n",
            "Iterations [9504/199482]\n",
            "Iterations [9536/199482]\n",
            "Iterations [9568/199482]\n",
            "Iterations [9600/199482]\n",
            "Iterations [9632/199482]\n",
            "Iterations [9664/199482]\n",
            "Iterations [9696/199482]\n",
            "Iterations [9728/199482]\n",
            "Iterations [9760/199482]\n",
            "Iterations [9792/199482]\n",
            "Iterations [9824/199482]\n",
            "Iterations [9856/199482]\n",
            "Iterations [9888/199482]\n",
            "Iterations [9920/199482]\n",
            "Iterations [9952/199482]\n",
            "Iterations [9984/199482]\n",
            "Iterations [10016/199482]\n",
            "Iterations [10048/199482]\n",
            "Iterations [10080/199482]\n",
            "Iterations [10112/199482]\n",
            "Iterations [10144/199482]\n",
            "Iterations [10176/199482]\n",
            "Iterations [10208/199482]\n",
            "Iterations [10240/199482]\n",
            "Iterations [10272/199482]\n",
            "Iterations [10304/199482]\n",
            "Iterations [10336/199482]\n",
            "Iterations [10368/199482]\n",
            "Iterations [10400/199482]\n",
            "Iterations [10432/199482]\n",
            "Iterations [10464/199482]\n",
            "Iterations [10496/199482]\n",
            "Iterations [10528/199482]\n",
            "Iterations [10560/199482]\n",
            "Iterations [10592/199482]\n",
            "Iterations [10624/199482]\n",
            "Iterations [10656/199482]\n",
            "Iterations [10688/199482]\n",
            "Iterations [10720/199482]\n",
            "Iterations [10752/199482]\n",
            "Iterations [10784/199482]\n",
            "Iterations [10816/199482]\n",
            "Iterations [10848/199482]\n",
            "Iterations [10880/199482]\n",
            "Iterations [10912/199482]\n",
            "Iterations [10944/199482]\n",
            "Iterations [10976/199482]\n",
            "Iterations [11008/199482]\n",
            "Iterations [11040/199482]\n",
            "Iterations [11072/199482]\n",
            "Iterations [11104/199482]\n",
            "Iterations [11136/199482]\n",
            "Iterations [11168/199482]\n",
            "Iterations [11200/199482]\n",
            "Iterations [11232/199482]\n",
            "Iterations [11264/199482]\n",
            "Iterations [11296/199482]\n",
            "Iterations [11328/199482]\n",
            "Iterations [11360/199482]\n",
            "Iterations [11392/199482]\n",
            "Iterations [11424/199482]\n",
            "Iterations [11456/199482]\n",
            "Iterations [11488/199482]\n",
            "Iterations [11520/199482]\n",
            "Iterations [11552/199482]\n",
            "Iterations [11584/199482]\n",
            "Iterations [11616/199482]\n",
            "Iterations [11648/199482]\n",
            "Iterations [11680/199482]\n",
            "Iterations [11712/199482]\n",
            "Iterations [11744/199482]\n",
            "Iterations [11776/199482]\n",
            "Iterations [11808/199482]\n",
            "Iterations [11840/199482]\n",
            "Iterations [11872/199482]\n",
            "Iterations [11904/199482]\n",
            "Iterations [11936/199482]\n",
            "Iterations [11968/199482]\n",
            "Iterations [12000/199482]\n",
            "Iterations [12032/199482]\n",
            "Iterations [12064/199482]\n",
            "Iterations [12096/199482]\n",
            "Iterations [12128/199482]\n",
            "Iterations [12160/199482]\n",
            "Iterations [12192/199482]\n",
            "Iterations [12224/199482]\n",
            "Iterations [12256/199482]\n",
            "Iterations [12288/199482]\n",
            "Iterations [12320/199482]\n",
            "Iterations [12352/199482]\n",
            "Iterations [12384/199482]\n",
            "Iterations [12416/199482]\n",
            "Iterations [12448/199482]\n",
            "Iterations [12480/199482]\n",
            "Iterations [12512/199482]\n",
            "Iterations [12544/199482]\n",
            "Iterations [12576/199482]\n",
            "Iterations [12608/199482]\n",
            "Iterations [12640/199482]\n",
            "Iterations [12672/199482]\n",
            "Iterations [12704/199482]\n",
            "Iterations [12736/199482]\n",
            "Iterations [12768/199482]\n",
            "Iterations [12800/199482]\n",
            "Iterations [12832/199482]\n",
            "Iterations [12864/199482]\n",
            "Iterations [12896/199482]\n",
            "Iterations [12928/199482]\n",
            "Iterations [12960/199482]\n",
            "Iterations [12992/199482]\n",
            "Iterations [13024/199482]\n",
            "Iterations [13056/199482]\n",
            "Iterations [13088/199482]\n",
            "Iterations [13120/199482]\n",
            "Iterations [13152/199482]\n",
            "Iterations [13184/199482]\n",
            "Iterations [13216/199482]\n",
            "Iterations [13248/199482]\n",
            "Iterations [13280/199482]\n",
            "Iterations [13312/199482]\n",
            "Iterations [13344/199482]\n",
            "Iterations [13376/199482]\n",
            "Iterations [13408/199482]\n",
            "Iterations [13440/199482]\n",
            "Iterations [13472/199482]\n",
            "Iterations [13504/199482]\n",
            "Iterations [13536/199482]\n",
            "Iterations [13568/199482]\n",
            "Iterations [13600/199482]\n",
            "Iterations [13632/199482]\n",
            "Iterations [13664/199482]\n",
            "Iterations [13696/199482]\n",
            "Iterations [13728/199482]\n",
            "Iterations [13760/199482]\n",
            "Iterations [13792/199482]\n",
            "Iterations [13824/199482]\n",
            "Iterations [13856/199482]\n",
            "Iterations [13888/199482]\n",
            "Iterations [13920/199482]\n",
            "Iterations [13952/199482]\n",
            "Iterations [13984/199482]\n",
            "Iterations [14016/199482]\n",
            "Iterations [14048/199482]\n",
            "Iterations [14080/199482]\n",
            "Iterations [14112/199482]\n",
            "Iterations [14144/199482]\n",
            "Iterations [14176/199482]\n",
            "Iterations [14208/199482]\n",
            "Iterations [14240/199482]\n",
            "Iterations [14272/199482]\n",
            "Iterations [14304/199482]\n",
            "Iterations [14336/199482]\n",
            "Iterations [14368/199482]\n",
            "Iterations [14400/199482]\n",
            "Iterations [14432/199482]\n",
            "Iterations [14464/199482]\n",
            "Iterations [14496/199482]\n",
            "Iterations [14528/199482]\n",
            "Iterations [14560/199482]\n",
            "Iterations [14592/199482]\n",
            "Iterations [14624/199482]\n",
            "Iterations [14656/199482]\n",
            "Iterations [14688/199482]\n",
            "Iterations [14720/199482]\n",
            "Iterations [14752/199482]\n",
            "Iterations [14784/199482]\n",
            "Iterations [14816/199482]\n",
            "Iterations [14848/199482]\n",
            "Iterations [14880/199482]\n",
            "Iterations [14912/199482]\n",
            "Iterations [14944/199482]\n",
            "Iterations [14976/199482]\n",
            "Iterations [15008/199482]\n",
            "Iterations [15040/199482]\n",
            "Iterations [15072/199482]\n",
            "Iterations [15104/199482]\n",
            "Iterations [15136/199482]\n",
            "Iterations [15168/199482]\n",
            "Iterations [15200/199482]\n",
            "Iterations [15232/199482]\n",
            "Iterations [15264/199482]\n",
            "Iterations [15296/199482]\n",
            "Iterations [15328/199482]\n",
            "Iterations [15360/199482]\n",
            "Iterations [15392/199482]\n",
            "Iterations [15424/199482]\n",
            "Iterations [15456/199482]\n",
            "Iterations [15488/199482]\n",
            "Iterations [15520/199482]\n",
            "Iterations [15552/199482]\n",
            "Iterations [15584/199482]\n",
            "Iterations [15616/199482]\n",
            "Iterations [15648/199482]\n",
            "Iterations [15680/199482]\n",
            "Iterations [15712/199482]\n",
            "Iterations [15744/199482]\n",
            "Iterations [15776/199482]\n",
            "Iterations [15808/199482]\n",
            "Iterations [15840/199482]\n",
            "Iterations [15872/199482]\n",
            "Iterations [15904/199482]\n",
            "Iterations [15936/199482]\n",
            "Iterations [15968/199482]\n",
            "Iterations [16000/199482]\n",
            "Iterations [16032/199482]\n",
            "Iterations [16064/199482]\n",
            "Iterations [16096/199482]\n",
            "Iterations [16128/199482]\n",
            "Iterations [16160/199482]\n",
            "Iterations [16192/199482]\n",
            "Iterations [16224/199482]\n",
            "Iterations [16256/199482]\n",
            "Iterations [16288/199482]\n",
            "Iterations [16320/199482]\n",
            "Iterations [16352/199482]\n",
            "Iterations [16384/199482]\n",
            "Iterations [16416/199482]\n",
            "Iterations [16448/199482]\n",
            "Iterations [16480/199482]\n",
            "Iterations [16512/199482]\n",
            "Iterations [16544/199482]\n",
            "Iterations [16576/199482]\n",
            "Iterations [16608/199482]\n",
            "Iterations [16640/199482]\n",
            "Iterations [16672/199482]\n",
            "Iterations [16704/199482]\n",
            "Iterations [16736/199482]\n",
            "Iterations [16768/199482]\n",
            "Iterations [16800/199482]\n",
            "Iterations [16832/199482]\n",
            "Iterations [16864/199482]\n",
            "Iterations [16896/199482]\n",
            "Iterations [16928/199482]\n",
            "Iterations [16960/199482]\n",
            "Iterations [16992/199482]\n",
            "Iterations [17024/199482]\n",
            "Iterations [17056/199482]\n",
            "Iterations [17088/199482]\n",
            "Iterations [17120/199482]\n",
            "Iterations [17152/199482]\n",
            "Iterations [17184/199482]\n",
            "Iterations [17216/199482]\n",
            "Iterations [17248/199482]\n",
            "Iterations [17280/199482]\n",
            "Iterations [17312/199482]\n",
            "Iterations [17344/199482]\n",
            "Iterations [17376/199482]\n",
            "Iterations [17408/199482]\n",
            "Iterations [17440/199482]\n",
            "Iterations [17472/199482]\n",
            "Iterations [17504/199482]\n",
            "Iterations [17536/199482]\n",
            "Iterations [17568/199482]\n",
            "Iterations [17600/199482]\n",
            "Iterations [17632/199482]\n",
            "Iterations [17664/199482]\n",
            "Iterations [17696/199482]\n",
            "Iterations [17728/199482]\n",
            "Iterations [17760/199482]\n",
            "Iterations [17792/199482]\n",
            "Iterations [17824/199482]\n",
            "Iterations [17856/199482]\n",
            "Iterations [17888/199482]\n",
            "Iterations [17920/199482]\n",
            "Iterations [17952/199482]\n",
            "Iterations [17984/199482]\n",
            "Iterations [18016/199482]\n",
            "Iterations [18048/199482]\n",
            "Iterations [18080/199482]\n",
            "Iterations [18112/199482]\n",
            "Iterations [18144/199482]\n",
            "Iterations [18176/199482]\n",
            "Iterations [18208/199482]\n",
            "Iterations [18240/199482]\n",
            "Iterations [18272/199482]\n",
            "Iterations [18304/199482]\n",
            "Iterations [18336/199482]\n",
            "Iterations [18368/199482]\n",
            "Iterations [18400/199482]\n",
            "Iterations [18432/199482]\n",
            "Iterations [18464/199482]\n",
            "Iterations [18496/199482]\n",
            "Iterations [18528/199482]\n",
            "Iterations [18560/199482]\n",
            "Iterations [18592/199482]\n",
            "Iterations [18624/199482]\n",
            "Iterations [18656/199482]\n",
            "Iterations [18688/199482]\n",
            "Iterations [18720/199482]\n",
            "Iterations [18752/199482]\n",
            "Iterations [18784/199482]\n",
            "Iterations [18816/199482]\n",
            "Iterations [18848/199482]\n",
            "Iterations [18880/199482]\n",
            "Iterations [18912/199482]\n",
            "Iterations [18944/199482]\n",
            "Iterations [18976/199482]\n",
            "Iterations [19008/199482]\n",
            "Iterations [19040/199482]\n",
            "Iterations [19072/199482]\n",
            "Iterations [19104/199482]\n",
            "Iterations [19136/199482]\n",
            "Iterations [19168/199482]\n",
            "Iterations [19200/199482]\n",
            "Iterations [19232/199482]\n",
            "Iterations [19264/199482]\n",
            "Iterations [19296/199482]\n",
            "Iterations [19328/199482]\n",
            "Iterations [19360/199482]\n",
            "Iterations [19392/199482]\n",
            "Iterations [19424/199482]\n",
            "Iterations [19456/199482]\n",
            "Iterations [19488/199482]\n",
            "Iterations [19520/199482]\n",
            "Iterations [19552/199482]\n",
            "Iterations [19584/199482]\n",
            "Iterations [19616/199482]\n",
            "Iterations [19648/199482]\n",
            "Iterations [19680/199482]\n",
            "Iterations [19712/199482]\n",
            "Iterations [19744/199482]\n",
            "Iterations [19776/199482]\n",
            "Iterations [19808/199482]\n",
            "Iterations [19840/199482]\n",
            "Iterations [19872/199482]\n",
            "Iterations [19904/199482]\n",
            "Iterations [19936/199482]\n",
            "Iterations [19968/199482]\n",
            "Iterations [20000/199482]\n",
            "Iterations [20032/199482]\n",
            "Iterations [20064/199482]\n",
            "Iterations [20096/199482]\n",
            "Iterations [20128/199482]\n",
            "Iterations [20160/199482]\n",
            "Iterations [20192/199482]\n",
            "Iterations [20224/199482]\n",
            "Iterations [20256/199482]\n",
            "Iterations [20288/199482]\n",
            "Iterations [20320/199482]\n",
            "Iterations [20352/199482]\n",
            "Iterations [20384/199482]\n",
            "Iterations [20416/199482]\n",
            "Iterations [20448/199482]\n",
            "Iterations [20480/199482]\n",
            "Iterations [20512/199482]\n",
            "Iterations [20544/199482]\n",
            "Iterations [20576/199482]\n",
            "Iterations [20608/199482]\n",
            "Iterations [20640/199482]\n",
            "Iterations [20672/199482]\n",
            "Iterations [20704/199482]\n",
            "Iterations [20736/199482]\n",
            "Iterations [20768/199482]\n",
            "Iterations [20800/199482]\n",
            "Iterations [20832/199482]\n",
            "Iterations [20864/199482]\n",
            "Iterations [20896/199482]\n",
            "Iterations [20928/199482]\n",
            "Iterations [20960/199482]\n",
            "Iterations [20992/199482]\n",
            "Iterations [21024/199482]\n",
            "Iterations [21056/199482]\n",
            "Iterations [21088/199482]\n",
            "Iterations [21120/199482]\n",
            "Iterations [21152/199482]\n",
            "Iterations [21184/199482]\n",
            "Iterations [21216/199482]\n",
            "Iterations [21248/199482]\n",
            "Iterations [21280/199482]\n",
            "Iterations [21312/199482]\n",
            "Iterations [21344/199482]\n",
            "Iterations [21376/199482]\n",
            "Iterations [21408/199482]\n",
            "Iterations [21440/199482]\n",
            "Iterations [21472/199482]\n",
            "Iterations [21504/199482]\n",
            "Iterations [21536/199482]\n",
            "Iterations [21568/199482]\n",
            "Iterations [21600/199482]\n",
            "Iterations [21632/199482]\n",
            "Iterations [21664/199482]\n",
            "Iterations [21696/199482]\n",
            "Iterations [21728/199482]\n",
            "Iterations [21760/199482]\n",
            "Iterations [21792/199482]\n",
            "Iterations [21824/199482]\n",
            "Iterations [21856/199482]\n",
            "Iterations [21888/199482]\n",
            "Iterations [21920/199482]\n",
            "Iterations [21952/199482]\n",
            "Iterations [21984/199482]\n",
            "Iterations [22016/199482]\n",
            "Iterations [22048/199482]\n",
            "Iterations [22080/199482]\n",
            "Iterations [22112/199482]\n",
            "Iterations [22144/199482]\n",
            "Iterations [22176/199482]\n",
            "Iterations [22208/199482]\n",
            "Iterations [22240/199482]\n",
            "Iterations [22272/199482]\n",
            "Iterations [22304/199482]\n",
            "Iterations [22336/199482]\n",
            "Iterations [22368/199482]\n",
            "Iterations [22400/199482]\n",
            "Iterations [22432/199482]\n",
            "Iterations [22464/199482]\n",
            "Iterations [22496/199482]\n",
            "Iterations [22528/199482]\n",
            "Iterations [22560/199482]\n",
            "Iterations [22592/199482]\n",
            "Iterations [22624/199482]\n",
            "Iterations [22656/199482]\n",
            "Iterations [22688/199482]\n",
            "Iterations [22720/199482]\n",
            "Iterations [22752/199482]\n",
            "Iterations [22784/199482]\n",
            "Iterations [22816/199482]\n",
            "Iterations [22848/199482]\n",
            "Iterations [22880/199482]\n",
            "Iterations [22912/199482]\n",
            "Iterations [22944/199482]\n",
            "Iterations [22976/199482]\n",
            "Iterations [23008/199482]\n",
            "Iterations [23040/199482]\n",
            "Iterations [23072/199482]\n",
            "Iterations [23104/199482]\n",
            "Iterations [23136/199482]\n",
            "Iterations [23168/199482]\n",
            "Iterations [23200/199482]\n",
            "Iterations [23232/199482]\n",
            "Iterations [23264/199482]\n",
            "Iterations [23296/199482]\n",
            "Iterations [23328/199482]\n",
            "Iterations [23360/199482]\n",
            "Iterations [23392/199482]\n",
            "Iterations [23424/199482]\n",
            "Iterations [23456/199482]\n",
            "Iterations [23488/199482]\n",
            "Iterations [23520/199482]\n",
            "Iterations [23552/199482]\n",
            "Iterations [23584/199482]\n",
            "Iterations [23616/199482]\n",
            "Iterations [23648/199482]\n",
            "Iterations [23680/199482]\n",
            "Iterations [23712/199482]\n",
            "Iterations [23744/199482]\n",
            "Iterations [23776/199482]\n",
            "Iterations [23808/199482]\n",
            "Iterations [23840/199482]\n",
            "Iterations [23872/199482]\n",
            "Iterations [23904/199482]\n",
            "Iterations [23936/199482]\n",
            "Iterations [23968/199482]\n",
            "Iterations [24000/199482]\n",
            "Iterations [24032/199482]\n",
            "Iterations [24064/199482]\n",
            "Iterations [24096/199482]\n",
            "Iterations [24128/199482]\n",
            "Iterations [24160/199482]\n",
            "Iterations [24192/199482]\n",
            "Iterations [24224/199482]\n",
            "Iterations [24256/199482]\n",
            "Iterations [24288/199482]\n",
            "Iterations [24320/199482]\n",
            "Iterations [24352/199482]\n",
            "Iterations [24384/199482]\n",
            "Iterations [24416/199482]\n",
            "Iterations [24448/199482]\n",
            "Iterations [24480/199482]\n",
            "Iterations [24512/199482]\n",
            "Iterations [24544/199482]\n",
            "Iterations [24576/199482]\n",
            "Iterations [24608/199482]\n",
            "Iterations [24640/199482]\n",
            "Iterations [24672/199482]\n",
            "Iterations [24704/199482]\n",
            "Iterations [24736/199482]\n",
            "Iterations [24768/199482]\n",
            "Iterations [24800/199482]\n",
            "Iterations [24832/199482]\n",
            "Iterations [24864/199482]\n",
            "Iterations [24896/199482]\n",
            "Iterations [24928/199482]\n",
            "Iterations [24960/199482]\n",
            "Iterations [24992/199482]\n",
            "Iterations [25024/199482]\n",
            "Iterations [25056/199482]\n",
            "Iterations [25088/199482]\n",
            "Iterations [25120/199482]\n",
            "Iterations [25152/199482]\n",
            "Iterations [25184/199482]\n",
            "Iterations [25216/199482]\n",
            "Iterations [25248/199482]\n",
            "Iterations [25280/199482]\n",
            "Iterations [25312/199482]\n",
            "Iterations [25344/199482]\n",
            "Iterations [25376/199482]\n",
            "Iterations [25408/199482]\n",
            "Iterations [25440/199482]\n",
            "Iterations [25472/199482]\n",
            "Iterations [25504/199482]\n",
            "Iterations [25536/199482]\n",
            "Iterations [25568/199482]\n",
            "Iterations [25600/199482]\n",
            "Iterations [25632/199482]\n",
            "Iterations [25664/199482]\n",
            "Iterations [25696/199482]\n",
            "Iterations [25728/199482]\n",
            "Iterations [25760/199482]\n",
            "Iterations [25792/199482]\n",
            "Iterations [25824/199482]\n",
            "Iterations [25856/199482]\n",
            "Iterations [25888/199482]\n",
            "Iterations [25920/199482]\n",
            "Iterations [25952/199482]\n",
            "Iterations [25984/199482]\n",
            "Iterations [26016/199482]\n",
            "Iterations [26048/199482]\n",
            "Iterations [26080/199482]\n",
            "Iterations [26112/199482]\n",
            "Iterations [26144/199482]\n",
            "Iterations [26176/199482]\n",
            "Iterations [26208/199482]\n",
            "Iterations [26240/199482]\n",
            "Iterations [26272/199482]\n",
            "Iterations [26304/199482]\n",
            "Iterations [26336/199482]\n",
            "Iterations [26368/199482]\n",
            "Iterations [26400/199482]\n",
            "Iterations [26432/199482]\n",
            "Iterations [26464/199482]\n",
            "Iterations [26496/199482]\n",
            "Iterations [26528/199482]\n",
            "Iterations [26560/199482]\n",
            "Iterations [26592/199482]\n",
            "Iterations [26624/199482]\n",
            "Iterations [26656/199482]\n",
            "Iterations [26688/199482]\n",
            "Iterations [26720/199482]\n",
            "Iterations [26752/199482]\n",
            "Iterations [26784/199482]\n",
            "Iterations [26816/199482]\n",
            "Iterations [26848/199482]\n",
            "Iterations [26880/199482]\n",
            "Iterations [26912/199482]\n",
            "Iterations [26944/199482]\n",
            "Iterations [26976/199482]\n",
            "Iterations [27008/199482]\n",
            "Iterations [27040/199482]\n",
            "Iterations [27072/199482]\n",
            "Iterations [27104/199482]\n",
            "Iterations [27136/199482]\n",
            "Iterations [27168/199482]\n",
            "Iterations [27200/199482]\n",
            "Iterations [27232/199482]\n",
            "Iterations [27264/199482]\n",
            "Iterations [27296/199482]\n",
            "Iterations [27328/199482]\n",
            "Iterations [27360/199482]\n",
            "Iterations [27392/199482]\n",
            "Iterations [27424/199482]\n",
            "Iterations [27456/199482]\n",
            "Iterations [27488/199482]\n",
            "Iterations [27520/199482]\n",
            "Iterations [27552/199482]\n",
            "Iterations [27584/199482]\n",
            "Iterations [27616/199482]\n",
            "Iterations [27648/199482]\n",
            "Iterations [27680/199482]\n",
            "Iterations [27712/199482]\n",
            "Iterations [27744/199482]\n",
            "Iterations [27776/199482]\n",
            "Iterations [27808/199482]\n",
            "Iterations [27840/199482]\n",
            "Iterations [27872/199482]\n",
            "Iterations [27904/199482]\n",
            "Iterations [27936/199482]\n",
            "Iterations [27968/199482]\n",
            "Iterations [28000/199482]\n",
            "Iterations [28032/199482]\n",
            "Iterations [28064/199482]\n",
            "Iterations [28096/199482]\n",
            "Iterations [28128/199482]\n",
            "Iterations [28160/199482]\n",
            "Iterations [28192/199482]\n",
            "Iterations [28224/199482]\n",
            "Iterations [28256/199482]\n",
            "Iterations [28288/199482]\n",
            "Iterations [28320/199482]\n",
            "Iterations [28352/199482]\n",
            "Iterations [28384/199482]\n",
            "Iterations [28416/199482]\n",
            "Iterations [28448/199482]\n",
            "Iterations [28480/199482]\n",
            "Iterations [28512/199482]\n",
            "Iterations [28544/199482]\n",
            "Iterations [28576/199482]\n",
            "Iterations [28608/199482]\n",
            "Iterations [28640/199482]\n",
            "Iterations [28672/199482]\n",
            "Iterations [28704/199482]\n",
            "Iterations [28736/199482]\n",
            "Iterations [28768/199482]\n",
            "Iterations [28800/199482]\n",
            "Iterations [28832/199482]\n",
            "Iterations [28864/199482]\n",
            "Iterations [28896/199482]\n",
            "Iterations [28928/199482]\n",
            "Iterations [28960/199482]\n",
            "Iterations [28992/199482]\n",
            "Iterations [29024/199482]\n",
            "Iterations [29056/199482]\n",
            "Iterations [29088/199482]\n",
            "Iterations [29120/199482]\n",
            "Iterations [29152/199482]\n",
            "Iterations [29184/199482]\n",
            "Iterations [29216/199482]\n",
            "Iterations [29248/199482]\n",
            "Iterations [29280/199482]\n",
            "Iterations [29312/199482]\n",
            "Iterations [29344/199482]\n",
            "Iterations [29376/199482]\n",
            "Iterations [29408/199482]\n",
            "Iterations [29440/199482]\n",
            "Iterations [29472/199482]\n",
            "Iterations [29504/199482]\n",
            "Iterations [29536/199482]\n",
            "Iterations [29568/199482]\n",
            "Iterations [29600/199482]\n",
            "Iterations [29632/199482]\n",
            "Iterations [29664/199482]\n",
            "Iterations [29696/199482]\n",
            "Iterations [29728/199482]\n",
            "Iterations [29760/199482]\n",
            "Iterations [29792/199482]\n",
            "Iterations [29824/199482]\n",
            "Iterations [29856/199482]\n",
            "Iterations [29888/199482]\n",
            "Iterations [29920/199482]\n",
            "Iterations [29952/199482]\n",
            "Iterations [29984/199482]\n",
            "Iterations [30016/199482]\n",
            "Iterations [30048/199482]\n",
            "Iterations [30080/199482]\n",
            "Iterations [30112/199482]\n",
            "Iterations [30144/199482]\n",
            "Iterations [30176/199482]\n",
            "Iterations [30208/199482]\n",
            "Iterations [30240/199482]\n",
            "Iterations [30272/199482]\n",
            "Iterations [30304/199482]\n",
            "Iterations [30336/199482]\n",
            "Iterations [30368/199482]\n",
            "Iterations [30400/199482]\n",
            "Iterations [30432/199482]\n",
            "Iterations [30464/199482]\n",
            "Iterations [30496/199482]\n",
            "Iterations [30528/199482]\n",
            "Iterations [30560/199482]\n",
            "Iterations [30592/199482]\n",
            "Iterations [30624/199482]\n",
            "Iterations [30656/199482]\n",
            "Iterations [30688/199482]\n",
            "Iterations [30720/199482]\n",
            "Iterations [30752/199482]\n",
            "Iterations [30784/199482]\n",
            "Iterations [30816/199482]\n",
            "Iterations [30848/199482]\n",
            "Iterations [30880/199482]\n",
            "Iterations [30912/199482]\n",
            "Iterations [30944/199482]\n",
            "Iterations [30976/199482]\n",
            "Iterations [31008/199482]\n",
            "Iterations [31040/199482]\n",
            "Iterations [31072/199482]\n",
            "Iterations [31104/199482]\n",
            "Iterations [31136/199482]\n",
            "Iterations [31168/199482]\n",
            "Iterations [31200/199482]\n",
            "Iterations [31232/199482]\n",
            "Iterations [31264/199482]\n",
            "Iterations [31296/199482]\n",
            "Iterations [31328/199482]\n",
            "Iterations [31360/199482]\n",
            "Iterations [31392/199482]\n",
            "Iterations [31424/199482]\n",
            "Iterations [31456/199482]\n",
            "Iterations [31488/199482]\n",
            "Iterations [31520/199482]\n",
            "Iterations [31552/199482]\n",
            "Iterations [31584/199482]\n",
            "Iterations [31616/199482]\n",
            "Iterations [31648/199482]\n",
            "Iterations [31680/199482]\n",
            "Iterations [31712/199482]\n",
            "Iterations [31744/199482]\n",
            "Iterations [31776/199482]\n",
            "Iterations [31808/199482]\n",
            "Iterations [31840/199482]\n",
            "Iterations [31872/199482]\n",
            "Iterations [31904/199482]\n",
            "Iterations [31936/199482]\n",
            "Iterations [31968/199482]\n",
            "Iterations [32000/199482]\n",
            "Iterations [32032/199482]\n",
            "Iterations [32064/199482]\n",
            "Iterations [32096/199482]\n",
            "Iterations [32128/199482]\n",
            "Iterations [32160/199482]\n",
            "Iterations [32192/199482]\n",
            "Iterations [32224/199482]\n",
            "Iterations [32256/199482]\n",
            "Iterations [32288/199482]\n",
            "Iterations [32320/199482]\n",
            "Iterations [32352/199482]\n",
            "Iterations [32384/199482]\n",
            "Iterations [32416/199482]\n",
            "Iterations [32448/199482]\n",
            "Iterations [32480/199482]\n",
            "Iterations [32512/199482]\n",
            "Iterations [32544/199482]\n",
            "Iterations [32576/199482]\n",
            "Iterations [32608/199482]\n",
            "Iterations [32640/199482]\n",
            "Iterations [32672/199482]\n",
            "Iterations [32704/199482]\n",
            "Iterations [32736/199482]\n",
            "Iterations [32768/199482]\n",
            "Iterations [32800/199482]\n",
            "Iterations [32832/199482]\n",
            "Iterations [32864/199482]\n",
            "Iterations [32896/199482]\n",
            "Iterations [32928/199482]\n",
            "Iterations [32960/199482]\n",
            "Iterations [32992/199482]\n",
            "Iterations [33024/199482]\n",
            "Iterations [33056/199482]\n",
            "Iterations [33088/199482]\n",
            "Iterations [33120/199482]\n",
            "Iterations [33152/199482]\n",
            "Iterations [33184/199482]\n",
            "Iterations [33216/199482]\n",
            "Iterations [33248/199482]\n",
            "Iterations [33280/199482]\n",
            "Iterations [33312/199482]\n",
            "Iterations [33344/199482]\n",
            "Iterations [33376/199482]\n",
            "Iterations [33408/199482]\n",
            "Iterations [33440/199482]\n",
            "Iterations [33472/199482]\n",
            "Iterations [33504/199482]\n",
            "Iterations [33536/199482]\n",
            "Iterations [33568/199482]\n",
            "Iterations [33600/199482]\n",
            "Iterations [33632/199482]\n",
            "Iterations [33664/199482]\n",
            "Iterations [33696/199482]\n",
            "Iterations [33728/199482]\n",
            "Iterations [33760/199482]\n",
            "Iterations [33792/199482]\n",
            "Iterations [33824/199482]\n",
            "Iterations [33856/199482]\n",
            "Iterations [33888/199482]\n",
            "Iterations [33920/199482]\n",
            "Iterations [33952/199482]\n",
            "Iterations [33984/199482]\n",
            "Iterations [34016/199482]\n",
            "Iterations [34048/199482]\n",
            "Iterations [34080/199482]\n",
            "Iterations [34112/199482]\n",
            "Iterations [34144/199482]\n",
            "Iterations [34176/199482]\n",
            "Iterations [34208/199482]\n",
            "Iterations [34240/199482]\n",
            "Iterations [34272/199482]\n",
            "Iterations [34304/199482]\n",
            "Iterations [34336/199482]\n",
            "Iterations [34368/199482]\n",
            "Iterations [34400/199482]\n",
            "Iterations [34432/199482]\n",
            "Iterations [34464/199482]\n",
            "Iterations [34496/199482]\n",
            "Iterations [34528/199482]\n",
            "Iterations [34560/199482]\n",
            "Iterations [34592/199482]\n",
            "Iterations [34624/199482]\n",
            "Iterations [34656/199482]\n",
            "Iterations [34688/199482]\n",
            "Iterations [34720/199482]\n",
            "Iterations [34752/199482]\n",
            "Iterations [34784/199482]\n",
            "Iterations [34816/199482]\n",
            "Iterations [34848/199482]\n",
            "Iterations [34880/199482]\n",
            "Iterations [34912/199482]\n",
            "Iterations [34944/199482]\n",
            "Iterations [34976/199482]\n",
            "Iterations [35008/199482]\n",
            "Iterations [35040/199482]\n",
            "Iterations [35072/199482]\n",
            "Iterations [35104/199482]\n",
            "Iterations [35136/199482]\n",
            "Iterations [35168/199482]\n",
            "Iterations [35200/199482]\n",
            "Iterations [35232/199482]\n",
            "Iterations [35264/199482]\n",
            "Iterations [35296/199482]\n",
            "Iterations [35328/199482]\n",
            "Iterations [35360/199482]\n",
            "Iterations [35392/199482]\n",
            "Iterations [35424/199482]\n",
            "Iterations [35456/199482]\n",
            "Iterations [35488/199482]\n",
            "Iterations [35520/199482]\n",
            "Iterations [35552/199482]\n",
            "Iterations [35584/199482]\n",
            "Iterations [35616/199482]\n",
            "Iterations [35648/199482]\n",
            "Iterations [35680/199482]\n",
            "Iterations [35712/199482]\n",
            "Iterations [35744/199482]\n",
            "Iterations [35776/199482]\n",
            "Iterations [35808/199482]\n",
            "Iterations [35840/199482]\n",
            "Iterations [35872/199482]\n",
            "Iterations [35904/199482]\n",
            "Iterations [35936/199482]\n",
            "Iterations [35968/199482]\n",
            "Iterations [36000/199482]\n",
            "Iterations [36032/199482]\n",
            "Iterations [36064/199482]\n",
            "Iterations [36096/199482]\n",
            "Iterations [36128/199482]\n",
            "Iterations [36160/199482]\n",
            "Iterations [36192/199482]\n",
            "Iterations [36224/199482]\n",
            "Iterations [36256/199482]\n",
            "Iterations [36288/199482]\n",
            "Iterations [36320/199482]\n",
            "Iterations [36352/199482]\n",
            "Iterations [36384/199482]\n",
            "Iterations [36416/199482]\n",
            "Iterations [36448/199482]\n",
            "Iterations [36480/199482]\n",
            "Iterations [36512/199482]\n",
            "Iterations [36544/199482]\n",
            "Iterations [36576/199482]\n",
            "Iterations [36608/199482]\n",
            "Iterations [36640/199482]\n",
            "Iterations [36672/199482]\n",
            "Iterations [36704/199482]\n",
            "Iterations [36736/199482]\n",
            "Iterations [36768/199482]\n",
            "Iterations [36800/199482]\n",
            "Iterations [36832/199482]\n",
            "Iterations [36864/199482]\n",
            "Iterations [36896/199482]\n",
            "Iterations [36928/199482]\n",
            "Iterations [36960/199482]\n",
            "Iterations [36992/199482]\n",
            "Iterations [37024/199482]\n",
            "Iterations [37056/199482]\n",
            "Iterations [37088/199482]\n",
            "Iterations [37120/199482]\n",
            "Iterations [37152/199482]\n",
            "Iterations [37184/199482]\n",
            "Iterations [37216/199482]\n",
            "Iterations [37248/199482]\n",
            "Iterations [37280/199482]\n",
            "Iterations [37312/199482]\n",
            "Iterations [37344/199482]\n",
            "Iterations [37376/199482]\n",
            "Iterations [37408/199482]\n",
            "Iterations [37440/199482]\n",
            "Iterations [37472/199482]\n",
            "Iterations [37504/199482]\n",
            "Iterations [37536/199482]\n",
            "Iterations [37568/199482]\n",
            "Iterations [37600/199482]\n",
            "Iterations [37632/199482]\n",
            "Iterations [37664/199482]\n",
            "Iterations [37696/199482]\n",
            "Iterations [37728/199482]\n",
            "Iterations [37760/199482]\n",
            "Iterations [37792/199482]\n",
            "Iterations [37824/199482]\n",
            "Iterations [37856/199482]\n",
            "Iterations [37888/199482]\n",
            "Iterations [37920/199482]\n",
            "Iterations [37952/199482]\n",
            "Iterations [37984/199482]\n",
            "Iterations [38016/199482]\n",
            "Iterations [38048/199482]\n",
            "Iterations [38080/199482]\n",
            "Iterations [38112/199482]\n",
            "Iterations [38144/199482]\n",
            "Iterations [38176/199482]\n",
            "Iterations [38208/199482]\n",
            "Iterations [38240/199482]\n",
            "Iterations [38272/199482]\n",
            "Iterations [38304/199482]\n",
            "Iterations [38336/199482]\n",
            "Iterations [38368/199482]\n",
            "Iterations [38400/199482]\n",
            "Iterations [38432/199482]\n",
            "Iterations [38464/199482]\n",
            "Iterations [38496/199482]\n",
            "Iterations [38528/199482]\n",
            "Iterations [38560/199482]\n",
            "Iterations [38592/199482]\n",
            "Iterations [38624/199482]\n",
            "Iterations [38656/199482]\n",
            "Iterations [38688/199482]\n",
            "Iterations [38720/199482]\n",
            "Iterations [38752/199482]\n",
            "Iterations [38784/199482]\n",
            "Iterations [38816/199482]\n",
            "Iterations [38848/199482]\n",
            "Iterations [38880/199482]\n",
            "Iterations [38912/199482]\n",
            "Iterations [38944/199482]\n",
            "Iterations [38976/199482]\n",
            "Iterations [39008/199482]\n",
            "Iterations [39040/199482]\n",
            "Iterations [39072/199482]\n",
            "Iterations [39104/199482]\n",
            "Iterations [39136/199482]\n",
            "Iterations [39168/199482]\n",
            "Iterations [39200/199482]\n",
            "Iterations [39232/199482]\n",
            "Iterations [39264/199482]\n",
            "Iterations [39296/199482]\n",
            "Iterations [39328/199482]\n",
            "Iterations [39360/199482]\n",
            "Iterations [39392/199482]\n",
            "Iterations [39424/199482]\n",
            "Iterations [39456/199482]\n",
            "Iterations [39488/199482]\n",
            "Iterations [39520/199482]\n",
            "Iterations [39552/199482]\n",
            "Iterations [39584/199482]\n",
            "Iterations [39616/199482]\n",
            "Iterations [39648/199482]\n",
            "Iterations [39680/199482]\n",
            "Iterations [39712/199482]\n",
            "Iterations [39744/199482]\n",
            "Iterations [39776/199482]\n",
            "Iterations [39808/199482]\n",
            "Iterations [39840/199482]\n",
            "Iterations [39872/199482]\n",
            "Iterations [39904/199482]\n",
            "Iterations [39936/199482]\n",
            "Iterations [39968/199482]\n",
            "Iterations [40000/199482]\n",
            "Iterations [40032/199482]\n",
            "Iterations [40064/199482]\n",
            "Iterations [40096/199482]\n",
            "Iterations [40128/199482]\n",
            "Iterations [40160/199482]\n",
            "Iterations [40192/199482]\n",
            "Iterations [40224/199482]\n",
            "Iterations [40256/199482]\n",
            "Iterations [40288/199482]\n",
            "Iterations [40320/199482]\n",
            "Iterations [40352/199482]\n",
            "Iterations [40384/199482]\n",
            "Iterations [40416/199482]\n",
            "Iterations [40448/199482]\n",
            "Iterations [40480/199482]\n",
            "Iterations [40512/199482]\n",
            "Iterations [40544/199482]\n",
            "Iterations [40576/199482]\n",
            "Iterations [40608/199482]\n",
            "Iterations [40640/199482]\n",
            "Iterations [40672/199482]\n",
            "Iterations [40704/199482]\n",
            "Iterations [40736/199482]\n",
            "Iterations [40768/199482]\n",
            "Iterations [40800/199482]\n",
            "Iterations [40832/199482]\n",
            "Iterations [40864/199482]\n",
            "Iterations [40896/199482]\n",
            "Iterations [40928/199482]\n",
            "Iterations [40960/199482]\n",
            "Iterations [40992/199482]\n",
            "Iterations [41024/199482]\n",
            "Iterations [41056/199482]\n",
            "Iterations [41088/199482]\n",
            "Iterations [41120/199482]\n",
            "Iterations [41152/199482]\n",
            "Iterations [41184/199482]\n",
            "Iterations [41216/199482]\n",
            "Iterations [41248/199482]\n",
            "Iterations [41280/199482]\n",
            "Iterations [41312/199482]\n",
            "Iterations [41344/199482]\n",
            "Iterations [41376/199482]\n",
            "Iterations [41408/199482]\n",
            "Iterations [41440/199482]\n",
            "Iterations [41472/199482]\n",
            "Iterations [41504/199482]\n",
            "Iterations [41536/199482]\n",
            "Iterations [41568/199482]\n",
            "Iterations [41600/199482]\n",
            "Iterations [41632/199482]\n",
            "Iterations [41664/199482]\n",
            "Iterations [41696/199482]\n",
            "Iterations [41728/199482]\n",
            "Iterations [41760/199482]\n",
            "Iterations [41792/199482]\n",
            "Iterations [41824/199482]\n",
            "Iterations [41856/199482]\n",
            "Iterations [41888/199482]\n",
            "Iterations [41920/199482]\n",
            "Iterations [41952/199482]\n",
            "Iterations [41984/199482]\n",
            "Iterations [42016/199482]\n",
            "Iterations [42048/199482]\n",
            "Iterations [42080/199482]\n",
            "Iterations [42112/199482]\n",
            "Iterations [42144/199482]\n",
            "Iterations [42176/199482]\n",
            "Iterations [42208/199482]\n",
            "Iterations [42240/199482]\n",
            "Iterations [42272/199482]\n",
            "Iterations [42304/199482]\n",
            "Iterations [42336/199482]\n",
            "Iterations [42368/199482]\n",
            "Iterations [42400/199482]\n",
            "Iterations [42432/199482]\n",
            "Iterations [42464/199482]\n",
            "Iterations [42496/199482]\n",
            "Iterations [42528/199482]\n",
            "Iterations [42560/199482]\n",
            "Iterations [42592/199482]\n",
            "Iterations [42624/199482]\n",
            "Iterations [42656/199482]\n",
            "Iterations [42688/199482]\n",
            "Iterations [42720/199482]\n",
            "Iterations [42752/199482]\n",
            "Iterations [42784/199482]\n",
            "Iterations [42816/199482]\n",
            "Iterations [42848/199482]\n",
            "Iterations [42880/199482]\n",
            "Iterations [42912/199482]\n",
            "Iterations [42944/199482]\n",
            "Iterations [42976/199482]\n",
            "Iterations [43008/199482]\n",
            "Iterations [43040/199482]\n",
            "Iterations [43072/199482]\n",
            "Iterations [43104/199482]\n",
            "Iterations [43136/199482]\n",
            "Iterations [43168/199482]\n",
            "Iterations [43200/199482]\n",
            "Iterations [43232/199482]\n",
            "Iterations [43264/199482]\n",
            "Iterations [43296/199482]\n",
            "Iterations [43328/199482]\n",
            "Iterations [43360/199482]\n",
            "Iterations [43392/199482]\n",
            "Iterations [43424/199482]\n",
            "Iterations [43456/199482]\n",
            "Iterations [43488/199482]\n",
            "Iterations [43520/199482]\n",
            "Iterations [43552/199482]\n",
            "Iterations [43584/199482]\n",
            "Iterations [43616/199482]\n",
            "Iterations [43648/199482]\n",
            "Iterations [43680/199482]\n",
            "Iterations [43712/199482]\n",
            "Iterations [43744/199482]\n",
            "Iterations [43776/199482]\n",
            "Iterations [43808/199482]\n",
            "Iterations [43840/199482]\n",
            "Iterations [43872/199482]\n",
            "Iterations [43904/199482]\n",
            "Iterations [43936/199482]\n",
            "Iterations [43968/199482]\n",
            "Iterations [44000/199482]\n",
            "Iterations [44032/199482]\n",
            "Iterations [44064/199482]\n",
            "Iterations [44096/199482]\n",
            "Iterations [44128/199482]\n",
            "Iterations [44160/199482]\n",
            "Iterations [44192/199482]\n",
            "Iterations [44224/199482]\n",
            "Iterations [44256/199482]\n",
            "Iterations [44288/199482]\n",
            "Iterations [44320/199482]\n",
            "Iterations [44352/199482]\n",
            "Iterations [44384/199482]\n",
            "Iterations [44416/199482]\n",
            "Iterations [44448/199482]\n",
            "Iterations [44480/199482]\n",
            "Iterations [44512/199482]\n",
            "Iterations [44544/199482]\n",
            "Iterations [44576/199482]\n",
            "Iterations [44608/199482]\n",
            "Iterations [44640/199482]\n",
            "Iterations [44672/199482]\n",
            "Iterations [44704/199482]\n",
            "Iterations [44736/199482]\n",
            "Iterations [44768/199482]\n",
            "Iterations [44800/199482]\n",
            "Iterations [44832/199482]\n",
            "Iterations [44864/199482]\n",
            "Iterations [44896/199482]\n",
            "Iterations [44928/199482]\n",
            "Iterations [44960/199482]\n",
            "Iterations [44992/199482]\n",
            "Iterations [45024/199482]\n",
            "Iterations [45056/199482]\n",
            "Iterations [45088/199482]\n",
            "Iterations [45120/199482]\n",
            "Iterations [45152/199482]\n",
            "Iterations [45184/199482]\n",
            "Iterations [45216/199482]\n",
            "Iterations [45248/199482]\n",
            "Iterations [45280/199482]\n",
            "Iterations [45312/199482]\n",
            "Iterations [45344/199482]\n",
            "Iterations [45376/199482]\n",
            "Iterations [45408/199482]\n",
            "Iterations [45440/199482]\n",
            "Iterations [45472/199482]\n",
            "Iterations [45504/199482]\n",
            "Iterations [45536/199482]\n",
            "Iterations [45568/199482]\n",
            "Iterations [45600/199482]\n",
            "Iterations [45632/199482]\n",
            "Iterations [45664/199482]\n",
            "Iterations [45696/199482]\n",
            "Iterations [45728/199482]\n",
            "Iterations [45760/199482]\n",
            "Iterations [45792/199482]\n",
            "Iterations [45824/199482]\n",
            "Iterations [45856/199482]\n",
            "Iterations [45888/199482]\n",
            "Iterations [45920/199482]\n",
            "Iterations [45952/199482]\n",
            "Iterations [45984/199482]\n",
            "Iterations [46016/199482]\n",
            "Iterations [46048/199482]\n",
            "Iterations [46080/199482]\n",
            "Iterations [46112/199482]\n",
            "Iterations [46144/199482]\n",
            "Iterations [46176/199482]\n",
            "Iterations [46208/199482]\n",
            "Iterations [46240/199482]\n",
            "Iterations [46272/199482]\n",
            "Iterations [46304/199482]\n",
            "Iterations [46336/199482]\n",
            "Iterations [46368/199482]\n",
            "Iterations [46400/199482]\n",
            "Iterations [46432/199482]\n",
            "Iterations [46464/199482]\n",
            "Iterations [46496/199482]\n",
            "Iterations [46528/199482]\n",
            "Iterations [46560/199482]\n",
            "Iterations [46592/199482]\n",
            "Iterations [46624/199482]\n",
            "Iterations [46656/199482]\n",
            "Iterations [46688/199482]\n",
            "Iterations [46720/199482]\n",
            "Iterations [46752/199482]\n",
            "Iterations [46784/199482]\n",
            "Iterations [46816/199482]\n",
            "Iterations [46848/199482]\n",
            "Iterations [46880/199482]\n",
            "Iterations [46912/199482]\n",
            "Iterations [46944/199482]\n",
            "Iterations [46976/199482]\n",
            "Iterations [47008/199482]\n",
            "Iterations [47040/199482]\n",
            "Iterations [47072/199482]\n",
            "Iterations [47104/199482]\n",
            "Iterations [47136/199482]\n",
            "Iterations [47168/199482]\n",
            "Iterations [47200/199482]\n",
            "Iterations [47232/199482]\n",
            "Iterations [47264/199482]\n",
            "Iterations [47296/199482]\n",
            "Iterations [47328/199482]\n",
            "Iterations [47360/199482]\n",
            "Iterations [47392/199482]\n",
            "Iterations [47424/199482]\n",
            "Iterations [47456/199482]\n",
            "Iterations [47488/199482]\n",
            "Iterations [47520/199482]\n",
            "Iterations [47552/199482]\n",
            "Iterations [47584/199482]\n",
            "Iterations [47616/199482]\n",
            "Iterations [47648/199482]\n",
            "Iterations [47680/199482]\n",
            "Iterations [47712/199482]\n",
            "Iterations [47744/199482]\n",
            "Iterations [47776/199482]\n",
            "Iterations [47808/199482]\n",
            "Iterations [47840/199482]\n",
            "Iterations [47872/199482]\n",
            "Iterations [47904/199482]\n",
            "Iterations [47936/199482]\n",
            "Iterations [47968/199482]\n",
            "Iterations [48000/199482]\n",
            "Iterations [48032/199482]\n",
            "Iterations [48064/199482]\n",
            "Iterations [48096/199482]\n",
            "Iterations [48128/199482]\n",
            "Iterations [48160/199482]\n",
            "Iterations [48192/199482]\n",
            "Iterations [48224/199482]\n",
            "Iterations [48256/199482]\n",
            "Iterations [48288/199482]\n",
            "Iterations [48320/199482]\n",
            "Iterations [48352/199482]\n",
            "Iterations [48384/199482]\n",
            "Iterations [48416/199482]\n",
            "Iterations [48448/199482]\n",
            "Iterations [48480/199482]\n",
            "Iterations [48512/199482]\n",
            "Iterations [48544/199482]\n",
            "Iterations [48576/199482]\n",
            "Iterations [48608/199482]\n",
            "Iterations [48640/199482]\n",
            "Iterations [48672/199482]\n",
            "Iterations [48704/199482]\n",
            "Iterations [48736/199482]\n",
            "Iterations [48768/199482]\n",
            "Iterations [48800/199482]\n",
            "Iterations [48832/199482]\n",
            "Iterations [48864/199482]\n",
            "Iterations [48896/199482]\n",
            "Iterations [48928/199482]\n",
            "Iterations [48960/199482]\n",
            "Iterations [48992/199482]\n",
            "Iterations [49024/199482]\n",
            "Iterations [49056/199482]\n",
            "Iterations [49088/199482]\n",
            "Iterations [49120/199482]\n",
            "Iterations [49152/199482]\n",
            "Iterations [49184/199482]\n",
            "Iterations [49216/199482]\n",
            "Iterations [49248/199482]\n",
            "Iterations [49280/199482]\n",
            "Iterations [49312/199482]\n",
            "Iterations [49344/199482]\n",
            "Iterations [49376/199482]\n",
            "Iterations [49408/199482]\n",
            "Iterations [49440/199482]\n",
            "Iterations [49472/199482]\n",
            "Iterations [49504/199482]\n",
            "Iterations [49536/199482]\n",
            "Iterations [49568/199482]\n",
            "Iterations [49600/199482]\n",
            "Iterations [49632/199482]\n",
            "Iterations [49664/199482]\n",
            "Iterations [49696/199482]\n",
            "Iterations [49728/199482]\n",
            "Iterations [49760/199482]\n",
            "Iterations [49792/199482]\n",
            "Iterations [49824/199482]\n",
            "Iterations [49856/199482]\n",
            "Iterations [49888/199482]\n",
            "Iterations [49920/199482]\n",
            "Iterations [49952/199482]\n",
            "Iterations [49984/199482]\n",
            "Iterations [50016/199482]\n",
            "Iterations [50048/199482]\n",
            "Iterations [50080/199482]\n",
            "Iterations [50112/199482]\n",
            "Iterations [50144/199482]\n",
            "Iterations [50176/199482]\n",
            "Iterations [50208/199482]\n",
            "Iterations [50240/199482]\n",
            "Iterations [50272/199482]\n",
            "Iterations [50304/199482]\n",
            "Iterations [50336/199482]\n",
            "Iterations [50368/199482]\n",
            "Iterations [50400/199482]\n",
            "Iterations [50432/199482]\n",
            "Iterations [50464/199482]\n",
            "Iterations [50496/199482]\n",
            "Iterations [50528/199482]\n",
            "Iterations [50560/199482]\n",
            "Iterations [50592/199482]\n",
            "Iterations [50624/199482]\n",
            "Iterations [50656/199482]\n",
            "Iterations [50688/199482]\n",
            "Iterations [50720/199482]\n",
            "Iterations [50752/199482]\n",
            "Iterations [50784/199482]\n",
            "Iterations [50816/199482]\n",
            "Iterations [50848/199482]\n",
            "Iterations [50880/199482]\n",
            "Iterations [50912/199482]\n",
            "Iterations [50944/199482]\n",
            "Iterations [50976/199482]\n",
            "Iterations [51008/199482]\n",
            "Iterations [51040/199482]\n",
            "Iterations [51072/199482]\n",
            "Iterations [51104/199482]\n",
            "Iterations [51136/199482]\n",
            "Iterations [51168/199482]\n",
            "Iterations [51200/199482]\n",
            "Iterations [51232/199482]\n",
            "Iterations [51264/199482]\n",
            "Iterations [51296/199482]\n",
            "Iterations [51328/199482]\n",
            "Iterations [51360/199482]\n",
            "Iterations [51392/199482]\n",
            "Iterations [51424/199482]\n",
            "Iterations [51456/199482]\n",
            "Iterations [51488/199482]\n",
            "Iterations [51520/199482]\n",
            "Iterations [51552/199482]\n",
            "Iterations [51584/199482]\n",
            "Iterations [51616/199482]\n",
            "Iterations [51648/199482]\n",
            "Iterations [51680/199482]\n",
            "Iterations [51712/199482]\n",
            "Iterations [51744/199482]\n",
            "Iterations [51776/199482]\n",
            "Iterations [51808/199482]\n",
            "Iterations [51840/199482]\n",
            "Iterations [51872/199482]\n",
            "Iterations [51904/199482]\n",
            "Iterations [51936/199482]\n",
            "Iterations [51968/199482]\n",
            "Iterations [52000/199482]\n",
            "Iterations [52032/199482]\n",
            "Iterations [52064/199482]\n",
            "Iterations [52096/199482]\n",
            "Iterations [52128/199482]\n",
            "Iterations [52160/199482]\n",
            "Iterations [52192/199482]\n",
            "Iterations [52224/199482]\n",
            "Iterations [52256/199482]\n",
            "Iterations [52288/199482]\n",
            "Iterations [52320/199482]\n",
            "Iterations [52352/199482]\n",
            "Iterations [52384/199482]\n",
            "Iterations [52416/199482]\n",
            "Iterations [52448/199482]\n",
            "Iterations [52480/199482]\n",
            "Iterations [52512/199482]\n",
            "Iterations [52544/199482]\n",
            "Iterations [52576/199482]\n",
            "Iterations [52608/199482]\n",
            "Iterations [52640/199482]\n",
            "Iterations [52672/199482]\n",
            "Iterations [52704/199482]\n",
            "Iterations [52736/199482]\n",
            "Iterations [52768/199482]\n",
            "Iterations [52800/199482]\n",
            "Iterations [52832/199482]\n",
            "Iterations [52864/199482]\n",
            "Iterations [52896/199482]\n",
            "Iterations [52928/199482]\n",
            "Iterations [52960/199482]\n",
            "Iterations [52992/199482]\n",
            "Iterations [53024/199482]\n",
            "Iterations [53056/199482]\n",
            "Iterations [53088/199482]\n",
            "Iterations [53120/199482]\n",
            "Iterations [53152/199482]\n",
            "Iterations [53184/199482]\n",
            "Iterations [53216/199482]\n",
            "Iterations [53248/199482]\n",
            "Iterations [53280/199482]\n",
            "Iterations [53312/199482]\n",
            "Iterations [53344/199482]\n",
            "Iterations [53376/199482]\n",
            "Iterations [53408/199482]\n",
            "Iterations [53440/199482]\n",
            "Iterations [53472/199482]\n",
            "Iterations [53504/199482]\n",
            "Iterations [53536/199482]\n",
            "Iterations [53568/199482]\n",
            "Iterations [53600/199482]\n",
            "Iterations [53632/199482]\n",
            "Iterations [53664/199482]\n",
            "Iterations [53696/199482]\n",
            "Iterations [53728/199482]\n",
            "Iterations [53760/199482]\n",
            "Iterations [53792/199482]\n",
            "Iterations [53824/199482]\n",
            "Iterations [53856/199482]\n",
            "Iterations [53888/199482]\n",
            "Iterations [53920/199482]\n",
            "Iterations [53952/199482]\n",
            "Iterations [53984/199482]\n",
            "Iterations [54016/199482]\n",
            "Iterations [54048/199482]\n",
            "Iterations [54080/199482]\n",
            "Iterations [54112/199482]\n",
            "Iterations [54144/199482]\n",
            "Iterations [54176/199482]\n",
            "Iterations [54208/199482]\n",
            "Iterations [54240/199482]\n",
            "Iterations [54272/199482]\n",
            "Iterations [54304/199482]\n",
            "Iterations [54336/199482]\n",
            "Iterations [54368/199482]\n",
            "Iterations [54400/199482]\n",
            "Iterations [54432/199482]\n",
            "Iterations [54464/199482]\n",
            "Iterations [54496/199482]\n",
            "Iterations [54528/199482]\n",
            "Iterations [54560/199482]\n",
            "Iterations [54592/199482]\n",
            "Iterations [54624/199482]\n",
            "Iterations [54656/199482]\n",
            "Iterations [54688/199482]\n",
            "Iterations [54720/199482]\n",
            "Iterations [54752/199482]\n",
            "Iterations [54784/199482]\n",
            "Iterations [54816/199482]\n",
            "Iterations [54848/199482]\n",
            "Iterations [54880/199482]\n",
            "Iterations [54912/199482]\n",
            "Iterations [54944/199482]\n",
            "Iterations [54976/199482]\n",
            "Iterations [55008/199482]\n",
            "Iterations [55040/199482]\n",
            "Iterations [55072/199482]\n",
            "Iterations [55104/199482]\n",
            "Iterations [55136/199482]\n",
            "Iterations [55168/199482]\n",
            "Iterations [55200/199482]\n",
            "Iterations [55232/199482]\n",
            "Iterations [55264/199482]\n",
            "Iterations [55296/199482]\n",
            "Iterations [55328/199482]\n",
            "Iterations [55360/199482]\n",
            "Iterations [55392/199482]\n",
            "Iterations [55424/199482]\n",
            "Iterations [55456/199482]\n",
            "Iterations [55488/199482]\n",
            "Iterations [55520/199482]\n",
            "Iterations [55552/199482]\n",
            "Iterations [55584/199482]\n",
            "Iterations [55616/199482]\n",
            "Iterations [55648/199482]\n",
            "Iterations [55680/199482]\n",
            "Iterations [55712/199482]\n",
            "Iterations [55744/199482]\n",
            "Iterations [55776/199482]\n",
            "Iterations [55808/199482]\n",
            "Iterations [55840/199482]\n",
            "Iterations [55872/199482]\n",
            "Iterations [55904/199482]\n",
            "Iterations [55936/199482]\n",
            "Iterations [55968/199482]\n",
            "Iterations [56000/199482]\n",
            "Iterations [56032/199482]\n",
            "Iterations [56064/199482]\n",
            "Iterations [56096/199482]\n",
            "Iterations [56128/199482]\n",
            "Iterations [56160/199482]\n",
            "Iterations [56192/199482]\n",
            "Iterations [56224/199482]\n",
            "Iterations [56256/199482]\n",
            "Iterations [56288/199482]\n",
            "Iterations [56320/199482]\n",
            "Iterations [56352/199482]\n",
            "Iterations [56384/199482]\n",
            "Iterations [56416/199482]\n",
            "Iterations [56448/199482]\n",
            "Iterations [56480/199482]\n",
            "Iterations [56512/199482]\n",
            "Iterations [56544/199482]\n",
            "Iterations [56576/199482]\n",
            "Iterations [56608/199482]\n",
            "Iterations [56640/199482]\n",
            "Iterations [56672/199482]\n",
            "Iterations [56704/199482]\n",
            "Iterations [56736/199482]\n",
            "Iterations [56768/199482]\n",
            "Iterations [56800/199482]\n",
            "Iterations [56832/199482]\n",
            "Iterations [56864/199482]\n",
            "Iterations [56896/199482]\n",
            "Iterations [56928/199482]\n",
            "Iterations [56960/199482]\n",
            "Iterations [56992/199482]\n",
            "Iterations [57024/199482]\n",
            "Iterations [57056/199482]\n",
            "Iterations [57088/199482]\n",
            "Iterations [57120/199482]\n",
            "Iterations [57152/199482]\n",
            "Iterations [57184/199482]\n",
            "Iterations [57216/199482]\n",
            "Iterations [57248/199482]\n",
            "Iterations [57280/199482]\n",
            "Iterations [57312/199482]\n",
            "Iterations [57344/199482]\n",
            "Iterations [57376/199482]\n",
            "Iterations [57408/199482]\n",
            "Iterations [57440/199482]\n",
            "Iterations [57472/199482]\n",
            "Iterations [57504/199482]\n",
            "Iterations [57536/199482]\n",
            "Iterations [57568/199482]\n",
            "Iterations [57600/199482]\n",
            "Iterations [57632/199482]\n",
            "Iterations [57664/199482]\n",
            "Iterations [57696/199482]\n",
            "Iterations [57728/199482]\n",
            "Iterations [57760/199482]\n",
            "Iterations [57792/199482]\n",
            "Iterations [57824/199482]\n",
            "Iterations [57856/199482]\n",
            "Iterations [57888/199482]\n",
            "Iterations [57920/199482]\n",
            "Iterations [57952/199482]\n",
            "Iterations [57984/199482]\n",
            "Iterations [58016/199482]\n",
            "Iterations [58048/199482]\n",
            "Iterations [58080/199482]\n",
            "Iterations [58112/199482]\n",
            "Iterations [58144/199482]\n",
            "Iterations [58176/199482]\n",
            "Iterations [58208/199482]\n",
            "Iterations [58240/199482]\n",
            "Iterations [58272/199482]\n",
            "Iterations [58304/199482]\n",
            "Iterations [58336/199482]\n",
            "Iterations [58368/199482]\n",
            "Iterations [58400/199482]\n",
            "Iterations [58432/199482]\n",
            "Iterations [58464/199482]\n",
            "Iterations [58496/199482]\n",
            "Iterations [58528/199482]\n",
            "Iterations [58560/199482]\n",
            "Iterations [58592/199482]\n",
            "Iterations [58624/199482]\n",
            "Iterations [58656/199482]\n",
            "Iterations [58688/199482]\n",
            "Iterations [58720/199482]\n",
            "Iterations [58752/199482]\n",
            "Iterations [58784/199482]\n",
            "Iterations [58816/199482]\n",
            "Iterations [58848/199482]\n",
            "Iterations [58880/199482]\n",
            "Iterations [58912/199482]\n",
            "Iterations [58944/199482]\n",
            "Iterations [58976/199482]\n",
            "Iterations [59008/199482]\n",
            "Iterations [59040/199482]\n",
            "Iterations [59072/199482]\n",
            "Iterations [59104/199482]\n",
            "Iterations [59136/199482]\n",
            "Iterations [59168/199482]\n",
            "Iterations [59200/199482]\n",
            "Iterations [59232/199482]\n",
            "Iterations [59264/199482]\n",
            "Iterations [59296/199482]\n",
            "Iterations [59328/199482]\n",
            "Iterations [59360/199482]\n",
            "Iterations [59392/199482]\n",
            "Iterations [59424/199482]\n",
            "Iterations [59456/199482]\n",
            "Iterations [59488/199482]\n",
            "Iterations [59520/199482]\n",
            "Iterations [59552/199482]\n",
            "Iterations [59584/199482]\n",
            "Iterations [59616/199482]\n",
            "Iterations [59648/199482]\n",
            "Iterations [59680/199482]\n",
            "Iterations [59712/199482]\n",
            "Iterations [59744/199482]\n",
            "Iterations [59776/199482]\n",
            "Iterations [59808/199482]\n",
            "Iterations [59840/199482]\n",
            "Iterations [59872/199482]\n",
            "Iterations [59904/199482]\n",
            "Iterations [59936/199482]\n",
            "Iterations [59968/199482]\n",
            "Iterations [60000/199482]\n",
            "Iterations [60032/199482]\n",
            "Iterations [60064/199482]\n",
            "Iterations [60096/199482]\n",
            "Iterations [60128/199482]\n",
            "Iterations [60160/199482]\n",
            "Iterations [60192/199482]\n",
            "Iterations [60224/199482]\n",
            "Iterations [60256/199482]\n",
            "Iterations [60288/199482]\n",
            "Iterations [60320/199482]\n",
            "Iterations [60352/199482]\n",
            "Iterations [60384/199482]\n",
            "Iterations [60416/199482]\n",
            "Iterations [60448/199482]\n",
            "Iterations [60480/199482]\n",
            "Iterations [60512/199482]\n",
            "Iterations [60544/199482]\n",
            "Iterations [60576/199482]\n",
            "Iterations [60608/199482]\n",
            "Iterations [60640/199482]\n",
            "Iterations [60672/199482]\n",
            "Iterations [60704/199482]\n",
            "Iterations [60736/199482]\n",
            "Iterations [60768/199482]\n",
            "Iterations [60800/199482]\n",
            "Iterations [60832/199482]\n",
            "Iterations [60864/199482]\n",
            "Iterations [60896/199482]\n",
            "Iterations [60928/199482]\n",
            "Iterations [60960/199482]\n",
            "Iterations [60992/199482]\n",
            "Iterations [61024/199482]\n",
            "Iterations [61056/199482]\n",
            "Iterations [61088/199482]\n",
            "Iterations [61120/199482]\n",
            "Iterations [61152/199482]\n",
            "Iterations [61184/199482]\n",
            "Iterations [61216/199482]\n",
            "Iterations [61248/199482]\n",
            "Iterations [61280/199482]\n",
            "Iterations [61312/199482]\n",
            "Iterations [61344/199482]\n",
            "Iterations [61376/199482]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5_X2WaaoDb8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the unlabeled data discourse type\n",
        "clean_unlabel_path = '/content/drive/My Drive/GT_classes/CSE_6240 - Text Mining/Reddit-Roles-Identification/discourse/unlabelled_dataset_predictions.csv'\n",
        "posts_annotations = open(preprocess_file, 'w')\n",
        "fieldnames = ['original_post', 'clean_post', 'post_type']\n",
        "writer = csv.DictWriter(posts_annotations, fieldnames=fieldnames)\n",
        "\n",
        "writer.writeheader()\n",
        "\n",
        "for i in range(len(clean_unlabel_text)):\n",
        "    print(clean_text)\n",
        "    original_post = posts[i]\n",
        "    clean_text = clean_unlabel_text[i]\n",
        "    dict_to_store = {'original_post':, 'clean_post':clean_text, \"post_type\":post_type}\n",
        "    if clean_text:\n",
        "    print(dict_to_store)\n",
        "        writer.writerow(dict_to_store)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}